---
title: "IES R Tutorial"
author: "Michael Chirico"
date: "Compiled `r format(Sys.time(), '%B %d, %Y at %R')`"
output: 
  rmarkdown::html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

First and foremost is to install R (following instructions [here](http://lib.stat.cmu.edu/R/CRAN/)) so that your computer knows how to interpret R code. Then, I highly recommend you install RStudio (following instructions [here](https://www.rstudio.com/products/rstudio/download/)), which is a powerful program for organizing your interaction with the R language (a subtle distinction, and you wouldn't really be ill-served by considering RStudio and R to be the same thing); for the jargon-hungry, RStudio is an Integrated Development Environment (IDE) for the R language.

Some general guidelines for getting the most out of this workshop:

 1. **Active participation is key!** You should be running every snippet of code I go over on your own machine. As with human languages, exposure is one of the building blocks of fluency. You would be insane to think that you can osmote the ability to understand and use R code just by watching me do things on a screen. This facility takes practice, practice, hair pulling, and more practice. For those with little exposure to more advanced programming languages, today's workshop will be _very_ heavy with material. You shouldn't expect to absorb everything, but actively participating will help _a lot_.
 
 2. **Questions are openly encouraged!** As I mentioned, this will, despite my efforts to make things as straightforward as possible, inevitably be a somewhat dense workshop. So nobody should hesitate to stop me if they're lost. I am no fan of hearing myself speak, so if nobody in the audience is following me, that's a waste of everyone's time. 

***
***
***
# R Basics (8 AM - 9 AM)

Before we get to the fun stuff (statistical analysis), it's important that we gain some facility with doing basic things in R first -- adding numbers, creating objects, and tools for exploring/understanding new objects as we come across them, including understanding and overcoming common errors that can creep into our code.

First things first, let's follow the time-honored tradition of making R say "Hello World".

Make sure you're in the console (the cursor in front of the right angle bracket (`>`) should be blinking) and type (or copy-paste) the following:

```{r hello}
print("Hello World")
```


Easy-peasy. The console is a great place to do what I call sandboxing -- running small commands to test whether they run as expected and produce the right output in the right format, etc. But it would be very cumbersome to use the console to do everything. There's a distinct lack of permanency to anything we do in the console.

More common is to run code from an R script. You can create an R script in RStudio by clicking the plus-page icon (![New R script](http://imgur.com/eCEtq9e.png "plus-page icon")) and hitting "R Script" or by using the keyboard shortcut `Ctrl`+`Shift`+`N`. Try printing "Hello World" again by adding `print("Hello World")` to your new script and pressing `Ctrl`+`Enter` on this line. `Ctrl`+`Enter` is the shortcut for running the selected line, or for running a highlighted section of code.

Once we save our R script, we can easily share it with coauthors or the general public (or even ourselves, on a different computer), who can simply run the code again on their machine to reproduce your analysis. 

***
## Assignment

Consider the following:

```{r assign_scalar}
x = 3
```

When we execute the above line of code, we're creating the _variable_ `x` and associating with it the value `3`. This is like creating a `local` in Stata.

Variables (a.k.a. objects) are the most crucial building block for everything we want to do in R. When we create a variable, we create a shorthand for some value that we'll refer to lower in our code.

Assignment has to pass from right to left -- the _object_ on the right of `=` is assigned to the _name_ on the left of `=`.

The opposite is an error, since we haven't told R what `y` is yet:

```{r assign_error, error = TRUE}
3 = y
```

This error tells us that `3` is not a valid thing to which to assign; as a rule of thumb for beginners, all variable names must start with letters (though can contain other characters thereafter).

> > Side note: the following works just as well:
> >  `x <- 3`
> > this method of assignment is probably more common, but we'll stick with using `=` today since I think it's more intuitive. The differences between `=` and `<-` are all-but irrelevant for beginners, but for future reference, [this Q&A](http://stackoverflow.com/questions/1741820/) is worth a read: 

***
## Vectors

The easiest way to think of a vector is as a column (or a row) in Excel. A column in excel can contain many numbers, but instead of referring to each of them individually, we refer to the row. Typically, these numbers all have something in common (for example, a column Name in Excel should be filled with peoples' names).

The way to declare vectors in R is using `c`, which stands for _**c**_oncatenate:

```{r assign_vector}
x <- c(1, 2, 3, 4)
x
```

Now `x` contains four numbers. Note that since this type of sequence is so common, R has built in the colon (`:`) as an operator to create a variable like this more quickly/concisely:

```{r assign_sequence}
x <- 1:4
x
#Also works in reverse
x <- 4:1
x
```

***
## Types

Thus far, everything we've assigned to a variable has been a number. But this is _far_ from the only thing we can assign to a variable in R. Consider:

```{r assign_non_numeric}
x <- c("Philadelphia", "Pennsylvania")
y <- c(TRUE, FALSE)
z <- c(1L, 2L)
```

`x` contains a _string_ of letters _demarcated by `""` (or `''`) to distinguish them from variable names_. We refer to `x` as a `character` vector. 

`y` is a `logical` vector; it is often very convenient to keep binary variables (gender, treatment group, etc.) stored as `logical` variables, for reasons we'll see below.

`z` is an `integer` vector. This is distinguished from `c(1, 2)`, which gets stored as a `numeric` vector. The difference between the two probably won't affect you for quite some time, but it's often important for saving computer memory. `1`, as a `numeric`, takes up more space in your computer's memory than does `1L`. `L` signifies integer; don't worry about why (if you insist, it stands for _**L**_ong integer, which is too involved for this session).

### Lists

Consider this:

```{r assign_coerce}
x <- c(1, 2L, TRUE, "America")
```

All four components of `x` are of different type (namely, `numeric`, `integer`, `logical`, `character`). Recall above that we said vectors must have something in common, but as we declared it, this couldn't be further from the truth. In fact, R will force all of these components to have the same type -- namely, character:

```{r assign_coerce_print}
x
```

Note the quotation marks -- none of the components are any longer considered as `character` strings in `x`.

> > The specific heirarchy is `logical` < `integer` < `numeric` < `character`

However, mixing types is a fundamental feature of almost all data analysis, so it stands to reason that there is a straightforward way to do so. In R, this is done with the `list` type. We can replace the code above using `list`:

```{r assign_list}
x <- list(1, 2L, TRUE, "America")
x
```

Note how different the output looks, as compared to using `c`!! The quotation marks are gone except for the last component. You can ignore the mess of `[[` and `[` for now, but as an intimation, consider some more complicated `list`s:

```{r assign_lists}
x <- list(c(1, 2), c("a", "b"), c(TRUE, FALSE), c(5L, 6L))
x

y <- list(list(1, 2, 3), list(4:5), 6)
y
```

`x` is a `list` which has 4 components, each of which is a vector with 2 components. This gives the first hint at how R treats a dataset with many variables of different types -- at core, R stores a data set in a `list`!

`y` is a _nested_ `list` -- it's a `list` that has `list`s for some of its components. This is very useful for more advanced operations, but probably won't come up for quite some time, so don't worry if you haven't wrapped your head around this yet.

***
## Extraction/Indexing

Consider a simple `numeric` vector:

```{r extract_assign}
x <- 5:14
```

How do we get at the various numbers stored in certain positions in `x`? For example, how could we get the first number in `x`, `5`?

This process is called _extraction_, and, for vectors, is done with square brackets (`[]`), e.g.:

```{r extract_vector}
x[1] #first element of x

x[5] #fifth element of x
```

This also works on `list`s, but `list`s also have some other ways to get at their contents:

```{r extract_list}
y <- list(1:3, 4:6, 7:9)
y[1]
```

Note that the output still has `[[` in it. This means the result of `y[1]` is _still a list_.

More typically, we want to _remove_ the `list` structure and just get `1:3` instead of `list(1:3)`. To do this, we use `[[`:

```{r extract_list_item}
y[[1]]
```

### Named vectors and `list`s

It is also possible to name the elements of vectors and lists. This is convenient for making it easy to get certain elements without having to remember whether you stored it first, third, or whatever:

```{r extract_named}
x <- c("Iowa" = "Cruz", "Ohio" = "Kasich", "Pennsylvania" = "Trump")

y <- list(names = c("Kasich", "Cruz", "Trump"),
          ages = c(63, 45, 69),
          hates_clinton = c(TRUE, TRUE, TRUE))
```

For named vectors, we keep using `[]` to extract elements, but we can use the name instead of the index:

```{r extract_named_vector}
x["Iowa"]
```

For named `list`s, we can use `[]`, but, as with numbered indices, we'll get a `list` in return. If we want the actual object contained at that point in the list, we can still use `[[`, or we can also use `$`, which is another extraction operator:

```{r extract_named_list}
#now that y has names, we no longer see [[ -- instead, we see
#  the name of each element of the list
y["ages"]

y[["ages"]]

y$names
```

### Multiple extraction

As often as not, when we need to extract, we need _more than one_ element of the vector. 

To do this, we _pass a vector to `[]`_. It's must intuitive when we need elements in sequence, e.g.:

```{r extract_sequence}
# R conveniently keeps the letters of the
#   alphabet stored automatically in two
#   vectors, letters and LETTERS;
#   the former is lower-, the latter uppercase
x <- LETTERS
x

#Get the first 5 letters
x[1:5]
```

What if we need the first and 5th elements, but not the 3rd/4th/5th elements?

Remember that `1:5` is the same as `c(1, 2, 3, 4, 5)`. So we could have just used the latter:

```{r extract_sequence_long}
x[c(1, 2, 3, 4, 5)]
```

By extension, if we just want the first and fifth elements:

```{r extract_non_adjacent}
x[c(1, 5)]
```

_**NOTE**_: there's no* way to extract multiple elements using the other extraction operators, `[[` and `$`. It's an error to try:

```{r extract_errors, error = TRUE}
x <- list(a = 1:3, b = 4:6)
#multiple extraction by names works fine:
x[c("a", "b")]

#but not with [[ or $
x[[1:2]] #technically, R interprets this as a recursive index...

x[[c("a", "b")]] #again, a recursive index...

x$c("a", "b")
```

### Logical indexing

Another exceedingly common approach is to subset an object based on a condition satisfied by some of the elements.

Here, we must introduce the logical operators (also called _binary operators_, which is important for understanding associated error messages) in R.

These are `<`, `>`, `<=`, `>=`, `==`, and `!=`. These are all the same in Stata (note: `~=`, which works in Stata as an alternate to `!=`, _doesn't work in R_). First, let's explore them:

```{r logical_operators}
ages <- c(12, 14, 16, 18, 20, 22, 24, 30,
          38, 40, 55, 60, 63, 66, 68, 70)

#Who is voting age?
ages >= 18

#Who can't drink?
ages < 21

#Who is exactly 40?
ages == 40
```

Note that R automatically figures out whether _each element_ of `ages` satisfies the condition.

The way to extend this to extraction is simple:

```{r extract_logical}
#Only take people under age 18
ages[ages < 18]

#Only take AARPers
ages[ages >= 50]

#Exclude any 70 year old:
ages[ages != 70]
```

The other common logical operations are intersection, union and negation, aka **AND**, **OR** and **NOT**, which in R, as in Stata, are `&`, `|` and `!` (you'll also often see `&&` and `||` in other peoples' code; don't worry about the difference yet, but for those who know Matlab -- it's the same).

Quickly, let's get working-agers:

```{r extract_logical_pair}
ages[ages >= 18 & ages <= 65]
```

We can combine these to our heart's fancy and create any combination of logical requirements, e.g.:

```{r extract_logical_chain}
#get people who are over 18, but not exactly 34 or 70
ages[ages >= 18 & !(ages == 34 | ages == 70)]
```

### Negative indexing

Sometimes, we'd rather _exclude_ a small number of elements, rather than _include_ elements like we have been doing so far.

To do this, we precede the vector we want to extract with `-` (if it's `numeric`/`integer`/`character`) or `!` (if it's `logical`). 

Suppose we wanted to find out how many years have passed since our earliest observation, and to exclude the earliest observation. We might do something like:

```{r extract_negate}
years <- c(1970, 1973, 1978, 1980, 1990, 1995)
#the earliest year is 1970, which comes first; to exclude:
years[-1]
#to exclude and subtract 1970:
years[-1] - years[1]
```

***
## Base functions

R comes equipped with a vast (_vast_) library of built-in functions intended to make your life as a data analyst as easy as possible. As of this writing I count 1,203 functions included in base R, and 2,374 total functions that come ready to use as soon as you open up RStudio.

These are the real work horses of R, and many of them should be familiar to Excel and Stata users. Like in those programs, functions are distinguished by the use of parentheses `()` (in fact we've already used many functions to this point, most obviously the concatentation function `c`). We won't be able to get anyhwere near understanding all of the multitudinous functions R makes available to us today, but we can make a start.

### Basic Arithmetic/Stats

Finding the sum of a column in Excel is sort of a pain. You have to write `=SUM(` and then highlight all the proper cells. It's kind of a pain in Stata as well. You either have to `summarize var` the variable and run `di r(sum)` or create a new variable with `egen sum_var = sum(var)`. I find this and many other artihmetic operations much simpler to do in R.

```{r basic_arithmetic}
x <- 1:10

#find the sum
sum(x)

#find the mean
mean(x)

#find the variance and standard deviation
var(x)
sd(x)

#scalar arithmetic
3 * x
x - 2
x^2
x / 4

#exponentials
exp(1:3)
log(1:10)

#other arithmetic
abs(-5:5)

#rounding up/down
x <- c(.1, 1.1, 1.9, 2, 2.8)
floor(x)
ceiling(x)
```

### Function arguments

Most functions accept more than one argument. Consider rounding a number:

```{r round}
x <- c(1.12, 10.2, 1.56, 21.9, 2)
round(x)

#round to 1 digit past the decimal
round(x, 1)

#round to -1 digit past the decimal
#  (i.e., 1 digit befor the decimal)
round(x, -1)
```

There's no limit to the number of arguments a function can accept to accomplish a wide variety of tasks. With `round`, it's easy to handle this because there are only two arguments -- first, a vector of numbers, and second, a (single) number of digits to which to round the vector.

When the number of arguments balloons, however, it gets more and more difficult to keep track of which argument goes where.

Consider the `quantile` function, which takes 5 arguments -- `x`, a vector of numbers; `probs`, a vector of probabilities (quantiles); `na.rm`, which tells it whether to ignore missing data (more on that later); `names`, which tells R whether the result should be named (see example); and `type`, which tells R the algorithm to use for measuring the quantiles (there are 9 readily available).

It would be quite a pain to expect users to memorize the order in which `quantile` expects arguments, especially if we had to do so for every function we ever wanted to use (remember, there are _thousands_). To facilitate this, R allows us to _name_ our arguments.

```{r function_named_arguments}
x <- c(1, 1, 1, 2, 3, 4, 5, 6, 
       7, 8, 8, 8, 9, 10, 10, 10)

#by default, quantile calculates all 5 quartiles:
#  min, 25%-ile, median, 75%-ile, max
quantile(x)
median(x)

#if we just want the median
quantile(x, .5)
#or, we can name it to be explicit
quantile(x, probs = .5)

#if we want to exclude the names
quantile(x, probs = .5, names = FALSE)
```

In addition to making it easier for us as analysts to use the functions, naming arguments also makes it easier for _others_ to read our code. While it's fair to expect most users to know the basic functions and their arguments pretty well, the more advanced the function is that you're using, the more understandable your code gets when aided by named arguments.

### Infix operators and `match`

_Infix operators_ are things that are written from left to right like basic arithmetic, but which do things not covered by the five main arithmetic operators (`+`, `-`, `*`, `/`, `^`). These operators in R are always surrounded by two percent signs (`%`). The two math-y ones have to do with modular arithmetic:

```{r modular_arithmetic}
x <- 1:20
# integer division
x %/% 3
# modular division (remainder)
x %% 3
```

By far the most commonly used infix, though, has to be `%in%`. This is used to determine whether some objects (the left) can be found in another set (the right). Best with an example:

```{r %in%}
candidates <- 
  c("Bernie Sanders", "Hillary Clinton", "Lincoln Chafee", 
    "Jim Webb", "Laurence Lessig", "Donald Trump", "John Kasich",
    "Ted Cruz", "Marco Rubio", "Jeb Bush", "Ben Carson",
    "Carly Fiorina", "Lindsey Graham", "Chris Christie",
    "Rick Santorum", "Rand Paul", "Jim Gilmore", "Mike Huckabee",
    "George Pataki", "Bobby Jindal", "Scott Walker", "Rick Perry")

nominees <- c("Hillary Clinton", "Donald Trump")

candidates %in% nominees
```

A quick note that what `%in%` is actually doing is using the `match` function, which is also useful. `match` takes three arguments. The first and second are like the left- and right-hand side of `%in%`, respectively. `match` tries to find each element of the left in the right, and, if it's found, gives the position. The third argument, `nomatch` tells it what to do when the left element is _not_ found. Example:

```{r match}
color <- c("red", "green", "orange", "blue",
           "purple", "teal", "mahogany", "yellow",
           "orange", "white", "lavendar")

roygbiv <- c("red", "orange", "yellow", "green",
             "blue", "indigo", "violet")

#%in% tells you if each element of
#  color is in roygbiv at all
color %in% roygbiv

#match tells you WHERE in roygbiv 
#  each element of color was found;
#  we set no.match to say what
#  we expect when the color isn't found
#  (it must be an integer)
match(color, roygbiv, nomatch = -1)
```

### More useful functions

#### `length`

Length is used to find how many elements something has (in data, how many observations there are)

```{r length}
x <- 1:50
length(x)
```

#### `rep`

`rep` is used to _**rep**_eat an object a certain number of times. The `each` argument repeats each element a set number of times. `length.out` makes sure the output has a certain number of elements.

```{r rep}
rep(1:3, 4)

rep(1:3, each = 4)

rep(1:3, each = 4, length.out = 10)
```

#### `unique` / `duplicated`

`unique` eliminates all duplicates of a vector; `duplicated` returns a `logical` vector telling you which elements appeared earlier in the vector.

```{r unique/duplicated}
x <- c(1, 1, 1, 2, 2, 3)
unique(x)
duplicated(x)
### unique is the same as (but faster than) x[!duplicated(x)]
x[!duplicated(x)]

#unique is often used in conjunction with length
#  to find out the number of IDs in a vector, e.g.
length(unique(x))
```

#### `seq`

`seq` produces a _**seq**_uence. It has four main arguments: `to` (where to start), `from` (where to end), `by` (increment), and `length.out` (what should be the `length` of the result?). We must specify no more than 3 of these. Some examples:

```{r seq}
#same as 1:10
seq(10)

#same as 2:11
seq(2, 11)

#make a grid from 0 to 10 with 30 points;
#  R automatically figures out the increment
seq(0, 10, length.out = 30)

#get all the odd numbers from 1 to 50:
#  note that the upper endpoint won't
#  necessarily be included in the output
seq(1, 50, by = 2)
```

#### `sort`

`sort` will give an ordered version of its input. Default is increasing order; use the `decreasing` argument to reverse this.

```{r sort}
x <- c(1, 2, 4, -1, 8, 9, 20, 13, 0)
sort(x)
sort(x, decreasing = TRUE)
```

#### `paste` / `paste0`

`paste` and `paste0` are R's ways of concatenating strings, i.e., combining them together. `paste` has an argument `sep` which tells how to _**sep**_arate the components; `paste0` is a slightly faster version of `sep` that uses `sep = ""` (i.e., don't separate the output). The `collapse` argument, available to both, will reduce a vector to a single string, using `collapse` to separate. Examples:

```{r paste}
first <- c("Sam", "Charles", "Matthew", "Thom")
last <- c("Beam", "Mingus", "Embree", "Yorke")

#default sep is a space, sep = " "
paste(first, last)
paste(last, first, sep = ", ")
paste(last, first, collapse = ", ")
paste0(first, last, collapse = ", ")
```

***
## Random Numbers

As statistical analysts, randomness is our lifeblood. As a language designed for statistical analysis, then, it stands to reason that R comes well-equipped to handle many common statistical operations. This includes producing random numbers a number of common distributions, random permutations, random subsets, etc.

### Uniform and normal random numbers

The most common kind of random numbers that people want are uniform draws and normal draws. 

```{r unif/norm}
#generate 10 U[0, 1] draws
runif(10)
#generate 10 U[3, 5] draws
runif(10, min = 3, max = 5)

#generate 10 N(0, 1) draws
rnorm(10)
#generate 10 N(3, 5) draws
rnorm(10, mean = 3, sd = 5)
```

These examples highlight the common format of random number generators in R. To get random numbers from a distribution, there is probably a function named like `r`**`dist`**, where **`dist`** is an abbreviation for the distribution (here, **unif**orm and **norm**al). The first argument is the number of draws; the rest of the arguments are parameters.

Here is a complete table of all of the common distributions built in to R, and the function used to invoke their random number generator (RNG):

                   Distribution RNG          Other Parameters          Wikipedia
 ------------------------------ ------------ ------------------------- ----------
                           Beta `rbeta`      `shape1`, `shape2`, `ncp` https://en.wikipedia.org/wiki/Beta_distribution
                       Binomial `rbinom`     `size`, `prob`            https://en.wikipedia.org/wiki/Binomial_distribution
                         Cauchy `rcauchy`    `location`, `scale`       https://en.wikipedia.org/wiki/Cauchy_distribution
                    Chi-Squared `rchisq`     `df`, `ncp`               https://en.wikipedia.org/wiki/Chi-squared_distribution
                    Exponential `rexp`       `rate`                    https://en.wikipedia.org/wiki/Exponential_distribution
                              F `rf`         `df1`, `df2`, `ncp`       https://en.wikipedia.org/wiki/F-distribution
                          Gamma `rgamma`     `shape`, `rate`, `scale`  https://en.wikipedia.org/wiki/Gamma_distribution
                      Geometric `rgeom`      `prob`                    https://en.wikipedia.org/wiki/Geometric_distribution
                 Hypergeometric `rhyper`     `m`, `n`, `k`             https://en.wikipedia.org/wiki/Hypergeometric_distribution
                     Log-Normal `rlnorm`     `meanlog`, `sdlog`        https://en.wikipedia.org/wiki/Log-normal_distribution
                       Logistic `rlogis`     `location`, `scale`       https://en.wikipedia.org/wiki/Logistic_distribution
                    Multinomial `rmultinom`  `size`, `prob`            https://en.wikipedia.org/wiki/Multinomial_distribution
              Negative Binomial `rnbinom`    `size`, `prob`, `mu`      https://en.wikipedia.org/wiki/Negative_binomial_distribution
                        Poisson `rpois`      `lambda`                  https://en.wikipedia.org/wiki/Poisson_distribution
 Wilcoxon Sign Ranked Statistic `rsignrank`* `n`                       https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
                      Student t `rt`         `df`, `ncp`               https://en.wikipedia.org/wiki/Student%27s_t-distribution
                        Weibull `rweibull`   `shape`, `scale`          https://en.wikipedia.org/wiki/Weibull_distribution
 Wilcoxon Sign Ranked Statistic `rwilcox`*   `m`, `n`                  https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
                        Wishart `rWishart`   `df`, `Sigma`             https://en.wikipedia.org/wiki/Wishart_distribution

Table: List of native distributions from built-in `stats` package

In addition to RNGs, R typically has three other functions associated with a given distribution: a **d**ensity function with prefix `d`, e.g. `dnorm`, giving the PDF; a **p**robability function with prefix `p`, e.g. `pnorm`, giving the CDF; and a **q**uantile function with prefix `q`, e.g. `qnorm`, giving quantiles. 

```{r dist}
#this is mainly used for plotting, see below
dnorm(seq(-3, 3, length.out = 30))

#great for calculating p values
pnorm(1.96)

#great for getting critical values
qnorm(c(.005, .025, .05))
```

### `sample`

As often as we need random draws from uniform or normal distributions, we need to take random subsets of vectors, or to get random integers. The main function for tasks like this is `sample`, which takes 4 arguments: `x` (a vector from which to sample, or an integer representing the max of numbers from which to sample), `size` (the number of elements to sample, which defaults to `length(x)`), `replace` (whether draws are taken with replacement), and `prob` (a vector of probabilities, for weighted sampling -- default is uniform).

As always, it's easiest to see with some examples:

```{r sample}
#A permutation of 1:5
sample(5)
#A size-3 subset of 1:5
sample(5, 3)
#A size-5 subset of 1:5, with replacement
sample(5, 5, replace = TRUE)

#A weighted sample of 1:5, heavily weighting 1
sample(5, 5, replace = TRUE, prob = c(.99, .0025, .0025, .0025, .0025))
```

We can use the basic tools here to run basic randomization exercises, like rolling dice, flipping coins, or drawing cards from a deck:

```{r dice/coins/cards}
#Flip 10 coins, get the percentage heads
flips <- sample(c("H", "T"), 10, replace = TRUE)
mean(flips == "H")

#Roll a pair of dice and add them
sum(sample(6, 2, replace = TRUE))

#create a deck of labeled cards:
#  (I copy-pasted the text for the suits from
#   Wikipedia, but we could have used S/H/D/C:
#   https://en.wikipedia.org/wiki/Playing_cards_in_Unicode)
deck <- paste0(rep(c(2:10, "J", "Q", "K", "A"), 4),      #card values
               rep(c("♠", "♥", "♦", "♣"), each = 13)) #suits
deck

#Now draw a poker hand
sample(deck, 5)
```

### `replicate`

Full scale Monte Carlo/bootstrapping requires replicating a simulation exercise over and over. Again, R is ready and has the `replicate` function for us. Let's simulate a few poker hands to find their frequency and compare with the theoretical frequency:

```{r poker}
#It'll be easier to deal with numbers than
#  with the fancy strings we used above
deck_nos <- 1:52
#Simulating a flush
simulations <- 
  replicate(10000, #number of repetitions/simulations
            { #use curly braces to run a simulation that takes more than one line
              hand <- sample(deck_nos, 5) #draw a five-card hand
              suits <- hand %% 4 # force card number into one of 4 categories
              length(unique(suits)) == 1 #returns TRUE if all suits are equal, FALSE else
            }) #close braces and parentheses
#How often does a flush happen?
#  True frequency is 0.1965%
mean(simulations)

#What about a pair (just two cards match)?
#  true frequency: 42%
simulations <- 
  replicate(10000, {
    hand <- sample(deck_nos, 5)
    faces <- hand %% 13 # force into one of 13 categories
    sum(duplicated(faces)) == 1 #one pair if there's exactly _one_ match
  })
mean(simulations)
```

### `set.seed`

Just a quick note that any time we run a simulation, it's a good idea (for _replicability_) to _set and save the random seed_! Random numbers from computers may look random, but in fact they're completely deterministic -- if we know the value of the random seed! This is a disadvantage for information security (which is only sometimes important to us as data analysts), but an advantage for academic research, where replicability is paramount.

A simple example:
```{r seed}
set.seed(100)
runif(1)

#if we run again, we'll get a different number
runif(1)
#but if we re-set the seed, we'll get the same
set.seed(100)
runif(1)
```

***
## Vectorization / `*apply`

The last set of major workhorse functions in `R` are the `*apply` functions -- `sapply` and `lapply`. `apply` is also important, but I want to avoid touching on matrices today, if I can. The oft-ignored cousins are `rapply` (**r**ecursive), `tapply` (**t**agged), `mapply` (**m**ultivariate), `vapply` (**v**erified), and `eapply` (**e**nvironment). 

You should think of `lapply` as **l**ist apply and `sapply` as **s**implified apply, because the former returns a `list`, and the latter returns a more simplified object (vector/matrix/array).

Both of these functions are, by and large, what you should be using in R instead of a `for` loop. A `for` loop often consists of going through every element of a `list` and doing something to its contents. `for` loops exist in R, as they probably do in every langauge, but are typically much slower than what is often called a vectorized approach. We'll get into that more later, but for now I'll just leave an example of `lapply` and `sapply` in action:

```{r apply}
#pick three triplets of numbers at random
l <- list(sample(100, 3), sample(100, 3), sample(100, 3))
l

#now find the max within each group
lapply(l, max)

#note that the output is a list. It's typically
#  more convenient to have the output as a vector:
sapply(l, max)
```

***
## Packages

One of the things that makes R truly exceptional is its vast library of user-contributed packages.

R comes pre-loaded with a boat-load of the most common functions / methods of analysis. But in no way is this congenital library complete.

Complementing this core of the most common operations are external _packages_, which are basically sets of functions designed to accomplish specific tasks. The entire afternoon of this workshop is devoted to gaining facility in just a few of these.

Best of all, unlike some super-expensive programming languages, all of the thousands of packages available to R users (most importantly through CRAN, the **C**omprehensive **R** **A**rchive **N**etwork) are _completely free of charge_. 

The two most important things to know about packages for now (we'll start using my favorite, `data.table`, shortly) is where to find them, how to install them, and how to load them.

### Where to find packages

Long story short: Google. Got a particular statistical technique in mind? The best R package for this is almost always the top Google result if asked correctly.

How about heirarchical linear modeling (HLM)? A quick Google for "HLM R" or "heirarchical linear modeling R" both turn up some articles/tutorials/CRAN pages referencing `lmer`. 

### How to install packages

Just use `install.packages`!

```{r install.packages, eval = FALSE}
#we won't be able to run this, because it requires
#  administrative priviliges, but you can do
#  this easily on your own computer
install.packages("lmer")
```

### How to load packages

Just add it to your library!

```{r library, eval = FALSE}
#this won't run if it's not installed
library(lmer)
```

_Et voila_! You'll now be able to run HLM in R. You can also Google "tutorial lmer" (or in general "tutorial [package name]") and you're very likely to find a trove of sites trying to help you learn the package. Most popular packages also come with worked examples available through the example function, e.g., `example(package = "lmer")`.

***
## Toolkit: `ls`/`rm`/`?`/`class`/`dput`/`str`

Just a few more things that aren't related to statistical analysis _per se_, but which will often facilitate your coding experience.

First, `ls` and `rm`, short-hand for **l**i**s**t objects and **r**e**m**ove objects. These can be useful for understanding what names you've assigned to objects, and for occasionally cleaning up the clutter of variables you'll no longer use (for the meticulously clean, but also because leaving a bunch of unused objects lying around is a great way to create irritating errors / problems with your code).

Let's see all the junk we've created thus far in the workshop:

```{r lsrm}
#ls is a function, so we have to use () to call it
ls()

#one of the optional arguments to ls is to 
#  also list some "hidden" objects:
ls(all = TRUE)

#what a mess! let's get rid of some stuff
rm(ages, candidates, color)

#ugh, this is getting boring. What about
#  a scorched earth technique?
rm(list = ls(all = TRUE))

#after the apocalypse, nothing remains:
ls(all = TRUE)
```

### Troubleshooting

Finally, I'll leave you with some tools that are often the first place to look when trying to troubleshoot code that isn't working correctly.

#### `?`

`?` is the help operator in R. If you'd like to know _anything_ about any function in R, just type `?` before its name and hit enter. Let's start with the help page for `replicate` by entering `?replicate`. All help files are structured similarly, with the following common sections (I've highlighted in **bold** the most useful for troubleshooting):

 1. Description: an overview of the function(s) described on the page. `?replicate`, for example, redirects us to the help page for `lapply`, since in addition to `replicate` this page covers `lapply`, `sapply`, `vapply`, and `simplify2array` (don't worry about the last two, they're a bit more advanced and won't come up today).
 2. **Usage**: a generic snippet of code which shows the name and default value of every argument of every function on the page. This is very useful for understanding the ordering and naming of all the arguments to a function. `replicate`, for example, has 3 arguments, only 2 of which we used above: `n` is the number of repetitions, which has no default; `expr` is the expression to repeat (surrounded in curly braces `{}` above), which has no default; and `simplify`, which gives us control over how/whether the results are converted from a `list`, which has default `"array"`, leading to the output we saw above. 
 3. **Arguments**: gives a brief overview of what each function expects to receive for each argument (the type, whether it can be a vector or has to be a scalar, etc.). You shouldn't expect to fully understand everything that's being told to you here for a while, since it's pretty specific, but it can still help.
 4. Details: Some nitty-gritty about how the functions work, some mistakes to avoid, some of the more intricate details of how exactly the function goes about its task. Tends to be very dense and jargon-heavy.
 5. Value: Tells you details about the _type_ of object that you should expect to be returned as a result of running each function (e.g., we noted that `lapply` _always_ returns a `list`; this is noted in the Value section of `?replicate` in the first sentence).
 6. Note: Some more technical details about the function and its edge cases.
 7. References: Especially useful for packages -- this usually tells you where to go to find some more reading about the algorithms implemented.
 8. See Also: Gives some related functions which might also help accomplish similar tasks.
 9. **Examples**: A great place to look for some simple reproducible examples of how the functions work, with code that can be copy-pasted and run by you, the user.
 
The help files are an absolute _must_ go-to reference -- it's always the first place you should check when you're stuck.

#### `class`

`class` simply tells you the class/type of an object in memory. You'd be surprised how often the root of an issue is simply that _you_ think a certain variable is type X (say, `numeric`), but actually R is keeping track of it as if it's type Y (frighteningly often, `factor` -- more on that later).

#### `dput`

`dput` is like a microscope for R objects. It deconstructs any variable in R to its atoms -- its absolute fundamental components -- and prints out what's hidden under the hood. This is especially useful because all of the most complicated objects in R tend to have an associated `print` method which renders them in a digestible form. This is great for taking a glance at a complicated object, but not useful for understanding what's going wrong with that object.

Take, for example:

```{r dput}
#we'll get more into how R understands dates/times
#  later in the day; the key takeaway here is 
#  how R prints the object vs. how it's actually stored.
t <- as.POSIXlt("2015-05-12")
t
#when we just enter the object at the console, its
#  print method is invoked, i.e.:
print(t)

#this is a bit misleading. From what we see, it would appear
#  that t is just a character string. However:
t == "2015-05-15 EDT"

#so, what is t? use dput
dput(t)
#actually, it's a rather complicated list with components
#  describing every which thing about the date/time we entered
```

#### `str`

Often the output of `dput` is far too verbose to be of real use. `str` is a similar function which gives us a more detailed look at the components of a complicated object, but with the advantage that it doesn't give us too many details; it's more like a table of contents for R objects.

```{r str}
#don't worry about what lm is yet, we'll get there soon
y <- rnorm(1000)
x <- rnorm(1000)
r <- lm(y ~ x)
str(r)
```

Whew, that was a lot! Congratulations!

Now, on to the more fun and useful stuff!

***
***
***
# Data Basics (9 AM - 11 AM)

This section is intended to lay the groundwork of all of the most common data analysis tasks we face when tackling our empirical problems. 

## Reading .csv Files

The most common format of file you're like to find in the real world is in .csv (comma-separated value) format.

Quick clarification that not all comma-separated files have the extension. Often you'll find a file with the extension .txt that actually contains text that is comma-separated. You just have to inspect the file to figure out how it's stored.

In fact that's the case for or first example. We're going to look at teacher attendance data for the School District of Philadelphia.

The school district releases, through their Open Data Initiative, a whole bunch of data files related to the topics like school catchment zones, employee salaries, budget details, etc. All of the data can be found here: http://webgui.phila.k12.pa.us/offices/o/open-data-initiative.

We could use `R` to download the files (we may touch on this in the afternoon), but to ease us into things, I've gone ahead and downloaded some files and put them on my GitHub page. Let's look at the data from 2013-14. [Here's a link to the data](https://raw.githubusercontent.com/MichaelChirico/iesRTutorial/in_progress/data/School%20Profiles%20Teacher%20Attendance%202013-2014.TXT), but it should also be loaded on your machine.

We can preview the file using the terminal on a Mac. Learning to do some basic stuff in the terminal is a great investment of your time, but that's not the focus of today's lesson, so we'll just stick with doing things through R. We can send commands to the terminal in R using the `system` command. The terminal command `head` (which is also a function in R, but only applies to objects already loaded) will spit out the first couple lines of a file so we can see what it looks like without having to load the whole thing (which, for larger files, can be time-consuming).

```{r head}
#Just to flex some of R's muscles, we'll find
#  the data file from within R. As often as not,
#  we'll just navigate to the file ourselves and
#  just copy-paste the file path.
#  We're looking for the folder with the data
#  so we use include.dirs to include directories
list.files(include.dirs = TRUE) 

#I've kept things in the data folder here. We
#  can tell it's a folder as it has no extensions.
#  We can find the files in that folder using a 
#  relative path (./ means starting from the 
#  current folder, enter the folder after /).
#  We use the full.names of the file for the next step.
list.files("./data", full.names = TRUE)

#Now we see the data files. The one we're after
#  currently is "School Profiles Teacher Attendance 2013-2014.TXT"
#  To get it previewed, we send the command head to the system
#  and follow it with the quoted file name (since the
#  file name has spaces, which is a pain)
command = paste0('head ', '"./data/School Profiles Teacher Attendance 2013-2014.TXT"')
```
```{r system_uneval, eval = FALSE}
system(command)
```
```{r system_eval, echo = FALSE}
#See: http://stackoverflow.com/questions/27388964
#  seems I have to jump through hoops to get system to work with knitr
getwd()
cat(system(command, intern = TRUE), sep = "\n")
```

We can see that even though the data has extension .txt, the file itself is very regular and clearly comma-separated.

### `fread`/`data.table`

To read in this data, I'm going to eschew the standard intro-to-R suggestion to use `read.csv` because we're presented now with the perfect opportunity to introduce to you the crown jewel of R -- `data.table` (full disclosure, I am an active contributor to it, but this has followed from my love for it).

`data.table` is a package created by Matthew Dowle and actively maintained by him, Arun Srinivasan, and Jan Gorecki. Basically, it's designed to make working with data in R _very straightforward/intuitive_ and _incredibly fast_ (since it's designed to handle data with tens of millions of observations). We'll be spending a lot of time working with `data.table` today, and picking most of it up as we go along. You can read more on the [`data.table` homepage](https://github.com/Rdatatable/data.table/wiki) when you get a chance.

The first and most famous tool of `data.table` is `fread`. The "f" means _**FAST**_. `fread` is a highly optimized tool for reading data into R at blitzing fast speeds. You'll barely notice the difference for typical files (less than 100,000 observations), but when you _do_ notice the difference, it's a sight to behold.

On your own machine, you'll need to install `data.table` (luckily for us, it's already been pre-installed on your machines for today) using:

```{r install_data.table, eval = FALSE}
install.packages("data.table")

#the more confident and ambitious can install
#  the development version of the package,
#  which tends to have more features, but may
#  occasionally be more error-prone.
#  Caveat coder.
install.packages("data.table", type = "source",
                 repos = "https://Rdatatable.github.io/data.table")
```

Once we're sure it's on our machine, we can read the data by running:

```{r fread}
#load the data.table package
library(data.table)

attendance <- 
  fread("./data/School Profiles Teacher Attendance 2013-2014.TXT")
```

Just like that!

Entering the data variable by itself will invoke the `print.data.table` command and display a preview of the data that was loaded:

```{r print.data.table}
attendance

#we can also explicitly invoke print,
#  which has a few more bells and whistles to
#  facilitate understanding what our data.table
#  looks like.
print(attendance, class = TRUE)
```

Using the `class` argument to `print`, the preview now includes an abbreviated type under each of the column headers letting you know what `class` R thinks each column is (_**note**_ `class` is a recent update to `data.table`; for the moment, it is not available in the version on CRAN, but should be available there soon).

What about `attendance` itself? It looks like something we've not seen yet. Let's explore:

```{r explore_data.table}
class(attendance)

str(attendance)

is.list(attendance)
```

OK, lots of important stuff here. We've discovered a new type of object! `attendance` is a `data.table`. You can think of this as being a single sheet of an Excel workbook. It is specific to the `data.table` package, and is an improvement on the `data.frame` type found in base R (we'll see this type a bit later).

As far as how R thinks of it, a `data.table` is a `list` on steroids. Each element of the `list` is a column, and each element has the same number of items. We can see from `str` that, like a `list`, we can use `$` to extract items (here: columns):

```{r extract_column}
attendance$SCH_TEACHER_ATTEND
```

### Exploring Attendance Data

What are the columns in this data? We usually hope we have a data dictionary, but I didn't find one on the SDP website. It appears that `ULCS_NO` is the same as `SCHOOL_ID` (padded with a `0`). It's not clear if the school district is anonymizing the schools or if we simply have to match the `SCHOOL_ID` to the school's name in another file.

The meat of the data are the two columns `SCH_TEACHER_ATTEND` and `SDP_TEACHER_ATTEND`. The former is the attendance rate of teachers at a given school; the latter is the average for the whole district (which is why it's the same in every row).

We can now start to explore the data. The only real column of interest is `SCH_TEACHER_ATTEND`. With this, we can explore how teacher attendance varies across the city.

Interacting with a `data.table` for exploration involves accessing it with `[]`. Within `[]`, there are _**three**_ _main_ arguments:

 1. `i`: which _rows_ do we want? (**subsetting**)
 2. `j`: which _columns_ do we want? (**selection** and **manipulation**)
 3. `by`: **grouping** our operations in `j` _by_ categories within a column.
 
 The `i` and `j` notation come from thinking of a `data.table` as a matrix. To express the `i`th row and `j`th column of a matrix `A`, math people often write `A[i,j]`.
 
 Let's see the first two in action in exploring our first data set

```{r attend_summary}
#Summarize the variable to get a quick understanding
#we're looking at ALL rows, so we leave the first argument BLANK
#we're trying to summarize SCH_TEACHER_ATTEND, so we
#  use the summary function in the second argument
attendance[ , summary(SCH_TEACHER_ATTEND)]
```

So most schools are within 2 percent of the district average. What about schools that are particularly high or particularly low?

```{r attend_high_low}
#We are focusing on certain rows, so we
#  say the condition defining those
#  rows in the first argument
attendance[SCH_TEACHER_ATTEND < 90]

#And schools with particularly high attendance
attendance[SCH_TEACHER_ATTEND > 97]
```

Without knowing more about the schools, all we can say is that there are a certain number of schools with very high or very low attendance. 

### Teacher Salaries

Another file posted to the SDP website contains information on teachers' salaries. I've again downloaded this data already for you, but if you're curious [here's](http://webgui.phila.k12.pa.us/offices/o/open-data-initiative/documents/employee---20160401.zip) a link to the data from the website.

This one actually comes with a data dictionary of sorts in the form of a README file. We can use R to display the README like so:

```{r readme, warning = FALSE, comment = ""}
writeLines(readLines("./data/README_SDP_Employee.txt"))
```

`readLines` is a function used to bring in text files to R objects. It returns every line of the file as one element of a `character` vector. `writeLines` is typically used for _creating_ text files, but if we don't tell it a file to use, it prints the output to the console.

The file describes all of three of the files that came with the download; of importance is `employee_imformation.csv`. We see here the columns are all described, which will help us understand what we see when we load in the data. Let's do it!

```{r read_salaries}
salaries <- fread("./data/employee_information.csv")
print(salaries, class = TRUE)

#it's a pain to have to lay on the SHIFT key all the
#  time, so let's rename the columns so that
#  are in lower case. We need three functions:
#  tolower, which takes all upper-case letters in a 
#  string and replaces them with their lower-case
#  counterpart; names, which returns the names of
#  any object (vector, list, data.table, etc.);
#  and setnames, which has three arguments:
#  1) a data.table, 2) the columns to rename, and
#  3) their replacements. If we only use two arguments,
#  R assumes we want to replace _all_ column names, and
#  that the second argument is the replacement values.
setnames(salaries, tolower(names(salaries)))

#checking the result:
salaries
```

This is a much bigger data set -- covering all employees of the SDP and giving many more details about them. Let's explore a bit:

```{r explore_salaries}
#table is a great function for describing discrete variables.
#  it gives the count of observations in each cell, and can also
#  be used to create what Stata calls two-way tables.
salaries[ , table(pay_rate_type)]

salaries[ , table(organization_level)]

#a cross-tabulation of these two; the first argument will
#  appear in the rows, the second in the columns
salaries[ , table(pay_rate_type, organization_level)]

salaries[ , table(organization_level, gender)]
```

Let's say we're only interested in salaried employees. How do we get rid of the hourly/daily employees?

Unlike in Stata, where we'd use `drop` and would have to jump through hoops to recover the lost observations (in case we decided to expand our focus later in our analysis), we can simply create a new `data.table` in R that contain only the observations we care about. This is, in my opinion, one of the most salient examples of a feature of R that blows Stata out of the water. We can keep many data sets at our disposal at once, without having to unload/reload the ones we're not currently using. This will prove _especially_ helpful when it comes to data cleaning/merging/reshaping.

```{r salaried}
salaried <- salaries[pay_rate_type == "SALARIED"]
```


Even more useful may be to exclude anyone who's not a teacher. First, we have to exclude anyone who's not a teacher. How do we figure out who's a teacher?

Approach:

 1. Count the number of observations in each category of `title_description` (we can see from the preview that there are at least some teachers under `TEACHER,FULL TIME`)
 2. Sort these categories to find the most common categories, since presumably teachers are by far the most common SDP employees.
 
Carrying this out will introduce a few new things:

 - Grouping. We're going to *group by `title_description`*, and count observations in each group. If I remember correctly, in Stata, this would be like `by title_description: count`. When `data.table` does this, you should think of your `data.table` being split into a whole bunch of smaller `data.table`s, each of which has all of the observations for exactly one level of `title_description`.
 
 Here's an illustration:
 
 ![group_split](http://i.imgur.com/LUpLn1V.png "grouping split")
 
 - `.N`. `data.table` uses the abbreviation `.N` to stand for "number of observations". It is defined _locally_, within each group, so all of the many `data.table`s mentioned above has its own `.N`. This is basically like `count` in Stata.
 - `order`. We can arrange the rows of a table by using `order` in the first argument (`i`); use a minus sign (`-`) to go in decreasing order, otherwise it's increasing order (like we saw with `sort`).
 - Chaining. We can tack on more and more `[]` to any given `data.table` operation to hone or continue or adjust our outcomes beyond what we could do in a single call to `[]`. The general syntax is `DT[...][...][...][...]`, where each `[...]` consists of doing some manipulation/subset/aggregation, etc.

```{r find_teachers}
#Let's take it step-by-step
# 1: Find the count of employees in each category
salaried[ , .N, by = title_description]

# 2: reorder these to find the most common
salaried[ , .N, by = title_description][order(N)]

# 3: put it in decreasing order
salaried[ , .N, by = title_description][order(-N)]

# 4: Only look at the top 20 most frequent positions
# (idea -- once it's resorted, these are simply
#  the first twenty rows of the sorted table)
salaried[ , .N, by = title_description][order(-N)][1:20]
```

We can start to see many interesting facets of employment at SDP here. Almost 300 school police officers but only 180 nurses. Ripe ground for exploring on your own!

I'll continue focusing on teachers; it appears all but the special ed. teachers are grouped under `TEACHER,FULL TIME`, so we can subset:

```{r get_teachers}
teachers <- salaried[title_description == "TEACHER,FULL TIME"]
teachers
```

### Group mean salaries -- various cuts

Now we can explore data by group. How does pay differ by gender?

```{r gender_gap}
#simple to code!
teachers[ , mean(pay_rate), by = gender]

#mean automatically got called V1. If we want
#  a more friendly output, we have to change a little:
teachers[ , .(avg_wage = mean(pay_rate)), by = gender]
```

So in teaching in Philly, actually women earn significantly more than men (this is indeed significant; I'll show you how to test that in a bit)!

(Of course men and women with the same experience and certification are paid basically the same -- it's written explicitly in the teachers' contract -- but we don't have any experience or certification measures to control for this)

How does pay differ by organization level?

```{r organization_gap}
teachers[ , .(avg_wage = mean(pay_rate)), by = organization_level]
```

Which school has the best-paid teachers?

```{r rich_school}
teachers[ , .(avg_wage = mean(pay_rate)),
         by = home_organization_description
          ][order(-avg_wage)]
```

This looks fishy! It's rare for any average worth knowing to come out exactly to an integer. I suspect there are very few teachers at these schools. How can we check?

```{r rich_school_2}
#remember our handy friend .N!!
teachers[ , .(.N, avg_wage = mean(pay_rate)),
          by = home_organization_description
          ][order(-avg_wage)]
```

In fact, almost all of the "best-" and "worst-" paid schools are outliers because there's only one employee listed. So that's not really a fair measure of dominance.

So what can we do to improve our measure?

What's typically done is to exclude all schools that don't have pass some cutoff minimum number of employees. For this exercise, let's say we want to exclude all schools that have fewer than 10 full-time teachers in the data.

The way to do so is pretty natural. Remember that when we use `by`, we're essentially splitting our main `data.table` up into `r teachers[ , uniqueN(home_organization)]` smaller `data.table`s, one for each school. We want to keep only those sub-`data.table`s that have at least 10 teachers -- i.e., they have at least 10 rows, i.e., `.N` is at least 10:

```{r rich_school_3}
#No longer keep .N as an output -- this is just for focus,
#  since we already know all the schools have at least
#  ten teachers
teachers[ , if (.N >= 10) .(avg_wage = mean(pay_rate)),
          by = home_organization_description
          ][order(-avg_wage)]
```

Here we see our first `if` statement in R. The basic form of an `if` statement in R is:

```{r if_statements, eval = FALSE}
#Note the parentheses around the logical!! Very easy to forget
if (some_logical){
  # what to do if true
} else {
  # what to do if false
}
```

It's important to note that `some_logical` has to be either `TRUE` or `FALSE` -- it can't be a vector!! This sounds easy now but it's guaranteed to trip you up at some point.

There's no `else` statement in the condition we used above. So, for all the schools where there are fewer than 10 teachers, nothing happens (technically, the statement returns `NULL` -- but don't worry about that for now), and `data.table` ignores any such sub-`data.table`.

I don't really know what `NON-PUBLIC PROGRAMS` means... but besides that, we see a lot of staple high school names. Of course, all teachers in Philly are on the same teachers' contract and hence the same pay scale -- so what this exercise has _really_ told us is that South Philly high has the most experienced and/or certified teachers, and that teachers at Dunbar are young and/or uncertified.

We could go on exploring / slicing&dicing this data all day, but I think we've learned enough essentials of manipulation/inspection for now. We'll pick up more tricks as we move along.

***
## Merging

Recall our teacher attendance data:

```{r attendance_redux}
attendance
```

We couldn't really tell anything about the schools because we didn't know what the school codes meant.

Well, it appears we've found the Rosetta Stone in the teacher salary data. The `home_organization` field looks very much like the `ULCS_NO` field in the attendance data.

Obviously, it'd be ideal to _know_ for sure that they represent the same ID for each school. In lieu of that, we can bolster our confidence in the alignment by comparing the fields a bit. Here are some techniques:

```{r compare_ids}
#We know from attendance there are 212 schools.
#  How many unique schools are represented in the teacher data?
teachers[ , uniqueN(home_organization)]

#How many of the ULCS_NO values are also found in home_organization
#  intersect find the middle of the venn diagram for the 
#  first and second arguments
length(intersect(attendance$ULCS_NO, teachers[ , unique(home_organization)]))
```

That's pretty convincing if you ask me.

So, how do we match the two data sets? In Stata, depending on which data set you currently have in memory, we would either perform a one-to-many or a many-to-one merge.

In R, merging either way is a cinch. But first, we need a few more tools.

### `:=` - Adding a column to a `data.table`

When we want to add a new column to an existing `data.table`, we use the `:=` operator `j` (the second argument). A quick aside, this is done _by reference_, which means it's **memory-efficient** and **fast**; see [this Q&A for more details](http://stackoverflow.com/questions/7029944/when-should-i-use-the-operator-in-data-table).

Let's try it out with two examples. Right now, in `attendance`, teacher attendance rates are listed as a percentage (out of 100); sometimes, it's more convenient to have percentages as a proportion (out of one).

Also, we might like to know the _relative_ attendance rates -- the ratio of attendance at a given school to average attendance.

In `teachers`, `gender` is stored as a `character` -- either `M` or `F`. But it's often quite convenient to have binary variables stored as `logical` -- for subsetting. 

```{r defining}
#add attendance as a proportion
attendance[ , attendance_proportion := SCH_TEACHER_ATTEND / 100]

#add relative attendance
attendance[ , attendance_relative := SCH_TEACHER_ATTEND / SDP_TEACHER_ATTEND]

#a quirk of data.table is that after using
#  :=, we sometimes have to force printing,
#  which we can do by appending [] to the end
attendance[]

#add logical version of gender
teachers[ , male := gender == "M"]

#now, subset to show only male teachers
teachers[(male)]
```

### Merge and update

Now we're equipped to understand the syntax for a merge. Let's add the school's name (`home_organization_description` in `teachers`) to the attendance data.

First, note that `teachers` has more data in it than we need. There are many teachers in each school, meaning the mapping between `home_organization` code and school name is repeated many times (once for each teacher in a given school). This presents a sort of an issue when we try and merge the `teachers` data into `attendance` -- for each `ULCS_NO` that gets matched to a `home_organization` in `teachers`, there will be many rows, even if they all have the same information. To deal with this we'll use `unique` to eliminate all the duplicates. We'll go step-by-step first, then all at once:

```{r merge_school_name}
#this will take the FIRST row associated
#  with each school. This is important in
#  general, but not here since we only
#  care about the school-level data for now,
#  which is the same in all rows.
schools <- unique(teachers, by = "home_organization")
schools

#now merge, using := to add the school name column
attendance[schools, school_name :=
             home_organization_description, 
           on = c(ULCS_NO = "home_organization")]
attendance
```

This general form of the syntax we just used is `X[Y, variable_in_x := variable_in_y, on = c(a = "b")]`. Let's break that down a bit:

 - `X` and `Y` are `data.table`s. We match their rows according to alignment in the variables specified by the `on` argument within the square brackets (`[]`) used to access `X`.
 - `on = c(a = "b")` tells R that rows in column `a` in `X` (both things on the _left_) should be matched & associated with rows of the same value in column `b` in `Y`.
 - we add the variable on the left of `:=` to `data.table` `X`. The variable on the right of `:=` is typically found in `Y` -- it could also be an expression consisting of columns in `X` _and_ `Y`.
 
 Let's explore a few other ways we could have gone about doing this merge. First, let's remove the column that we just created so that we're really starting from scratch. The way to remove columns from a `data.table` is to set them to `NULL`, which is _very_ fast.
 
 
```{r merge_alternatives}
attendance[ , school_name := NULL][]

#alternative 1: skip defining schools
#  extra tidbit: sometimes, both the 
#  main table (x) and the merging table
#  (in the first argument, i -- we called
#   it Y above) have columns with the same
#  name. To overcome the ambiguity this 
#  engenders, we can prepend i. to the
#  column names in teachers to tell R
#  that we're referring _explicitly_
#  to that column in teachers. Similarly,
#  we can prepend x. to tell R that we're
#  referring to the column in attendance
attendance[unique(teachers, by = "home_organization"),
           school_name := i.home_organization_description,
           on = c(ULCS_NO = "home_organization")][]

#reset again
attendance[ , school_name := NULL]

#alternative 2: use unique within the merge
#  by = .EACHI is a syntax unique to merging.
#  Using this tells R to do the thing in j
#  within each group that's matched.
#  So, here, each ULCS_NO gets matched to many
#  home_organization rows in teachers.
#  within this group, we use unique to
#  get the school's name, since unique turns the
#  vector of school names (one per teacher in
#  each school that's matched) into a single value.
attendance[teachers, 
           school_name := unique(home_organization_description),
           on = c(ULCS_NO = "home_organization"), by = .EACHI][]

#reset again
attendance[ , school_name := NULL]

#alternative 3: like 2, except use [] instead of unique.
#  This alternative works, and is faster, but is less robust
#  to finding mistakes in your data. Thankfully, this data is
#  pretty clean, but we often find messy data in the wild --
#  maybe there was a typo in the organization code or school name,
#  and instead of each home_organization being matched to
#  exactly one home_organization_description, we might find
#  it matched to several (e.g., "PS 131" and "PS #131").
#  If we use the unique approach from alternative 2,
#  we'll get an error, which tells us we need to examine our data
#  more closely. This approach will definitely not generate an error.
attendance[teachers,
           school_name := home_organization_description[1],
           on = c(ULCS_NO = "home_organization"), by = .EACHI][]
```

It's up to your taste which approach suits you -- each is perfect for a certain situation.

### Missing Data

No data set is perfect. In fact, most data sets are nightmares. Missing data is pervasive, so it stands to reason that R is well-equipped to handle missing data.

All missing data is coded in R as `NA`. That is, if we've been careful when loading the data into R, usually by telling R what missing data looks like in a given file. Take `fread`, for example. We haven't needed it yet because the two data sets we've loaded have been complete/balanced, but `fread` has an argument called `na.strings` which is used to tell R what to look for in a data file to consider as missing (i.e., as `NA`).

We now have some missing data after our merge, however. To see this, note the following:

```{r show_na}
#is.na returns a logical vector saying
#  whether each element of school_name
#  is missing (NA) or not
attendance[is.na(school_name)]

#NOTE: TESTING NA WITH EQUALITY DOESN'T WORK
attendance[school_name == NA]
```

So what's up with `ULCS_NO` `2140`? Why does this school have no teachers in the salary data??

Turns out, it _is_ in the salary data, but none of the teachers are listed as `TEACHER,FULL TIME`:

```{r show_masterman}
#remember, salaries is the full set
#  of ALL employees of SDP -- we
#  didn't remove any observations from here
#(using the nrows argument for print to condense output)
print(salaries[home_organization == "2140"], nrows = 10)

#since they're not full-time teachers, what are they?
salaries[home_organization == "2140",
         table(title_description)]
```

That's right -- at Masterman, none of the teachers appear to be classified as such. They're classified for wage purposes as `TEACHER,DEMONSTRATION`. I don't really know what this means... I was only able to find a [small tidbit on the SDP website](http://webgui.phila.k12.pa.us/offices/e/ee/opportunitites/demonstration-teachers/demonstration-teachers-positions/demonstration-teacher-of-elementary-education):

> Demonstration Teachers are unique educators. They utilize the most current educational techniques and instructional media, which contribute to effective teaching practices in their specific teaching areas. They are observed at times by personnel from within the School District as well as visitors from other areas and institutions, both national and international.

This appears to be something basically exclusive to Masterman, with a fair presence at John Hancock as well:

```{r other_demo_teachers}
salaries[title_description == "TEACHER,DEMONSTRATION",
         .N, by = home_organization_description]
```

So, it turns out we'd be better off just using `salaries` for the merge to define the school's name:

```{r re_merge}
attendance[salaries, 
           school_name := unique(home_organization_description),
           on = c(ULCS_NO = "home_organization"), by = .EACHI]
attendance[is.na(school_name)] #fixed!
```

Now we can repeat our earlier exercise and reveal the names of the best- and worst- attended schools in Philly. While we're at it, we can introduce column subsetting. To focus attention on only a few columns, we can simply put them in `j` and wrap them in `.()`. Only the columns we name here will show up in the output (so that it doesn't take up as much room and we can immediately focus on the most important information)

```{r call_out_attendance}
attendance[SCH_TEACHER_ATTEND < 90 | 
             SCH_TEACHER_ATTEND > 97
           ][order(-SCH_TEACHER_ATTEND), 
             .(ULCS_NO, school_name, SCH_TEACHER_ATTEND)]
```

How fitting! Motivation High School appears to value teacher attendance very highly.

## Non-.csv Data, Part I: Excel Files

Often, the world is cruel and we are unable to find the data that we want to use for our project in a standard character-separated format. When this situation arises, we have two options.

The first is to simply convert that data into our favorite format and treat the new file as the master data file, never looking back.

There are three major problems with this approach:

 1. Processing time. It can be time-consuming to convert many files; this is a fixed cost, but often our time is better spent analyzing the data than toying with it and tinkering with the parameters of a data converter.
 2. Data fidelity. All data converters have their flaws. Encoding issues, unexpected data types, you name it. It's almost impossible to tell when, in the process of converting your data, some observation's entries have been mistakenly changed without your knowing.
 3. Software licenses. Some files are stored in program-specific binary file formats (SAS, Stata, even R all have specific file types). This is intended to make it really easy to work from session to session within that program, and re-loading a data environment is fast and simple. Moving to machines that don't have that software installed, however, can be nightmarish.
 
My preferred approach is to use R itself to read in file in _all_ formats. Users of R have encountered all of our problems before, and many kind souls out there in the ether have been so generous as to donate their tailor-made solutions to the R community.

Excel files are an example of one such foreign format. As you can imagine, Excel files are ubiquitous. It's no surprise, then, that CRAN is _plastered_ with different packages designed to load and interact with data created and stored in Excel formats. See [this Q&A](http://stackoverflow.com/q/6099243/3576984) for the gamut of options.

Today we'll cover just two options, since I have found them to be quite effective and have never needed any of the others. The first is the package `readxl`. It is designed to be streamlined and fast. It has few bells and whistles and is not very flexible. To handle more fringe/out-of-the-ordinary data files, there is `xlsx`. The main advantage I've found for `xlsx` is that it's better at properly reading fields formatted as dates, which is a [complicated facet of how .xlsx files are stored](http://www.cpearson.com/excel/datetime.htm).

On our own machines, we would install them like we would any other package:

```{r install_excel, eval = FALSE}
#we can install multiple packages at once by
#  passing a character vector to install.packages
install.packages(c("xlsx", "readxl"))
```

### School Demographic Data

Philly's ODI also releases some Excel files summarizing some basic demographic information about each school. This data, as usual already on your machine, was downloaded from [here](http://webgui.phila.k12.pa.us/offices/o/open-data-initiative/documents/enrollment-and-demographics_20160125.zip).

R doesn't have any easy facility to preview the data in an Excel file, so we'll just pause from R for now and check it out in Excel.

Excuse us for a moment, R.

`<<R sips coffee>>`

As we can see, the data is a tad messy, but R has the strength to deal with it! Let's go!

```{r readxl, results = "hide"}
library(readxl)
#As we saw, this file has many sheets. the read_excel function
#  has an argument sheet which we use to specify the sheet we want
#Also, as we saw, there's a bunch of extraneous rows at the top of the file.
#  We use the skip argument to ignore them. But in so doing, we necessarily
#  lose some information (namely, which columns correspond to Female and
#  which correspond to Male).
#To overcome this, we're going to supply
#  the column names ourselves. This is also favorable since the column
#  headers in the raw data file are plagued by spaces -- which makes
#  them quite hard to deal with once they're brought into R.
school_gender <-
  read_excel("./data/2015-2016 Enr Dem (Suppressed).xls", 
             sheet = "Gender", skip = 6,
             #enter a blank for the first column, which
             #  will be dropped anyway
             col_names = c("", "school_id", "school_name", "grade",
                           "total_enrolled", "count_female",
                           "pct_female", "count_male", "pct_male"))
```

#### `setDT`

Now that we've ventured outside of the `data.table` world, we'll have to deal with a minor annoyance -- `data.frame`s.

`data.frame`s are R's native data management class. I find them to be a pain to deal with; for example, if we `print` a `data.frame`, it will print the _entire_ table, instead of truncating all but the beginning and end of the object, which is basically a useless overload of information. Also, most of the handy syntax that we're starting to get used to from `data.table` is inaccessible/doesn't work in `data.frame`s. `data.frame` syntax is ugly.

Luckily for us, `data.table` has a perfect tool for people like us that are lost in the `data.frame` wilderness: `setDT`!

`setDT` will do its best to convert non-`data.table` objects to `data.table`s _by reference_, which means that there's no copy being made. This is probably meaningless to you now, but just know that if you have a huge data set (millions of observations), making a copy of your data is something you should try and avoid doing at all costs, as it's very time- and memory-consuming. 

```{r setDT}
class(school_gender)
#(usually, I just do this wrap this
# around read_excel to do it all at once)
setDT(school_gender)
class(school_gender)
```

#### Forcing numeric: `lapply(.SD, ...)`

```{r gender_preview}
print(school_gender, class = TRUE)
```

We've got some issues to deal with.

 1. There's too much data here (for what we've got in mind) -- we're only interested in the school-wide demographics for now. So we've got to cut all the rows where `grade` is not `ALL GRADES` (we know how to do this already)
 2. A couple of columns that are clearly numeric are currently being stored as `character` -- `count_female`, `pct_female`, `count_male`, `pct_male`. This is because some data has been suppressed, probably for privacy reasons when the sample size is too small (e.g., at Philadelphia Virtual Academy, it appears there are only 46 students, which would mean roughly 23 students of each gender -- it's common to hide data that comes from such small samples for privacy). We could have used the `na` argument in `read_excel` (setting it to `"s"` instead of the default `""`), but cleaning up ex-post allows us to show some new tricks!
 
 
```{r gender_cleanup}
#subsetting
school_gender <- school_gender[grade == "ALL GRADES"]

#forcing numeric -- note the warning telling us 
#  there is some data loss because R doesn't know
#  how to convert "s" to a number, so it forces it to be NA
school_gender[ , count_female := as.numeric(count_female)]

#repeat for the other columns
school_gender[ , pct_female := as.numeric(pct_female)]
school_gender[ , count_male := as.numeric(count_male)]
school_gender[ , pct_male := as.numeric(pct_male)]
```

It's kind of a pain to run basically the same line of code over and over again, isn't it?

Don't worry, R has a simple way to automate this process for us, of course!

We'll need two more bits of knowledge about `data.table` before we proceed.

First, there's another `data.table` variable like `.N` that is useful to know. `.SD` (stands for **S**ubset of the **D**ata), like `.N`, is two things, depending on whether we doing a grouping operation using `by`. If we're not using `by`, `.SD` is just our `data.table`. If we're using `by`, there is a different `.SD` for every group (recall the idea of splitting the `data.table` into one group for each level of the `by` variable -- each of the split sub-tables has its own `.SD`) Watch:

```{r .SD}
school_gender[ , .SD]
```

`.SD` is _itself_ a `data.table`. That means we can use `[]` on it to treat it just like any other `data.table`:

```{r .SD_subset}
#the same as school_gender[1:10]
school_gender[ , .SD[1:10] ]

#get the entire first row associated with each school_id
school_gender[ , .SD[1], by = school_id]
```

This is mainly useful in conjunction with another argument we can use within `[]` with `data.table`: `.SDcols`. `.SDcols` tells R which columns to include in the current operation:

```{r .SDcols}
school_gender[ , .SD, .SDcols = c("school_id", "grade", "total_enrolled")]
```

This is mainly useful for what we're trying to do with `as.numeric`, where we want to apply the same function to many columns. Recall from earlier that the `*apply` functions are our friends when it comes to doing repetitive things. When we do `lapply(DT, sum)`, we apply `sum` to every column of `DT`. For `as.numeric`, it's much the same:

```{r lapply_SD}
#Troublesome. We don't want to convert
#  most of the columns, and we go too far here
school_gender[ , lapply(.SD, as.numeric)]

#Use .SDcols to control which columns we affect
school_gender[ , lapply(.SD, as.numeric), 
               .SDcols = c("count_female", "pct_female", 
                           "count_male", "pct_male")]

#Great, we've converted the columns to numeric, right?
#  No, not quite. We have done the calculation,
#  but we haven't _assigned_ the output to anything.
#  To do that, we need := again.

num_cols <- c("count_female", "pct_female",
              "count_male", "pct_male")
#**NOTE** we have to surround num_cols on the left with
#  parentheses so that data.table knows we're not
#  trying to create a column called num_cols.
school_gender[ , (num_cols) := lapply(.SD, as.numeric),
               .SDcols = num_cols]
print(school_gender, class = TRUE)
```

#### Merging, and Repeating

Now, we can add this data to our teacher data file from earlier. How? A merge of course!

First, unfortunately, the school codes in `school_gender` are missing the trailing 0, so we have to fix this.

```{r merge_gender}
school_gender[ , school_id := paste0(school_id, "0")]

teachers[school_gender, 
         school_pct_male := i.pct_male, 
         on = c(home_organization = "school_id")]
```

The school demographics Excel file also contains data on ELL status, IEP status, ethnicity, and economic disadvantage. The approach to adding this is basically the same, so let's do it all at once.

```{r other_demos}
#For conciseness, we'll do all of the data manipulation we did
#  in addition to reading in the gender sheet all at once here
school_ell <- 
  setDT(read_excel("./data/2015-2016 Enr Dem (Suppressed).xls", 
                   sheet = "ELL", skip = 6,
                   col_names = c("", "school_id", "school_name", "grade",
                                 "total_enrolled", "count_ell", "pct_ell", 
                                 "count_non_ell", "pct_non_ell"))
        )[grade == "ALL GRADES"
          #we're only going to keep pct_ell, so only convert that;
          #  also convert school code
          ][ , c("school_id", "pct_ell") := 
               .(paste0(school_id, "0"), as.numeric(pct_ell))]

#note: ELL status is commonly missing
school_ell[is.na(pct_ell), .N]

#merge
teachers[school_ell, 
         school_pct_ell := i.pct_ell,
         on = c(home_organization = "school_id")]

##NOW: IEP
## Note: since all we're doing is selecting a column, we don't
##   even have to store the Excel sheet as a new data set --
##   we can pass the intermediat result directly to the
##   merge with teachers, like so:
teachers[setDT(read_excel(
  "./data/2015-2016 Enr Dem (Suppressed).xls", 
  sheet = "IEP", skip = 6,
  col_names = c("", "school_id", "school_name", "grade",
                "total_enrolled", "count_iep", "pct_iep", 
                "count_non_iep", "pct_non_iep"))
        )[grade == "ALL GRADES"
          ][ , c("school_id", "pct_iep") := 
               .(paste0(school_id, "0"), as.numeric(pct_iep))],
  school_pct_iep := i.pct_iep, 
  on = c(home_organization = "school_id")]

##NOW: Ethnicity
##  Ethnicity is not binary, so things will be a little bit more involved.
##  we'll use the `:=`() form of assignment here. Basically, we can
##  treat := like a function (surrounded by backticks ``), since, 
##  technically, it IS a function
teachers[setDT(read_excel(
  "./data/2015-2016 Enr Dem (Suppressed).xls", 
  sheet = "Ethnicity", skip = 6,
  col_names = c("", "school_id", "school_name", 
                "grade", "total_enrolled", 
                "count_amerindian", "pct_amerindian", 
                "count_asian", "pct_asian", 
                "count_black", "pct_black",
                "count_hispanic", "pct_hispanic", 
                "count_multi", "pct_multi", 
                "count_pacific", "pct_pacific",
                "count_white", "pct_white"))
        )[grade == "ALL GRADES"
          ][ , c("school_id",
                 paste0("pct_", c("asian", "black", 
                                  "hispanic", "white"))) := 
               #we could do this with lapply, but it looks ugly...
               .(paste0(school_id, "0"), as.numeric(pct_asian),
                 as.numeric(pct_black), as.numeric(pct_hispanic),
                 as.numeric(pct_white))],
  `:=`(school_pct_asian = i.pct_asian,
       school_pct_black = i.pct_black,
       school_pct_hispanic = i.pct_hispanic,
       school_pct_white = i.pct_white),
  on = c(home_organization = "school_id")]

#Whew, what a mess! If you prefer, of course, you
#  can always do this step by step.

#Lastly, economically disadvantaged.
#  this sheet is formatted a bit differently,
#  so our merge must adapt accordingly.
#  In particular, there are more blank columns.
teachers[setDT(read_excel(
  "./data/2015-2016 Enr Dem (Suppressed).xls", 
  sheet = "Econ Disadv", skip = 6,
  col_names = c("", "school_id", "school_name", 
                "grade", "pct_disadv", rep("", 4)))
        )[ , school_id := paste0(school_id, "0")],
  school_pct_disadv := 100 * i.pct_disadv,
  on = c(home_organization = "school_id")]

teachers
```

## Regressions

OK, enough data manipulation for now. Let's get to some actual analysis, for cryin' out loud!

We of course don't have any experimental or quasi-experimental data here, so we'll only be able to tease out correlations from our data. But we do have enough data to illustrate the basics of typical regression techniques, so here we go!

### OLS

Most linear regressions in R are handled by the `lm` function (stands for **l**inear **m**odel).

Let's run a very simple regression -- the most basic regression testing what the gender pay gap is. Like any of the other typical functions we've been using, we run `lm` in `j` of our `data.table`.

```{r reg_gender}
##no frills regression
teachers[ , lm(pay_rate ~ gender)]
```

The basic output of `lm` tells us 1) the regression we ran and 2) the point estimates of the coefficients. This is nice, but I find it rather uninformative -- crucially, the standard errors are missing. But fear not! We're only seeing the surface of what `lm` does.

```{r reg_gender_assign}
#now we assign the output of lm so
#  we can explore it in a bit more detail
gender_reg <- teachers[ , lm(pay_rate ~ gender)]
str(gender_reg)
```

We can see from `str` that `lm` is a `list` with a long set of components. The output is a bit information overload; we can hone in on some important details by tinkering with our `gender_reg` object some more:

```{r reg_gender_explore}
names(gender_reg)
```

Our `gender_reg` object is actually pretty complicate! Just by running `lm`, we're given back all sorts of essential results about the relationship between `pay_rate` and `gender`:

* `coefficients` are of course the coefficients of the regression. We can access them with `gender_reg$coefficients`.
* `residuals` are of course the residuals -- `actual - predicted` value for each observation.
* `effects` are "the uncorrelated single-degree-of-freedom values obtained by projecting the data onto the successive orthogonal subspaces generated by the QR decomposition during the fitting process." (I have no idea what this means)
* `rank` should be equal to the number of coefficients (including the intercept), unless there is a multicollinearity problem.
* `fitted.values` are the `y_hat`, the predicted `pay_rate` produced by the model for each observation.
* `assign` helps map the matrix of regressors to the formula (I've never used this)
* `qr` is the QR decomposition matrix associated with the regression (I've never used this)
* `df.residual` is residual degrees of freedom (`= n - k`), where `n` is the total observations and `k` is the number of regressors
* `contrasts` tell which contrasts were used (contrasts are levels of discrete variables)
* `xlevels` gives the levels of the factors (discrete variables) that we included -- for `gender_reg`, this is `F` and `M`, e.g.
* `call` stores the regression we ran as a call to `lm`
* `terms` gives a bunch of information about the variables we included in the regression -- what they are, which are factors, which is the intercept, etc.
* `model` gives the X matrix (predictor matrix)

It's wonderful to know that R keeps track of all of this stuff for us, but what about the real meat of our regression -- the stars! For this, we'll use the `summary` function, which has a method for the output of `lm` that gives a nice, easily-digested summary of the model we ran:

```{r reg_gender_summary}
summary(gender_reg)
```

This calls attention to some summary statistics about the residuals (5-number-summary, as well as residual standard erro), the predictive power (R squared, multiple R squared, F statistic), but most importantly, it prints the standard error of each coefficient, and the associated two-sided t test results for each coefficient.

Here, we can see that men are paid on average significantly less (at alpha = .05) than are women among Philadelphia teachers, by about $1,000.

We spent some time going over the details, so let's summarize quickly with a comparison to Stata. Here's the boiled-down version of getting a regression summary in R:

```{r reg_gender_minimal}
teachers[ , summary(lm(pay_rate ~ gender))]
```

In Stata, to do what we just did, one would simply run `reg pay_rate gender`. Hard to get simpler than that! My sales pitch in favor of R is only that it's not _that_ much more complicated, and as we saw above, R gives us at-hand access to a slew of summary statistics about the regression that are a bit less transparent to access in Stata.

#### Other covariates

It's not a stretch to expand our regression to include other covariates to try and tame their influence. Perfect covariates here would be experience and certification, but sadly we're not so lucky. But we do have a bunch of other school-level covariates we can include. Let's try a few:

```{r reg_gender_covariates}
#skipping right to the summary
teachers[ , summary(lm(pay_rate ~ gender + school_pct_male + school_pct_black))]
```

Note that we lost a lot of observations because `school_pct_male` and `school_pct_black` are frequently missing. This was included in the summary output: `(336 observations deleted due to missingness)`.

How do we interpret the results of this regression? The most naive possible conclusion is that mostly-black schools pay their teachers less -- i.e., that there's somehow a causal relationship between student makeup and the wages offered to teachers.

But we know that the teachers' contract is fixed district-wide, so each school has no power whatsoever (well, not quite...) over what its teachers are paid. Instead, we're witnessing the direct result of selection bias. We can get into this more later when we explore plotting.

#### Interactions

Stata handles interactions with the `#` operator. R, by contrast, uses `*`. So if we wanted to explore the interaction between `gender` and `school_pct_male` (i.e., to allow the slope of `school_pct_male` to differ by gender), we'd simply run:

```{r reg_gender_interaction}
#In Stata: reg pay_rate gender#school_pct_white
teachers[ , summary(lm(pay_rate ~ gender*school_pct_white))]
```
```{r reg_gender_interaction_hide, echo = FALSE}
x <- teachers[ , lm(pay_rate ~ gender*school_pct_white)]
female_gains = round(x$coefficients[3]/100)
interact = round(x$coefficients[4]/100)
```

The output `genderM:school_pct_white` is the interaction term -- it's the difference slope of the linear relationship between salary and student gender for female and male teachers. Specifically, the expected pay change for a female teacher from a one-percentage-point increase in the percentage of male students at her school is `r female_gains` (we divide by 100 since `school_pct_white` is measured in proportions, not percentages); for men, the expected pay change is `r paste0(female_gains, " + ", interact, " = ", female_gains + interact)`.

This is not significant, meaning that the relationship between school whiteness, gender, and pay mainly play out through student ethnicity.

#### Functions of variables

One of my big pet peeves in Stata is the inability to run something like `reg y log(x)` -- we're forced to first _create_ a `logx` variable like `gen logx = log(x)`. Now we've got two objects in memory that are basically the same information! A waste of memory, in addition to being a pain to remember to do this all the time.

R does not suffer from this inanity. We can simply use functions within the formula specification:

```{r reg_gender_log, error = TRUE}
#Let's try!
teachers[ , summary(lm(log(pay_rate) ~ gender))]

#Whoops. Turns out we still have some teachers
#  who are stored as having pay rate 0:
teachers[pay_rate == 0, .N]

#So we need to exclude them because log(0) is -Inf:
teachers[pay_rate>0, summary(lm(log(pay_rate) ~ gender))]
```

It's a bit harder to interpret the coefficients in a regression with logs and discrete covariates, but the rule is still roughly that the coefficients are percentages. So men are paid about 2% less on average in Philly.

#### More functions of variables

What if we wanted to control not for the _combined_ percentage of black and hispanic students at the school?

In Stata, we'd have to do something like `gen black_hisp = school_pct_black + school_pct_hispanic`. And from what we've seen so far, `lm(pay_rate ~ school_pct_black + school_pct_hispanic)` will give us _separate_ coefficients for black and hispanic groups.

To get variables like this (most commonly, involving simple arithmetic operations like `+` or `*`), there's the special `I` function for regression formulas. Basically, we surround some operation done on our variables with `I()`, and whatever results from that will be considered as a single covariate. Watch:

```{r reg_gender_plus}
teachers[ , summary(lm(pay_rate ~ gender + 
                         I(school_pct_black + school_pct_hispanic)))]
```

### Predictions from a model

One of the most common things to do with a fitted regression is to use it for in- or out-of-sample predictions. For this, R has the `predict` function, which takes as its first argument the result of a model (e.g., `lm`), and as its second argument a new `data.table`.

Any prediction is model dependent; we'll compare the predictions that come out of a few different relevant specifications.

```{r predict_models}
#Model 1: Simplest linear specification
reg1 <- 
  teachers[ , lm(pay_rate ~ gender + school_pct_male +
                   school_pct_black + school_pct_disadv)]

#Model 2: Some interactions
reg2 <- 
    teachers[ , lm(pay_rate ~ gender*school_pct_disadv + 
                     school_pct_male + school_pct_black)]

#Model 3: Full interactions
reg3 <- 
    teachers[ , lm(pay_rate ~ gender*school_pct_disadv* 
                     school_pct_male*school_pct_black)]
```

Let's try to predict the wages of a teacher who is male and at a school with 70% male, 40% black students, and 80% economically disadvantaged students with each model.

```{r predictions}
#declare new data to use for prediction
predict_table <- data.table(gender = "M", school_pct_male = .7,
                            school_pct_black = .4,
                            school_pct_disadv = .8)

#prediction from Model 1
predict(reg1, predict_table)

#prediction from Model 2
predict(reg2, predict_table)

#prediction from Model 3
predict(reg3, predict_table)
```

#### Many predictions

This will be more useful when we get to plotting, but often in more complicated models we want to trace out the marginal effect of a variable on our prediction, holding other regressors constant -- is the relationship monotonic? Increasing?

For this, we simply provide more than one row in the `predict_table` that we supply, e.g.:

```{r predict_many}
#explore the relationship between % male students
#  and teacher pay
predict_table <- 
  data.table(gender = "M", 
             school_pct_male =
               #only use 10 points for now for
               #  simplicity since we're
               #  not plotting
               seq(.5, 1, length.out = 10),
             school_pct_black = .4,
             school_pct_disadv = .8)

#Since all of these models are linear, 
#  we know for sure the relationship
#  is monotonic, and we can predict
#  the change from step to step because
#  we know the coefficient on school_pct_male
predict(reg1, predict_table)
```

#### Prediction intervals

Point estimates are rarely the only thing we care about -- as important are reasonable ranges in which our predictions might fall. We may predict wages of $75,000, but it would hardly call our model into question if we found actual wages of $75,001 for that person. Prediction intervals are a way of coupling our predictions with a range of estimates that are consistent with our model.

To this end, `predict` also has a few other arguments we can use to produce prediction intervals, e.g., `interval` and `level`, which tell the type of interval we want and the "alpha" to use, respectively:

```{r predict_interval}
predict(reg1, predict_table,
        interval = "prediction", level = .95)
```

That the intervals are so wide shows just how poor our chosen covariates are at predicting teachers' wages.

### Probit/Logit

The next most common form of regression analysis is logit/probit, for when we're interested in understanding predictors of a binary dependent variable. I've never heard a compelling empirical reason for using one over the other; logistic is far more common, probably because it provides odds ratios, which are infinitely more interpretable than are the plain coefficients of either logit or probit models. But the difference in R between running logit and probit is trivial, so we'll cover both here.

To avoid simply re-hashing what we did in the previous section and inducing a room more rife with glaze than Krispy Kreme, let's focus on a new variable for our left-hand side. 

Noticing that most schools are 100% economically disadvantaged, I wonder, are there certain characteristics which help predict that a teacher is at a school with less than full "poverty"?

We'll define "less than full poverty" as `school_pct_disadv < 100`, which is true for `teachers[school_pct_disadv < 100, .N]` = `r teachers[school_pct_disadv < 100, .N]` teachers (and `unique(teachers, by = "home_organization")[school_pct_disadv < 100, .N]` = `r unique(teachers, by = "home_organization")[school_pct_disadv < 100, .N]` schools).

The workhorse of binary-dependent-variable models in R is `glm`. This stands for **g**eneralized **l**inear **m**odels. Generalized linear models subsume OLS and incorporate several regression models which can be related to OLS by a "link function", including logit, probit and Poisson regression. See [Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model) for more.

`glm` works basically like `lm` -- its first argument is specified in exactly the same way (specifically, with an object of class `formula`), but we also have to include a second argument -- `family`. This is basically specifying to `glm` the type of link function that you've got in mind for your regression. There is a multitude of options -- `Gamma`, `inverse.gaussian`, `poisson`, `quasipoisson`, etc. (see `?family`), but the most common and important is `binomial`.

Technically, what we pass to the `family` argument is a `function` -- meaning most importantly that _it itself accepts arguments_! Specifically, the `binomial` function accepts one important argument, `link`. By default, this is `"logit"`, meaning that the default for `family = binomial` is to run a logistic regression. There are two other options -- `family = "probit"` (probit regression) and `family = "cauchit"` (which uses a Cauchy CDF, if you know what that is).

Let's see this in action and run our first logit and probits:

```{r logit_probit}
#We could define our own "poverty" variable,
#  but instead we'll illustrate again another
#  way to use the I function in formulas
logit <- teachers[ , glm(I(school_pct_disadv < 100) ~ 
                           pay_rate + gender + school_pct_male, 
                         family = binomial)]

summary(logit)

#run a probit instead by specifying the link
#  argument in the binomial function
probit <- teachers[ , glm(I(school_pct_disadv < 100) ~ 
                            pay_rate + gender + school_pct_male, 
                          family = binomial(link = "probit"))]


summary(probit)                  
```

We can't compare coefficients between the models, except for their signs. The signs and significance both agree (as we would hope), and again seem to be telling us stuff we may have surmised -- high-poverty schools have lower-wage teachers, have fewer male teachers, and have more male students.

To compare the models, we can look at the predicted probability of being in a low-poverty district for a typical teacher. We'll define a typical teacher as having the _median_ pay rate; we'll plug in the _percentage of male teachers_ to the coefficient on `gender`; and we'll plug in the _average_ percentage of male students.

I'm unaware of a way to tell R to use a percentage instead of 0/1 in `predict`, so we have to get our predictions by hand. Luckily this isn't so bad:

```{r predict_glm}
#here is the "typical teacher"; we
#  include 1 first to stand for the
#  intercept term
typical <- teachers[!is.na(school_pct_male),
                    c(1, median(pay_rate), mean(gender == "M"),
                      mean(school_pct_male))]

#prediction from the logit model; recall that 
#  the interpretation of logit is that:
#  P[y = 1 | X] = F(X'B)
#  where X are our predictors, F is the link
#  function CDF, and B is the coefficient vector

#In the case of logistic regression, F is the
#  logistic CDF:
plogis(sum(logit$coefficients * typical))

#In the case of probit, F is the normal CDF:
pnorm(sum(probit$coefficients * typical))

#For comparison, the overall average in the sample is:
teachers[ , mean(school_pct_disadv < 100, na.rm = TRUE)]
```

## Reshaping

Let's read the full demographic file on student gender again. This is just a copy-paste of what we did above.

```{r excel_gender_redux, warning = FALSE, results = "hide"}
school_gender <-
  setDT(read_excel("./data/2015-2016 Enr Dem (Suppressed).xls", 
                   sheet = "Gender", skip = 6,
                   col_names = c("", "school_id", "school_name", "grade",
                                 "total_enrolled", "count_female",
                                 "pct_female", "count_male", "pct_male")))
```
```{r print_gender}
school_gender

#Note that grade is pretty poorly formatted.
#  Let's fix that quickly. I'll explain what's
#  going on here later when we go into
#  regular expressions in a bit more depth.
#  Basically, we're deleting the decimal and everything after it.
school_gender[grade != "ALL GRADES", 
              grade := paste0("GRADE_", gsub("\\..*", "", grade))][]
```

Above, we were focused only on school-level demographic data, so we threw away a lot of information.

In certain settings, we may be more focused on grade-level outcomes (if, for example, we knew the _grade_ that each teacher taught, primarily, than we might want to associate the grade-level demographics to each teacher).

All of this information is here in the file we've got, but it's in a slightly unfortunate format -- there are many rows associated with each school. It's much easier to merge a `data.table` where each school is found in one, and only one, row. 

### Long to Wide: `dcast`

To accomplish this in Stata we would use `reshape wide`, like `reshape wide stub, i(i) j(j)`.

In R, we use the `dcast` function (I believe this means **d**ata **cast**, with the idea that we're casting data "out and to the side" by going from long to wide). There are three main arguments to `dcast`: `data` (the `data.table` to be reshaped), `formula` (a specification of how to reshape, akin to the `i(i)` and `j(j)` in Stata), and `value.var` (the equivalent of `stub` in Stata).

It can be confusing to wrap your head around which arguments go where; here is an illustration to help orient your thinking about a given problem:

![Dcast Illustration](http://i.imgur.com/TbHd96c.png "dcast_chart")

Basically, the variables that we want to end up as rows, we put on the _left_ of the formula (**L**eft-**H**and **S**ide), and those that we want to end up as columns we put on the _right_ of the formula. `value.var` is(are) the variable(s) dictating what comes from each LHS/RHS pair in the data.

Let's reshape the gender demographics data so that there is only one row associated with each school:

```{r reshape_gender}
school_gender_wide <- 
  dcast(school_gender, school_id ~ grade, value.var = "pct_male")
school_gender_wide
```

We now have all of the grades as columns in the output. Note how much data is missing -- this shows that most schools only cover a small range of grades. The grades are also out of order -- they've been alphabetized, and `"GRADE 10"` comes before `"GRADE 2"` alphabetically. We can fix this by using `factor`s; we may get into that later.

Note that we've lost a lot of information that was in or data originally -- the data on student counts, the name of the school, etc. We can keep this sort of thing by using `dcast` more flexibly:

```{r reshape_gender_extra}
#keep school name by adding it as an ID
#  variable; since school_name is the same
#  for every school_id, this will not
#  change our output.
dcast(school_gender, school_id + school_name ~ grade, value.var = "pct_male")

#keep the count of male students; for brevity,
#  we'll also exclude a bunch of grades to the
#  output is not as space-consuming
dcast(school_gender[grade %in% paste0("GRADE_", 1:6)],
      school_id ~ grade, value.var = c("pct_male", "count_male"))
```

### Wide to Long: `melt`

As often as we're given data in long form that we prefer in wide form, we're presented with data in _wide_ form that we'd prefer in _long_ form. Stata uses similar terminology for both -- `reshape wide` for the former and `reshape long` for the latter, but R uses a different word -- `dcast` for the former and `melt` for the latter. I picture the data set made of wax and melting away into a longer form.

`melt` also takes three main arguments -- `data` (same as before), `id.vars` (same as LHS above), and `measure.vars` (basically RHS above). `measure.vars` can be a bit tricky, and there are a few ways to use it. First we can specify the column names explicitly. Second, we can specify a pattern matched by the names of the columns we'd like. Last, we can use column numbers (this isn't recommended, so I won't demonstrate it). This gets slightly more complicated when we want to split into more than one new column.

Let's try this with our `school_gender` data. We'll stack the genders on top of each other and create a single column for student counts, then another for percentages (basically doubling the number of rows in the table).

```{r gender_melt}
#First, using explicit column names
melt(school_gender, 
     id.vars = c("school_id", "grade"), 
     measure.vars = c("count_male", "count_female"))

#It's usually more concise to use the patterns
#  convenience "function"; the syntax for this
#  generally involves regular expressions, so
#  I'll hold off on demonstrating the full
#  power of this approach for now. 
melt(school_gender, 
     id.vars = c("school_id", "grade"), 
     measure.vars = patterns("count"))

#A bit more careful about the output to make
#  it a bit more understandable
melt(school_gender, 
     id.vars = c("school_id", "grade"), 
     measure.vars = patterns("count"),
     variable.name = "gender",
     value.name = "count")
```

We can extend this approach to handle reshaping both `count` and `pct` like so:

```{r gender_melt_both}
#Admittedly a bit tough to tell whether variable = 1
#  corresponds to Male or Female
melt(school_gender,
     id.vars = c("school_id", "grade"),
     measure.vars = patterns("count", "pct"))

#If we want to use the explicitly-name approach
#  of before, we have to keep all associated columns
#  as one item of a list -- here, the first
#  item of the list is for counts, and the second
#  item of the list is for percentages.
#Advantage: we know for sure that count_male
#  and pct_male correspond to variable = 1
melt(school_gender,
     id.vars = c("school_id", "grade"),
     measure.vars = list(c("count_male", "count_female"),
                         c("pct_male", "pct_female")))

#We can again specify value.name to facilitate understanding
melt(school_gender,
     id.vars = c("school_id", "grade"),
     measure.vars = list(c("count_male", "count_female"),
                         c("pct_male", "pct_female")),
     value.name = c("count", "pct"))
```

## String Data / regex

String data is ubiquitous. And it is ubiquitously messy. Typos, transpositions, inconsistent abbreviations, strange character encodings, arbitrary punctuation... there's no end to the nightmares that can be caused to a data analyst forced to work with string data.

The most powerful tool that a data analyst can have in their belt to attack such data is a facility with _regular expressions_.

Basically, regular expressions are a language for talking to and manipulating string data. Have you ever wondered how many words in English fit some certain pattern? Regular expressions are your best friend.

What are all the words in the English language that have the five vowels in order? I used the [Regex Dictionary](http://www.visca.com/regexdict/) to search for the pattern `a.*e.*i.*o.*u.*` (we'll cover what this means momentarily) and got the following list:

`abstemious, adventitious, amentiferous, anemophilous, arenicolous, argentiferous, arsenious, arteriovenous, autoecious, cavernicolous, facetious, garnetiferous, sacrilegious, theater-in-the-round`

Just like that, instantly! Regular expressions is basically a programming language in and of itself. They can be accessed through most other programs -- Stata, Excel, SAS, MATLAB, Python, you name it. Here we'll obviously focus on their use in R.

To just introduce some of the most common things used in regex, I'll just do a bunch of quick examples demonstrating a certain type of pattern on a single string; then at the end we'll quickly apply this to some of our school data.

### `gsub`, `grep`, `grepl`, `strsplit`

These four are the workhorse functions of using regular expressions in R. They each serve a distinct purpose, and I'll try and use all four in this brief overview a couple of different times.

 1. `gsub` is used for **sub**stitution of patterns. Want to delete all numbers? Cut certain strings from names? This is your friend. A quick side note on etymology -- the "g" in `gsub` stands for **g**lobal. There is a rarely-used alternative to `gsub`, `sub`, which will replace only the first match of the supplied pattern... I've only ever used it once, just something to keep in mind.
 2. `grep` (an obscure acronym for **g**lobal **r**egular **e**xpression **p**rint, though I only ever say "grep") has two purposes. The first identifies if elements of a string match a pattern, and then returns the indices of the vector corresponding to matches. The second (with setting the argument `value = TRUE`) returns _the matched strings themselves_.
 3. `grepl` (the "l" stands for **l**ogical) is used to simply test for existence of the pattern in strings. If the pattern is matched, we get `TRUE`, otherwise `FALSE`.

### Simple, Exact Substring Matching

The most common regular expression is simply an _exact_ sequence of letters. Suppose we wanted to find all the people named Bob. Then we might do:

```{r find_bob}
strings <- c("Bob Dylan", "Johnny Cash", "Bob Marley", "Janis Joplin")

#note the order of arguments!!
#  this is a pet peeve of mine, as I'd expect
#  strings to go first.
grep("Bob", strings, value = TRUE)

firsts <- c("Bob", "Johnny", "Bob", "Janis")
lasts <- c("Dylan", "Cash", "Marley", "Joplin")
#We might use grep without value = TRUE to
#  find the last names of all Bobs:
grep("Bob", strings)

lasts[grep("Bob", strings)]

mons <- c("January", "February", "March", "April",
          "May", "June", "July", "August",
          "September", "October", "November", "December")

#Which months can we eat oysters?
grep("r", mons)
```

### Beginning/End Anchors: `^` and `$`

Sometimes we want to be sure we're matching a pattern from the beginning or end of a string. Regex uses two symbols to represent these: `^` (beginning) and `$` (end).

Let's try it out:

```{r regex_beg_end}
strings <- c("Bob Dylan", "Johnny Cash",
             "Bob Marley", "Janis Joplin",
             "Billy-Bob Thornton")

#Just matching Bob will give us the last
#  string as well:
grep("Bob", strings, value = TRUE)

#To exclude this, we can force Bob
#  to have come from the beginning
#  of the string:
grep("^Bob", strings, value = TRUE)

states <- 
  c("Alabama", "Alaska", "Arizona", "Arkansas",
    "California", "Colorado", "Connecticut",
    "Delaware", "Florida", "Georgia", "Hawaii",
    "Idaho", "Illinois", "Indiana", "Iowa",
    "Kansas", "Kentucky", "Louisiana", "Maine",
    "Maryland", "Massachusetts", "Michigan",
    "Minnesota", "Mississippi", "Missouri",
    "Montana", "Nebraska", "Nevada", "New Hampshire", 
    "New Jersey", "New Mexico", "New York", 
    "North Carolina", "North Dakota", "Ohio", 
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", 
    "South Carolina", "South Dakota", "Tennessee", 
    "Texas", "Utah", "Vermont", "Virginia", "Washington", 
    "West Virginia", "Wisconsin", "Wyoming")

#Which states start with New?
grep("^New", states, value = TRUE)

#Which states DON'T end with a?
states[!grepl("a$", states)]

#We could also have used another
#  argument of grep, invert,
#  to accomplish the same thing as
#  x[!grepl("pattern", x)]
grep("a$", states, value = TRUE, invert = TRUE)
```

### Multiple Possibilities: `|`

As typically in programming, we can use `|` to mean "or". `|` separates patterns. Let's try it out:

```{r regex_or}
#Let's get states starting with directional names
grep("North|South|East|West", states, value = TRUE)
```

### White Space: `\\s`

White space comes in many forms. Obviously the most common is a simple space. But also included in this are other sorts of blank characters that may show up unexpectedly in our data, like tabs (`\t`), newlines (`\n`), a carriage return (`\r`), a form feed (`\f`), etc. (like I said, string data is messy).

In regex in R, `\\s` matches _all_ of these (those who have seen regex before should note that we need two backslashes because the backslash itself needs to be escaped in an R string). For those interested in web scraping, unfortunately, it appears that `\\s` does _not_ match `&nbsp;`; we'll return to this later.

```{r regex_whitespace}
#Which states have a space of any sort?
grep("\\s", states, value = TRUE)
```


### Wildcard: `.`

Sometimes, we don't care _what_ character is in a certain place, just that it's there. This type of pattern is called a wildcard, and it's denoted in regex by `.`.

Actually, it's rare to use it by itself, but it'll come up a lot when we have more tools. The only thing I can think of for this alone is cheating on crosswords. Suppose we wanted to fill in a crossword answer that had the pattern `L _ V _ _ _ _ _ _`. As long as we know it's one word, we can go to the Regex dictionary and search the pattern: `l.v......` and we'll get the full list of possible answers: `lava-lava, lavaliere, leviathan, Levitical, Leviticus, liverleaf, liverwort, liveryman, livestock`. We'll use this more soon.

### Quantifiers: `*`, `+`, `?`, `{m(, n)}`

The most common appearance of `.` from the last section is as `.*`. We saw this when I found all the pan-vowel words in the introduction to this section.

`*` is a quantifier -- it quantifies the piece of the pattern directly before it. In particular, `*` says to match the thing before it _0 or more times_. So `.*` means "match _anything_ at least 0 times." So the regex in the introduction, `a.*e.*i.*o.*u` means match "'a' followed by _anything any number of times_ followed by 'e' followed by _anything any number of times_ followed by 'i' followed by _anything any number of times_ followed by 'o' followed by _anything any number of times_ followed by 'u'".

Here are all the possible quantifiers:

 1. `*`: Match _at least 0 times_
 2. `+`: Match _at least 1 time_
 3. `?`: Match _exactly_ 0 or 1 time.
 4. `{m}`: Match _exactly_ `m` times.
 5. `{m,n}`: Match _between_ `m` and `n` times, inclusive. If `n` is blank, it's _at least_ `m` times.
 
 Some demonstration:
 
 
```{r regex_quantifiers}
#What states have a doubled s?
grep("s{2}", states, value = TRUE)

#Which states have an a in them
#  between the first and the last letter?
#  (first letter is excluded because
#   we're only looking for lowercase a)
grep("a.+", states, value = TRUE)

flips <-
  c("HTHHHTHHHTH",
    "THTTHTTHHTH",
    "THTHTHHTHHHT",
    "THTHTTTHTTHTTT")

#Which sequences of coin flips had
#  at least 3 heads in a row?
grepl("H{3, }", flips)

#Let's match the middle initial K, possibly
#  followed by some punctuation
grepl("\\sK.?", c("Johnson, Mary K",
                  "Hobbes, Calvin K.",
                  "Martin, George R. R."))
```

### Group Matching: `[]`

`[]` is used for a less inclusive version of `.`. We specify the characters that we're OK with inside of `[]` and they'll be matched. Most commonly, this is used for matching _any letter_ or _any number_. To do this, we use `-` inside `[]` to mean "through", like `[A-Z]` (A through Z):

```{r regex_group}
#Let's match ANY middle initial
nms <- c("Johnson, Mary K",
         "Hobbes, Calvin K.",
         "Martin, George R. R.", "Obama, Barack",
         "Prince, The Artist Formerly Known As")
grepl("\\s[A-Z]\\.?$", nms)

#If we want to find anyone with a period, 
#  we have to be careful. This won't work:
grepl(".", nms)

#We CAN put a period inside a group:
grepl("[.]", nms)

#OR we can escape it with \\:
grepl("\\.", nms)

#Let's remove all digits from some addresses
gsub("[0-9]", "", c("3718 Locust Walk",
                    "219 S 41st Street",
                    "1600 Pennsylvania Ave NW"))

#Whoops, we took away too much there!
#  We also missed the leading white space
#  Let's try again:
gsub("^[0-9]*\\s*", "", c("3718 Locust Walk",
                          "219 S 41st Street",
                          "1600 Pennsylvania Ave NW"))
```

#### Specials in Grouping: `[:punct:]`, `[:alnum:]`

Also within `[]` we can refer to certain sets of characters with special shorthands. The two most useful are `[:punct:]` to refer to all punctuation (well, most -- specifically, `[:punct:]` matches all of `[-[]\;',./!@#%&*()_{}::"?]`), and `[:alnum:]` to refer to all alphanumeric characters (i.e., `[:alnum:]` is the same as `[A-Za-z0-9]`). (There are also `[:upper:]`, `[:lower:]`, `[:digit:]`, and `[:alpha:]`, but all of these are longer to write out than just writing `A-Z`, `a-z`, `0-9`, and `a-zA-Z`, respectively)

```{r regex_punct_alnum}
#get rid of all punctuation
gsub("[[:punct:]]", "", "John C. Willington, Jr.")

#get rid of all alphanumeric characters
gsub("[[:alnum:]]", "", "John C. Willington, Jr.")
```

#### Negative Group Matching: `[^]`

We can specify a group of characters _not_ to match by starting the matching group with `[^` instead of `[`.

```{r regex_group_not}
#Keep only digits
gsub("[^0-9]", "", c("3718 Locust Walk",
                     "219 S 41st Street",
                     "1600 Pennsylvania Ave NW"))
```

### Awesome Tools for Practicing

I think that's enough for today. Some other common things to look up:

* `()` to denote matched groups, along with `\\1`, `\\2`, etc.
* Look-ahead and look-behind, both positive and negative, using `perl = TRUE`
* `\\U` and `\\L` to force letters upper- or lower-case within regex

Besides that, regex is all about playing around extensively with your actual data.

And staring at a regex cheat sheet! I have this printed and pasted above my desk:

https://www.cheatography.com/davechild/cheat-sheets/regular-expressions/

I also frequently visit this website, which allows you to test regexes and breaks down the pattern step-by-step with explanations and pretty colors:

https://regex101.com/

### A Quick Data Exercise

Remember how we found out that Masterman had a strange job description for its teachers? The reason we were susceptible to this is that we relied on finding the most common job description to identify teachers. This was bound to fail if a small, specialized school like Masterman uses some alternative description on a small scale.

A better approach to tackling this problem would have been to use regex. After some brief inspection of our `salaries` data, we could have seen that `TEACHER` appears to be a constant in the job description of teachers. We can start from there and winnow down our options to only identify the teachers we're interested in, but also be more inclusive than just including `TEACHER,FULL TIME`:

```{r regex_teacher_job}
#putting grepl in the first argument means
#  we'll only get rows for which there is
#  a match to the pattern
salaries[grepl("TEACHER", title_description),
         .N, by = title_description][order(-N)]
```

On a smaller scale, it's a lot easier to piece through _every_ possible `TEACHER`-related job title. We can see Masterman's `TEACHER,DEMONSTRATION` right there as 5th-most-common. Suppose we had wanted to keep only `TEACHER,FULL TIME` and `TEACHER,DEMONSTRATION` for our `teachers` data set. We might have subsetted like this:

```{r regex_teacher_subset}
#(FULL|DEMO) matches ONLY
#  TEACHER,FULL or TEACHER,DEMO;
#  the rest of the regex is to exclude
#  TEACHER,DEMONSTRATION,SPEC ED, which
#  is distinguished by having a comma
#  later in the string, so we require
#  our match to have all the remaining
#  characters after (FULL|DEMO) to
#  NOT be a comma.
salaries[grepl("TEACHER,(FULL|DEMO)[^,]*$", title_description),
         unique(title_description)]
```

What about if we only wanted to compare the average wage of PFT employees vs. non-PFT employees? PFT has several arms/classes of employee employed by the district, so we might not want to look solely at `PFT-TEACHER`. We can do this with regex:

```{r regex_pft_wages}
#make sure we don't mix hourly employees
#  since their wages are quoted by the hour
salaries[pay_rate_type == "SALARIED", 
         .(avg_wage = mean(pay_rate)), 
         by = .(pft = grepl("^PFT", type_of_representation))]
```

Lastly, we can extract only employees working at schools (as opposed to offices) with something like:

```{r regex_schools}
salaries[grepl("SCHOOL", organization_level),
         .N, organization_level][order(-N)]
```


## Non-.csv Data

Comma- or tab-separated data is ideal, especially since we have the incredibly-fast `fread` by our side to facilitate the data reading process when we're lucky enough to have this kind of data.

But we're not always so lucky. We've already covered how to read in Excel data files. Here we'll cover three more file types which I've found frequently in the wild -- files created in SAS, files created in Stata, and fixed-width files. All are handled by the `haven` package, which is specifically designed to handle reading foreign, non-csv data types into R. 

### SAS: `read_sas`

Users of SAS may be familiar with its `sas7bdat` export file (I think it stands for "SAS 7 binary data"). I have found this file format in one place while working on projects in education -- the [Common Core of Data](https://nces.ed.gov/ccd/ccddata.asp). Many of the district- and school-level data files found there are available in `sas7bdat` format.

To just run through a quick example, let's load in the most recent LEA Universe Survey Data, from 2013-14. The original data can be found on [this site](https://nces.ed.gov/ccd/pubagency.asp) [here](https://nces.ed.gov/ccd/data/zip/ag131a_supp_sas.zip), but has been loaded onto your computer already.

Basically, reading such a file is as simple as using the `read_sas` function from the `haven` package:

```{r haven_load, eval = FALSE, cache = TRUE}
#To install from CRAN
install.packages("haven")
```
```{r read_sas}
#Now load the package:
library(haven)

leas_13 <- read_sas("data/ag131a_supp.sas7bdat")
class(leas_13)

#again, it's been read as a data.frame,
#  so we'll use setDT to make it a data.table
setDT(leas_13)

#these files are huge...
ncol(leas_13)

#so let's just preview the first, say, 30 columns:
leas_13[ , 1:30, with = FALSE]
```

### Stata: `read_dta`

I haven't come across any files in education that are only available in .dta format, so we'll just quickly digress into using non-educational data to demonstrate how to read such files in case you find them in your research.

The file I found is from [here](http://faculty.econ.ucdavis.edu/faculty/cameron/stata/stataintro.html), [this file](http://faculty.econ.ucdavis.edu/faculty/cameron/stata/carsdata.dta). Again, it's pre-loaded on your machine. 

As I mentioned, the `haven` package is designed to handle reading all sorts of foreign formats into R; Stata is among them.

```{r read_dta}
#this is a very tiny file
car_data <- read_dta("data/carsdata.dta")
#we could use setDT, etc., but we won't
car_data
```

### Fixed-width `read_fwf`/`fread` redux

Fixed-width files are basically the bane of my existence. Trouble is, I keep running into them all the time, regardless of how nightmarish they are to deal with. Especially if you have to create the width dictionary yourself.

This is essentially the case with _old_ Universe files from the Common Core. Take, for example, the [1995-96 LEA Universe Flat File](https://nces.ed.gov/ccd/data/zip/pau95data.zip), which is in fixed-width format. There is a [record layout](https://nces.ed.gov/ccd/data/txt/pau95lay.txt) on the NCES website as well... unfortunately, it, too is in fixed-width format! See what I mean about fixed-width...

To simplify our exercise a bit, instead of using the NCES files, we'll use some files from Wisconsin. The main advantage being I've already done a fair bit of data cleaning on the original raw files.

Much like Philadelphia, Wisconsin publishes data on all of its teachers. But the Wisconsin data is much more comprehensive. It's statewide, and it includes information on teachers' age, experience, fringe benefits pay, etc. The data goes back to 1994-95, but we'll just use the 1999-2000 data for now.

The data files can be found [here](http://dpi.wi.gov/cst/data-collections/staff/published-data), but be forewarned -- they're _very_ messy. Messy enough to have served as inspiration for the "Data Cleaning" section in the appendix. I've done everyone the favor of including the more manageable version in today's data files. 

#### `read_fwf`

The package we'll use for reading fixed-width files is `readr`. Let's load it:

```{r readr_install, eval = FALSE}
install.packages("readr")
```
```{r readr_load}
library(readr)
```

`read_fwf` is the function in `readr`. In addition to the column names, it requires an extra argument -- `col_positions`. This tells `read_fwf` how to find the columns in the data. It can be specified in a number of ways (see `?read_fwf`), but the one we'll use today is to use the helper function `fwf_widths`. This latter function maps column widths into something that `col_positions` can understand. It also takes an argument allowing us to specify column names, which is convenient.

```{r readr_fwf, cache = TRUE}
#The column widths are kept in a separate dictionary file,
#  which I had to create myself
fwf_dict <- fread("data/00keys.csv", header = FALSE)
teachers_wisc <-
  read_fwf("data/00staff.dat", 
           col_positions = 
             fwf_widths(fwf_dict$V2, fwf_dict$V1))
#we gotta setDT again
class(teachers_wisc)
setDT(teachers_wisc)[]
```

# Plotting (11 AM - 12 PM)

Now we can finally start working on some design!

Fine-tuning plots is a pain in any language/program. But doing basic plots in R very simple. We'll start with some easy plots and move on to some more bells and whistles. 

## `base` Plots

R comes ready and loaded with functions for all of the most common types of plots -- histograms, kernel density, scatter plots, line plots, barplots, etc. I'll go through these one by one, sprinkling in some tricks along the way which are applicable to any plotting sceneario (e.g., setting the axis labels and plot title).

After we have all of the main plot types handy, I'll move on to tiling plots (plotting more than one plot at once).

### Histograms

Histograms are probably the most common way to get a glance at the way that a variable is distributed in your data. Let's examine the distribution of birth years in the Wisconsin teacher data. 

First, a quick jab of data cleaning:

```{r wisc_data_clean}
#Only include teachers matching the following criteria:
#  * Have bachelor's or master's degree
#  * Are full-time
#  * Strictly positive salary
#  * Aren't hired by administrative buildings

teachers_wisc <- 
  teachers_wisc[highest_degree %in% c(4, 5) & 
                  position_code == "53" &
                  as.integer(salary) > 0 & 
                  substr(agency, 1, 2) != "99" & 
                  as.integer(total_exp) %between% c(10, 300)]

#Now set salaries and fringe benefits to be numeric
teachers_wisc[ , salary := as.numeric(salary)]
teachers_wisc[ , fringe := as.numeric(fringe)]

#experience is kept in units of tenth-years, for some reason
teachers_wisc[ , total_exp := as.numeric(total_exp)/10]
```

Now we can examine the distribution on a more select/interesting sample of employees. Drawing histograms is managed through the `hist` function (so it's _very_ similar to Stata). It only needs one argument -- a vector which will be converted into a distribution.

```{r wisc_age_hist, results = "hide"}
#no-frills histogram
teachers_wisc[ , hist(birth_year)]
```

That's not bad considering how simple it was to program. There are three things that jump out at me immediately as being deficiencies of this plot:

 1. The title is abstruse
 2. The axis titles are odd
 3. There's no color -- how drab!
 
Let's take care of the three of these in turn.

#### Main Title

The main title of a plot is almost always handled by the `main` argument to a plotting function:

```{r wisc_age_hist_title, results = "hide"}
#Customizing the main title
teachers_wisc[ , hist(birth_year, 
                      main = "Distribution of Year of Birth among Wisconsin Teachers")]

#May be preferable to have the title split over
#  two lines -- to do this, add \n where
#  you want the title to split
teachers_wisc[ , hist(birth_year, 
                      main = "Distribution of Year of Birth\nAmong Wisconsin Teachers")]

#To make our code less espansive, we can split the
#  title over two lines with paste0:
teachers_wisc[ , hist(birth_year, 
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]
```

#### Axis Labels

Looking better already. Now let's handle the axes. There are four arguments involved here. `xlab` and `ylab` handle the labels of the two axes. And we can use `xaxt` and `yaxt` to tell R not to plot an axis (usually this means we want to specify our own axes/ticks; we'll experiment with this later):

```{r wisc_age_hist_axes, results = "hide"}
#remember, argument order doesn't
#  matter as long as we name the arguments
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth",
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]
```

#### Colors

Now let's add some real life by coloring our plot! This is handled with the `col` argument.

There are 657 color names understood by this argument. You can see all of them by running `colors()`. More often when choosing a color, I visit [this website](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf), which has the name of all of these colors as well as a preview of the color itself. Here's the first page:

![Thanks Tian Zhang!](http://i.imgur.com/iwnA8hD.png "Thanks Tian Zhang!")

```{r wisc_age_hist_col, results = "hide"}
#something plain
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth", col = "red",
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]

#something more exotic
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth",
                      col = "darkslateblue",
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]
```

#### More Histogram Bells & Whistles

There are a bunch of other ways we can spruce up plots, but we'll save those for other plot types. For now, let's just name a few extras specifically made for histograms.

##### Density vs. Frequency

Histograms can be used to represent either frequency or probability. Basically, frequencies are raw numbers, but when plotted as probabilities, we force the total area of the histogram to be 1 (just like every random variable). We can switch to probabilities by setting the argument `freq` to `FALSE`:

```{r wisc_age_hist_freq, results = "hide"}
#note the different y axis
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth",
                      col = "darkslateblue", freq = FALSE,
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]
```

##### Customized Bins/Breaks

Actually, the histogram is a fairly well-studied (and oft-criticized) non-parametric estimator of the distribution of a random variable. There is extensive debate and argument about the proper way to construct a histogram -- how wide the bins should be, how many there should be, where the bins should be centered, etc.

By default, R uses Sturges' formula (based on the range of the data)for determining most of this. But there is also easy access to using alternative algorithms in the `breaks` argument. If we pass `"Scott"` (specifically tailored to the normal distribution) or `"FD"` (based on the inter-quartile range), we'll get a slightly different histogram based on the bins produced by these approaches. To be honest, I've never used any of these, so we'll skip them for now. 

We can also do send some other things to `breaks` -- most commonly, a single number (meaning the number of bins to be used), but also a vector (specifying the endpoints of the bins) and some other more obscure options (see `?hist` for more). Let's test these out:

```{r wisc_hist_age_breaks_inv, results = "hide", fig.keep = "all"}
#Same as above, for comparison without scrolling
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth",
                      col = "darkslateblue", freq = FALSE,
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]

#Setting the breakpoints ourself to have one for every year
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth",
                      breaks = min(birth_year):max(birth_year),
                      col = "darkslateblue", freq = FALSE,
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]

#Setting the NUMBER of bins ourself
teachers_wisc[ , hist(birth_year, xlab = "Year of Birth", breaks = 20,
                      col = "darkslateblue", freq = FALSE,
                      main = paste0("Distribution of Year of Birth\n",
                                    "Among Wisconsin Teachers"))]
```

### Kernel Density Plots

A kernel density plot is simply the continuous version of a histogram. Rather than trying to approximate the distribution of a continuous variable with a necessarily-discrete histogram, statisticians have devised kernel density estimates as a continuous approximation for continuous variables. 

#### Barebones

These are handled in R with the `density` function (_c.f._ `kdensity` in Stata). Unlike `hist`, the `density` function does not itself plot the kernel density. Instead, we have to send the output of `density` to the _main_ plotting function in R `plot`. `plot` accepts all of the same arguments that we sent to `hist` as plot-style bells and whistles.

```{r wisc_dens_age, results = "hide"}
teachers_wisc[ , plot(density(salary), main = "Density of Salaries",
                      xlab = "$", col = "darkgreen")]
```

A few things about this basic plot jump out to me and highlight some new things we can add to our toolbelt to improve the readability of this plot.

 1. The line is too thin. It's a bit hard to see.
 2. The numbers on the x- and y-axes are impossible to understand.
 
After handling each of these, we'll turn to addressing some `density`-specific parameters.

#### Line Width

Line width is handled by the `lwd` parameter. This is specified as a number; the default is 1. I usually specify `lwd = 3` in presentation-ready plots. As we draw more and more complicated plots, this is something we can use to distinguish certain parts of our plot.

```{r wisc_dens_age_lwd, results = "hide"}
teachers_wisc[ , plot(density(salary), main = "Density of Salaries",
                      xlab = "$", col = "darkgreen", lwd = 3)]
```

#### Axis Labels

R does its best to come up with nice, round, easy-on-the-eyes axis labels, but sometimes it doesn't do so well. When we notice this is the case, R has given us the tools to customize what shows up on all of the axes. Surprise, surprise, this is done through the `axis` function. `axis` takes three main arguments: 1) `side`, which tells to which side of the plot to add the axis (from 1 through 4, going clockwise from the bottom, so `side = 1` is the x axis, `side = 2` is the y axis, etc.); 2) `at`, which tells _where_ (which x or y values) to put ticks (this defaulted to `c(0, 20000, 40000, 60000, 80000)` in our original plot); and 3) `labels`, which tells _what_ to write at those ticks.

When we specify custome axis labels, we probably want to shut off the automatic axis creation done by the original plot; we can do this by specifying `xaxt = "n"` (or similarly for `yaxt`). Let's first demonstrate the effect of this:

```{r wisc_dens_age_no_x, results = "hide"}
teachers_wisc[ , plot(density(salary), main = "Density of Salaries",
                      xlab = "$", col = "darkgreen", lwd = 3,
                      xaxt = "n")]
```

If we follow a `plot` call with a call to `axis`, it will add the axis we specify to the "current" plot. Basically, the "current" plot is the one drawn the most recent time we ran `plot` (with some exceptions we won't get into today). So let's add an axis:

```{r wisc_dens_add_axis_inv, echo = -1, results = "hide"}
teachers_wisc[ , plot(density(salary), main = "Density of Salaries",
                      xlab = "$", col = "darkgreen", lwd = 3,
                      xaxt = "n")]
axis(side = 1, at = seq(0, 90000, by = 10000),
     labels = paste0("$", seq(0, 90, by = 10), "k"))
```

You'll also notice the numbers on the y axis are hard to read. To be honest, I don't think those numbers tell us much of anything, so I'm generally fine excluding the y axis on a density plot entirely:

```{r wisc_dens_no_y, results = "hide"}
teachers_wisc[ , plot(density(salary), main = "Density of Salaries",
                      xlab = "$", col = "darkgreen", lwd = 3,
                      xaxt = "n", yaxt = "n")]
axis(side = 1, at = seq(0, 90000, by = 10000),
     labels = paste0("$", seq(0, 90, by = 10), "k"))
```

#### Density Parameters

Just like with histograms, and perhaps even more so, kernel densities can be micromanaged to no end by specifying a number of parameters. See `?density` for more, as I don't think it's worthwhile to spend much time on the nuanced differences between the estimators; here, I'll just list a few options and some sample values.

* `bw`: the bandwidth. As with `breaks` from `hist`, there are a few typical algorithms available out of the box. I'll just name them here; see `?bandwidth` for more: `"nrd0"`, `"nrd"`, `"ucv"`, `"bcv"`, `"SJ"`. We can also specify bandwidth as a number ourselves; the larger is the bandwidth, the further away informative points can be.

* `kernel`: the kernel used to accrue information from nearby points. These basically differ in how much information is ascribed to very-nearby points as opposed to slightly-further points. The default in R is the Gaussian (normal) kernel (`kernel = "gaussian"`), which means R differs from Stata, which uses the Epanechnikov kernel by default. The latter has some better statistical properties, but is less appealing for other reasons. Besides Gaussian, R also supports the following values for `kernel`: `"epanechnikov"`, `"rectangular"`, `"triangular"`, `"biweight"`, `"cosine"`, and `"optcosine"`. 

### Scatterplots

Histograms and kernel densities are the main tools of examining graphically one-dimensional random variables.

Now, we'll move on to examining joint distributions graphically. The main tool people use here is the scatter plot.

Let's examine the relationship between teachers' salary and fringe benefits. This would be an essential first step in trying to understand our data if we planned to run a regression involving these two variables. Since it's not very interesting to see a scatterplot of ~65,000 points, we'll focus more specifically on English teachers (distinguished by having the variable `area` equal `300`:

```{r english}
english <- teachers_wisc[area == 300]
```

#### Barebones

Scatterplots are handled through the `plot` function. If we pass two vectors and say nothing else, `plot` assumes we want a scatterplot and treats the first vector as the "x" variable, and the second as the "y" variable. Other than that, business as usual, per above:

```{r scatter, results = "hide"}
english[ , plot(salary, fringe, xlab = "Salary ($)",
                ylab = "Fringe Benefits ($)",
                main = "Fringe Benefits vs. Salary",
                col = "darkblue")]
```

Immediately, we can see that some outliers are going to be tainting any naive analysis we may perform on the data. After fixing this, we'll learn one more general plotting trick, and one scatterplot-specific trick.

```{r scatter_out, results = "hide"}
english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary",
                         col = "darkblue")]
```

#### Axis Label Orientation

In some plots, having the y axis labels parallel to the y axis makes it harder to read. There is a parameter that controls the orientation of axis labels relative to each axis -- `las`. It can take 4 possible values -- the default, `0`, means "always parallel"; `1` means "always horizontal"; `2` means "always perpendicular"; and `3` means "always vertical". These numbers don't really correspond to anything, so far as I can tell -- in a given situation, I usually just plug in one or two of the options to see what works best. Here, we want `las = 1`:

```{r scatter_las, results = "hide"}
english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary",
                         col = "darkblue", las = 1)]

#This makes the y axis label conflict
#  with the tick labels; there's another way
#  to deal with this, but for now we can just abbreviate
#  the axis labels like we did above
english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary",
                         col = "darkblue", yaxt = "n")]
#Since we suppressed the y axis in plot, changing las
#  there wouldn't matter (since the x axis is
#  unaffected by our choice of las); to handle
#  the orientation here, we have to send our
#  las value of choice to the axis function
axis(side = 2, at = 5000*(0:5), las = 1,
     labels = paste0("$", 5*(0:5), "k"))
```

#### Conditional Coloring

One awesome thing about scatterplots is that we can set the color of each point to be different if we so choose. We can do this by supplying a _vector_ to `col` that is equal in length to our data. This is rare, but it _is_ common to set the color of each point according to some condition. The easiest way to do this is with `ifelse` (this is [inefficient](http://stackoverflow.com/questions/16275149/does-ifelse-really-calculate-both-of-its-vectors-every-time-is-it-slow), and I hate `ifelse`, but will suffice for today). Let's color the points by the degree that a teacher has.

```{r scatter_color, results = "hide"}
english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary", yaxt = "n",
                         col = ifelse(highest_degree == 4, "darkred", "darkblue"))]
axis(side = 2, at = 5000*(0:5), las = 1,
     labels = paste0("$", 5*(0:5), "k"))
```

#### Legends

The above plot clearly demonstrates that there is a strong correlation between the degree you have, the fringe benefits you receive, and your salary. But it's not at all clear to a casual observer what the difference is between red and blue dots (Republican vs. Democrat?). To elucidate this, we'll add a legend. You guessed right -- the function we need is `legend`. `legend`, like `axis`, is a plotting function that will add its output to the "current" plot. `legend` takes a bunch of parameters. The first (or first and second) specifies the position (either relative position -- e.g. `"top"` or `"bottomright"`, or, when specifying x and y, the exact coordinates of the box). The `legend` argument tells the labels in the legend.

In our case, we also need the `pch` argument, which tells the **p**oint **ch**aracter to associate with each entry (since we're making a legend about _points_ and not _lines_). The default (open circles) is `pch = 1`. Here is a chart of all the possible _numeric_ values of `pch` and their associted plotting symbol (we can also set `pch` to any single character, e.g., `pch = "x"` or `pch = "$"`):

![pch](http://i.imgur.com/X1DH6Ai.png "pch")

```{r scatter_legend_inv, echo = -(1:2)}
invisible(
  english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary", yaxt = "n",
                         col = ifelse(highest_degree == 4, "darkred", "darkblue"))]
)
axis(side = 2, at = 5000*(0:5), las = 1,
     labels = paste0("$", 5*(0:5), "k"))
legend("topleft", pch = 1,
       legend = c("Bachelor's", "Master's"), 
       #make sure to make the first color
       #  correspond to the first item in legend, etc.
       col = c("darkred", "darkblue"))
```

### Line Plots

The next very common plot type is line plots. Most often I use these for time series. Line plots are also created with `plot`, but with a different option activated -- we have to set `type = "l"`.

We'll look at how pay changes with experience. I'll also introduce a new option for inside `[]` in `data.table`: `keyby`. `keyby` is just like `by` -- it tells `data.table` how to group the operation you ask for in `j`, with the added bonus that the resultant output is _sorted by the column(s) in `by`_. We require this for creating a line plot because, when plotting lines, R considers successive elements of the input to `plot` as endpoints of a line segment -- so if the input has the "x" values out of order, the result will jump all over the screen. Let's demonstrate:

```{r mean_salary_by_exp}
#Create the average pay by experience level
teachers_wisc[ , mean(salary), by = total_exp]
```
```{r line_plot_whoops, results = "hide"}
#We then build on this output with a plot
teachers_wisc[ , mean(salary), by = total_exp
               ][ , plot(total_exp, V1, type = "l")]
```

A total mess! Because `total_exp` was out of order. Let's fix that with `keyby`:

```{r line_plot_keyby, results = "hide"}
teachers_wisc[ , mean(salary), keyby = total_exp
               ][ , plot(total_exp, V1, type = "l", 
                         ylab = "Salary ($)", col = "blue",
                         xlab = "Total Experience", lwd = 3,
                         main = "Evolution of Pay with Experience")]
```

Looking much better. I don't like those jumps. I assume it's due to some fractional values of experience, which may have a smaller sample size. To fix this, we might force experience to be an integer; let's use the floor, for example:

```{r line_plot_floor, results = "hide"}
teachers_wisc[ , mean(salary), keyby = .(exp = floor(total_exp))
               ][ , plot(exp, V1, type = "l", ylab = "Salary ($)", 
                         col = "blue", xlab = "Total Experience", lwd = 3,
                         main = "Evolution of Pay with Experience")]
```

I suspect the remaining jitters are due to most teachers retiring by 30 years' experience (so the selection effects become powerful enough to distort experience effects).

One more thing we can introduce which you might want to change about plots in general is the axis limits. These are controlled by the `xlim` and `ylim` parameters, and are automatically chosen to bracket the range of your data. However, we might want to force the y axis to include 0, for example. Both `xlim` and `ylim` take a length-two numeric vector -- the first element being the minimum of the axis, the second being the maximum.

```{r line_ylim, results = "hide"}
#force 0 at the bottom, then set the top
#  to be the highest average salary
teachers_wisc[ , mean(salary), keyby = .(exp = floor(total_exp))
               ][ , plot(exp, V1, type = "l", ylab = "Salary ($)", 
                         col = "blue", xlab = "Total Experience", 
                         lwd = 3, ylim = c(0, max(V1)),
                         main = "Evolution of Pay with Experience")]
```

We'll get into some more facets of line plotting later.

### Bar Plots

Bar plots a few common uses. Their simplest use is to demonstrate the relationship between a discrete variable and a moment (a mean, an average, a count) of a continuous variable. Let's start by showing the average pay by CESA. A CESA (Cooperative Educational Service Agency) is basically an administrative unit of school districts between the state level and the district level. Here's a map of the 12 CESAs of Wisconsin:

![CESA Map](http://i.imgur.com/NPie6OC.gif "CESA Map")

#### Categorical Bar Plot (1D)

The function used to create a bar plot (R is full of surprises) is `barplot`. It can be pretty confusing to use (as we'll see below), but for this example will be simple. We need to supply a vector to the first argument, `height`, which will be used as plot heights. We can also supply a `character` vector to `names.arg` which will help us distinguish the bars from one another. That should suffice for now:

```{r cesa_barplot, results = "hide"}
teachers_wisc[ , mean(salary), keyby = cesa
               ][ , barplot(V1, names.arg = cesa, 
                            col = "cornflowerblue",
                            main = "Average Pay in Each CESA")]
```

We could also associate colors to each CESA, as was done on the map:

```{r cesa_colors, results = "hide"}
cols <- c("01" = "khaki", "02" = "darkred",
          "03" = "darkgreen", "04" = "goldenrod",
          "05" = "cornflowerblue", "06" = "magenta",
          "07" = "navy", "08" = "salmon",
          "09" = "steelblue4", "10" = "lavender",
          "11" = "tomato", "12" = "wheat")
teachers_wisc[ , mean(salary), keyby = cesa
               ][ , barplot(V1, names.arg = cesa, col = cols[cesa],
                            main = "Average Pay in Each CESA")]
```

If you're going to be analyzing things by CESA consistently in your paper, you probably want to create a color scheme for consistency throughout your paper -- `cols` as used here is a great approach.

#### Categorical Bar Plot (2D)

Another highly-useful but somewhat-more-complicated version of the barplot is [like](http://www.eanalytica.com/blog/why-i-hate-pie-charts/) [the](http://dutchdatadude.com/i-hate-pie-charts-and-so-should-you/) [much](http://www.storytellingwithdata.com/blog/2011/07/death-to-pie-charts)-[hated](http://www.businessinsider.com/pie-charts-are-the-worst-2013-6) [pie](https://www.quora.com/How-and-why-are-pie-charts-considered-evil-by-data-visualization-experts) [chart](https://www.entrepreneur.com/article/239932).

Basically, we want to show the breakdown of some variable within each of several categories. Let's stick with CESA as our category, and see what Masters' certification rates are like in each.

In this case, we need to supply a `matrix` to height. Each column of the matrix will represent one bar. The values _within each column_ will correspond to (cumulative) _heights within each bar_. To create this matrix, we'll use `rbind`, which **bind**s vectors together as **r**ows (there's also `cbind` for columns). Let's demonstrate:

```{r barplot_2d_setup}
#We want to know the prevalence of
#  masters' certification within each CESA
teachers_wisc[ , .(bach = mean(highest_degree == 4)), keyby = cesa]
```
```{r barplot_2d, results = "hide"}
teachers_wisc[ , .(bach = mean(highest_degree == 4)), 
               keyby = cesa
               #1-bach is the masters' incidence,
               #  since there are only two degree
               #  types in our subsetted data
               ][ , barplot(rbind(1-bach, bach), names.arg = cesa,
                            col = c("red", "blue"), las = 1,
                            ylab = "Proportion",
                            main = "Certification Rates by CESA")]
```

In this case, `col` can only be specified within each bar. Since each column has two rows, we can only specify two colors. This is somewhat unfortunate; see [this](http://stackoverflow.com/questions/31840378/how-to-avoid-recycling-of-colors-in-barplot-to-achieve-different-colors-within-e) rather intricate approach for a way to get around this problem. 

#### Side-by-Side Bar Plot

As common as comparing breakdown within a category is to do so with side-by-side bars. This is almost identical to the previous case, except we add the argument `beside = TRUE`. Let's use this to see ethnicity by CESA. We'll have to take a slightly more complicated approach here, since not every ethnicity is represented in every CESA.

We'll also need a new function -- `as.matrix` -- which can be used to convert an object into a matrix. In particular, if we use `as.matrix` on a `data.table`, it will convert each column of the `data.table` into a column of the resulting matrix. If there are conflicts in data types within the `data.table`, they'll all be coerced, since a `matrix` can only store one type of data. Typically this means that if there's a `character` column, everything will be converted to `character`. 

```{r ethnicity_barplot_setup}
#First, count by CESA & ethnicity;
#  note that in, e.g., CESA 12, there are
#  no black teachers
teachers_wisc[ , .N, keyby = .(cesa, ethnicity)]

#We'll now reshape the result of this wide so that
#  the columns are CESAs, since that's how we'll
#  plot it afterwards
dcast(teachers_wisc[ , .N, keyby = .(cesa, ethnicity)],
      ethnicity ~ cesa, value.var = "N")

#Uh-oh! NAs are no good. We'll need a new argument for
#  dcast -- fill, which tells dcast what to do
#  when a certain combination is missing:
dcast(teachers_wisc[ , .N, keyby = .(cesa, ethnicity)],
      ethnicity ~ cesa, value.var = "N", fill = 0)
```
```{r ethnicity_barplot, results = "hide"}
#Now we've got a format we can work with, that
#  barplot understands
dcast(teachers_wisc[ , .N, keyby = .(cesa, ethnicity)],
      ethnicity ~ cesa, value.var = "N", fill = 0
      )[ , barplot(as.matrix(.SD[ , -1, with = FALSE]),
                   main = "Ethnicity of Teachers by CESA",
                   beside = TRUE, names.arg = names(.SD)[-1],
                   col = c("blue", "red", "darkgreen",
                           "orange", "purple"))]
#Add a legend for clarity
legend("topright", legend = c("Asian", "Black", "Hispanic",
                              "Indian", "White"),
       #pch 15 is a solid block
       pch = 15, col = c("blue", "red", "darkgreen",
                           "orange", "purple"))
```

(Wisconsin isn't very ethnically diverse...)

### Box and Whisker Plots

Box and whisker plots provide a graphical version of the five-number summary for a distribution. This is especially useful for comparing the distribution of a continuous variable with respect to a discrete variable. Above, we looked at _average_ salary and how it changes by CESA. But this might be obscuring some more interesting action going on underneath the surface. Are the different averages being driven by skewing in one direction or the other in some CESAs? A boxplot will evince this for us graphically.

In a typical show of originality, a boxplot is generated by the `boxplot` command. There are a few ways to specify your boxplot; I find the approach using a formula to be most intuitive. `boxplot` _can_ be deployed within the `j` argument to `data.table`, as above, but doing so is somewhat troublesome for reasons I don't really want to get into. Instead we'll use the `data` argument to `boxplot`; this tells `boxplot` where to find the `salary` and `cesa` variables we specify in our formula:

```{r salary_boxplot}
boxplot(salary ~ cesa, data = teachers_wisc,
        main = "Distribution of Wages by CESA",
        xlab = "CESA", ylab = "Salary ($)", col = cols)
```

`boxplot` automatically singles out the outliers in the data. However, I am unsatisfied with this plot. I don't like the orientation, and there is a simple argument we can add to make it much more informative.

I think this plot is much easier to read if the bars are stacked vertically. To do this, we use the `horizontal` argument:

```{r salary_boxplot_horiz}
boxplot(salary ~ cesa, data = teachers_wisc,
        main = "Distribution of Wages by CESA",
        horizontal = TRUE, las = 1, ylab = "CESA",
        #note we have to switch to xlab and ylab
        xlab = "Salary ($)", col = cols)
```

`boxplot` also comes with a handy argument for displaying confidence intervals around the sample medians -- `notch`. Observe:

```{r salary_boxplot_notch}
boxplot(salary ~ cesa, data = teachers_wisc,
        main = "Distribution of Wages by CESA",
        horizontal = TRUE, las = 1, ylab = "CESA",
        xlab = "Salary ($)", col = cols, notch = TRUE)
```
 
### Multiple Plots

Often we need to combine more than one plot at once to concisely present more information. This comes in two basic flavors -- adding more points/plots/lines to a single set of axes, and creating multiple sets of axes with different plots in the same output. The former is easier to handle (it's basically a matter of learning a few new functions and managing not to create a new plot window by accident).

#### More Information on the Same Axes

Let's return to the wage-tenure curve highlighted above:

```{r wage_tenure_redux, results = "hide"}
<<line_ylim>>
```

This shows the _mean_ wage at each experience level. It could also be instructive to include lines denoting some quantiles of the wage distribution at experience level, say the 25th and 75th percentile. We'll show two ways to do this.

##### `lines`

The first is a minor step beyond what we did above. This entails adding lines to the existing plot using the `lines` function. `lines` cannot be called to _start_ a plot -- it can only be called to _add to_ an existing plot. And it can only draw lines.

We'll also demonstrate here the use of curly braces `{}` within `j`. When we surround our statements in `j` with curly braces, it allows us to execute more than one line of code there. Basically, when we uses braces, `j` is considered to be all of the lines between the braces.

```{r lines_wage_tenure, results = "hide"}
teachers_wisc[ , .(avg = mean(salary), 
                   #include the other quantities of interest
                   q25 = quantile(salary, .25),
                   q75 = quantile(salary, .75)), 
               keyby = .(exp = floor(total_exp))
               ][ , {#start the braces. The first statement is to plot
                 plot(exp, avg, type = "l", ylab = "Salary ($)", 
                      col = "blue", xlab = "Total Experience", 
                      #also need to adjust the y limits since
                      #  the high quantile is likely to
                      #  exceed the average
                      lwd = 3, ylim = c(0, max(q75)),
                      main = "Evolution of Pay with Experience")
                 #now we can start our second statement within j
                 lines(exp, q25)
                 lines(exp, q75)
               }]
```

This looks OK, but I'm not satisfied with the quantile lines. I'd prefer them not to be solid, but _dashed_ lines. We can do this with a new argument -- `lty`. `lty` specifies the **l**ine **ty**pe. Like `pch`, it for the most part takes integer arguments. Here are the possibilities illustrated:

![lty](http://i.imgur.com/82vrkh1.png "lty")

```{r lines_wage_tenure_lty, results = "hide"}
teachers_wisc[ , .(avg = mean(salary),
                   q25 = quantile(salary, .25),
                   q75 = quantile(salary, .75)), 
               keyby = .(exp = floor(total_exp))
               ][ , {
                 plot(exp, avg, type = "l", ylab = "Salary ($)", 
                      col = "blue", xlab = "Total Experience",
                      lwd = 3, ylim = c(0, max(q75)),
                      main = "Evolution of Pay with Experience")
                 lines(exp, q25, lwd = 3, col = "blue", lty = 2)
                 lines(exp, q75, lwd = 3, col = "blue", lty = 2)
               }]
```

##### `matplot`

The other approach introduces a new function, `matplot`. This operates like `barplot` in that it accepts a `matrix` argument and turns each column into a new plot object. The main advantage of this over using `lines` is conciseness and extensibility. If we wanted to add 15 lines to a plot, it would be nightmarish to repetitively type `lines` over and over (yes, there is a way to do this in a `for` loop, but that will necessarily be ugly and hard to wrangle with). `matplot` allows us to plot all of the lines at once. Let's see it in action:

```{r matplot_wage_tenure, results = "hide"}
teachers_wisc[ , .(avg = mean(salary),
                   q25 = quantile(salary, .25),
                   q75 = quantile(salary, .75)), 
               keyby = .(exp = floor(total_exp))
               #the [ , !"col", with = FALSE] syntax
               #  tells data.table to drop the column "col"
               ][ , matplot(exp, .SD[ , !"exp", with = FALSE],
                            type = "l", xlab = "Total Experience",
                            #we have to specify one lty for each
                            #  line we add (order matters)
                            ylab = "Salary ($)", lty = c(1, 2, 2), 
                            lwd = 3, col = "blue",
                            main = "Evolution of Pay with Experience")]
#Specify top-to-bottom order so it jives with what we see in the plot
legend("topleft", legend = c("Q75", "Mean", "Q25"),
       lty = c(2, 1, 2), lwd = 3, col = "blue")
```

##### Adding Vertical/Horizontal/Trend Lines

There is one more function that is worth mentioning since it comes up so often -- `abline` (so named because it's designed to add lines of the form `a + b*x`, i.e., in slope-intercept form). Primarily, I use this to add vertical, horizontal, and trend lines to a plot. Let's return to our scatterplot from before, plotting fringe benefits vs. salary. Let's decorate this plot with some extra lines. Like `lines`, `points`, `legend`, and `axis`, `abline` adds objects to the current plot. 

```{r salary_fringe_abline, results = "hide"}
english[fringe<3e4, plot(salary, fringe, xlab = "Salary ($)",
                         ylab = "Fringe Benefits ($)",
                         main = "Fringe Benefits vs. Salary",
                         col = "darkblue", yaxt = "n")]
axis(side = 2, at = 5000*(0:5), las = 1,
     labels = paste0("$", 5*(0:5), "k"))
#add a vertical line at the average salary
english[fringe<3e4, abline(v = mean(salary),
                           #and a horizontal line at average fringe
                           h = mean(fringe),
                           #and a trend line as determined by the results
                           #  of the regression of fringe on salary
                           reg = lm(fringe ~ salary),
                           #emphasize the trend line
                           lwd = c(3, 1, 1))]
```

#### Multiple Axis Sets

Plotting in one figure on more than one pair of axes can get very messy and involved, if perfection is what you're after. The basic approach basically consists of specifying an option before plotting. 

First, a quick aside to talk about plotting metaparameters. Every time we create a plot, R draws from a list of 72 (yes, SEVENTY-TWO) parameters dictating subtle things about how the plot is presented, from the x:y ratio to the current plot limits to the font and scaling of the axes. If you have a few hours on your hand, try and pore through `?par`. To this day, I'm only familiar with about 30% of these parameters.

That said, there are a few that are crucial to be aware of if you want to create more fine-tuned, perfectionist plots in your research. The first involve the figure's margins. In all of the plots we used above, we implicitly allowed the default plot margins to reign supreme. This created somewhat of an issue when we ran into the issue of the y axis label smushing up next to the y axis ticks. Rather than abbreviating the axis ticks, we could have given more space to the left-hand-side margin. Earl Glynn has provided this [excellent page](http://research.stowers-institute.org/efg/R/Graphics/Basics/mar-oma/) with a few graphics that I have stared at time and time again (pro tip: Google "oma r" and it's the first result). Here's the one explicating margins in simple plots:

![oma](http://i.imgur.com/BUD8AOs.gif "oma")

Both `oma` and `mar` are specified as four numbers, each representing one side of the plot. By default, `oma` is 0 on all sides; it's only really useful for tiling plots. `mar`, on the other hand, has the cryptic defaults of `c(5.1, 4.1, 4.1, 2.1)`. I got nothing as far as an explanation goes -- this is just something I've committed to memory. As with `axis`, they proceed clockwise starting from the bottom. So if we wanted to give some extra space to the margin next to the y axis, we would probably use `par(mar = c(5.1, 5.1, 4.1, 2.1))`.

The next most common to adjust is `mfcol`. This stands for "**m**ultiple **f**igures, **col**umn-wise" (see [this site](https://www.stat.auckland.ac.nz/~paul/R/parMemnonics.html) of mnemonics created by Paul Murrell for an attempt to create mnemonics for all of the `par` parameters). It is a length-two vector specifying how many rows (first element) and columns (second element) the total figure to plot will have. If `mfcol = c(2, 2)`, we will fill the figure by making four calls to `plot`. The first will go in the upper-left; the second in the lower-left; the third in the upper-right; and the fourth in the lower right. Earl Glynn's site is again handy:

![mfcol](http://i.imgur.com/V0sNBsX.gif "mfcol")

Let's make a wage-tenure scatter plot overlaid with per-experience average wages for four Wisconsin districts: Milwaukee, Madison, Green Bay, and Manitowoc. Here's a first pass:

```{r wage_tenurex4, results = "hide"}
#set main plot parameter to create 2x2 figure
par(mfcol = c(2, 2))
#first plot: Milwaukee scatter
teachers_wisc[agency_name == "Milwaukee Sch Dist",
              plot(total_exp, salary, main = "Milwaukee")]
#add by-tenure average using lines
teachers_wisc[agency_name == "Milwaukee Sch Dist",
              mean(salary), keyby = .(exp = floor(total_exp))
              ][ , lines(exp, V1, lwd = 3, col = "red")]
#now repeat for the other districts
teachers_wisc[agency_name == "Madison Metropolitan Sch Dist",
              plot(total_exp, salary, main = "Madison")]
teachers_wisc[agency_name == "Madison Metropolitan Sch Dist",
              mean(salary), keyby = .(exp = floor(total_exp))
              ][ , lines(exp, V1, lwd = 3, col = "red")]

teachers_wisc[agency_name == "Green Bay Area Sch Dist",
              plot(total_exp, salary, main = "Green Bay")]
teachers_wisc[agency_name == "Green Bay Area Sch Dist",
              mean(salary), keyby = .(exp = floor(total_exp))
              ][ , lines(exp, V1, lwd = 3, col = "red")]

teachers_wisc[agency_name == "Manitowoc Sch Dist",
              plot(total_exp, salary, main = "Manitowoc")]
teachers_wisc[agency_name == "Manitowoc Sch Dist",
              mean(salary), keyby = .(exp = floor(total_exp))
              ][ , lines(exp, V1, lwd = 3, col = "red")]
```

This is fine, but it leaves something to be desired. First, the plots are hard to compare since they're on different scales. Second, we had to spend a lot of time writing repetitive code. We should be able to generate this plot with a `for` loop. Third, look at all the wasted white space! The plot makes inefficient use of the space allotted. Fourth, in a similar vane, we don't need all of these axes. Once we fix the same scale on all four plots, we can infer the axes of plots from those to the left/right or above/below.

All of these are somewhat simple to take care of, but it'll probably look intimidating at a first pass. The idea is to crush the margins of each individual figure to save on white space, and to put all of the relevant information in the outer margins. We'll also add some bonus information to each sub-plot -- the number of teachers at each district, prettily formatted.

```{r wage_tenurex4_improved, results = "hide"}
dists <- c("Milwaukee", "Madison Metropolitan",
           "Green Bay Area", "Manitowoc")
#get full range of salaries to use as axis limits:
yl <- teachers_wisc[ , range(salary)]
par(mfcol = c(2, 2), 
    mar = c(0, 0, 0, 0),
    oma = c(5.1, 4.1, 4.1, 2.1))
#seq_along dists is the same as 1:length(dists)
for (ii in seq_along(dists)){
  #get the name of the current district
  dd <- dists[ii]
  teachers_wisc[agency_name == paste(dd, "Sch Dist"),
                plot(total_exp, salary,
                     #we'll use the title function to 
                     #  add the main title; also
                     #  set the ylimits so they're common
                     #  across charts
                     main = "", ylim = yl,
                     #exclude x axes from plots 1 & 3
                     xaxt = if (ii %% 2 == 0) "s" else "n",
                     #exclude y axes from plots 3 & 4
                     yaxt = if (ii <= 2) "s" else "n")]
  teachers_wisc[agency_name == paste(dd, "Sch Dist"),
                mean(salary), keyby = .(exp = floor(total_exp))
                ][ , lines(exp, V1, lwd = 3, col = "red")]
  
  #count teachers
  N_teach <- teachers_wisc[agency_name == paste(dd, "Sch Dist"), .N]
  
  #line = -1 forces the title to come down closer to the x axis
  #prettyNum prettifies numbers, in particular adding
  #  comma-separation (1000 -> 1,000)
  title(paste0(dd, " (", prettyNum(N_teach, big.mark = ","),
               " teachers)"), line = -1)
}

#Now for some bonus fun! Adding margin text and a main title
#  we specify outer = TRUE to make sure that we mean to use
#  the OUTER margin instead of the CURRENT FIGURE's margins.
#  line controls how far away from the main plot body the
#  text is written
mtext(side = 1, text = "Total Experience", outer = TRUE, line = 3)
mtext(side = 2, text = "Salary ($)", outer = TRUE, line = 2.5)
title(main = "Wage-Tenure Profiles in Wisconsin Districts", outer = TRUE)
```

## `ggplot`

I don't know how to use it really, but many of my R-using friends _swear_ by `ggplot2`. It is the plotting method/aesthetic of choice for Nate Silver's [fivethirtyeight.com](http://fivethirtyeight.com/), a site about quantitative analysis of politics, sports, etc. Here's an example:


![538 Style](http://i.imgur.com/jEYcR08.png "538 style")

There are whole textbooks dedicated to how to use `ggplot`. Search around on Google for some examples and see if you like the aesthetic (I don't), in which case it may be worth exploring and investing some time in learning how to use it. [This](http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403) book was written by the package author (Hadley Wickham, who in fact used this package as a substantial portion of his PhD thesis), and there's a free chapter on graphs of [Winston Chang's R Cookbook](http://www.cookbook-r.com/Graphs/).

## LaTeX Tables: `texreg`/`xtable`/`stargazer`

I don't think we'll have time today to get into LaTeX tables, especially since I don't think much of the audience has ever heard of LaTeX (for those who haven't -- look it up when you get a chance, it's awesome). There are three main packages -- `texreg` (for the output of regression models), `xtable` (for other tables), and `stargazer` (I've honestly never used it).

For the TeX-savvy, see [this Q&A](http://stackoverflow.com/questions/5465314/tools-for-making-latex-tables-in-r).

```{r clear_all_after_morning, echo = FALSE}
#let's wipe everything clean to free up some
#  room in the namespace for the afternoon
rm(list = ls(all = TRUE))
```


# Web Scraping: `rvest` (1:30 PM - 2:30 PM)

A lot of data that we find is seemingly locked out of our grasp -- we find it on a website, ripe for absorbing, but we're estopped from reaping the fruits of those sweet sweet raw data seeds by a lack of access to the raw data in a readily-handled format. Tables on sites like Wikipedia, the Census Bureau, ESPN, you name it, all tantalize the data analyst who hasn't added the tools of web scraping to their belt. 

Web scraping is basically the process of extracting data from websites. I have mainly used this for toy side projects -- scraping data about baseball statistics, Supreme Court judges, swing states, etc.

Scraping in R is done almost entirely through the `rvest` package. If I'm being honest, it's not very fully-flushed out, since it's still quite a young package, but I've been able to make it work in every situation I've come to so far. Python-ers are blessed with having methods for web scraping which are mature and robust -- though to be honest I found them much harder to use/pick up on than `rvest`. Installing `rvest`:

```{r install_rvest, eval = FALSE}
install.packages("rvest")
```


_**CAVEAT SCRAPER**_: It's not always legal to scrape a website. Amazon, for example, includes in its terms of service that using automated scrapers is against policy. Something to be aware of.

Also, too much scraping can get your IP address banned and/or throttled. This means that you'll be denied access to a website, perhaps permanently, if you generate too much traffic.

The proper approach for longer-term, more intense scraping operations is thus to download the HTML file associated with each web page, then read from that file, rather than loading the page over and over again, which causes undue stress on companies'/organizations' servers (i.e., you want to avoid anything looking like a [DDoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack)).

All that said, web scraping is powerful and (to me) fun. This is basically what's being done when you hear people talking about a sentiment analysis of Facebook or Twitter.

All web scraping starts with finding a page with data on it.

Let's try and glean some data from U.S. News & World Report's [National Universities Rankings](http://colleges.usnews.rankingsandreviews.com/best-colleges/rankings/national-universities/data). Again we have to give R a moment's rest.

`<<R sips coffee>>`

Web scraping illustrates one of the best features about Google Chrome -- the Inspect view for trying to target information about key elements of web pages.

We're targeting the main table of information, so right-click it, and hit "Inspect". This will bring up the Inspection Toolbar which gives a surfeit of information about the HTML tree of the current web page. 

If you hover over an HTML element in the inspection view, the output on the web page that it corresponds to is highlighted. You can expand/contract nodes of the HTML tree by clicking the arrows next to the ones visible. Clicking an arrow pointing down will collapse its "children"; clicking an arrow pointing right will expand its "children".

After hovering around a bit, we see that the following item, when we hover over it, appears to encompass the entire table of interest:

`<table class="ranking-data table table-lighthead table-packed ranking-data-free  ">`

It's no coincidence that this item is a `<table>`. The `<table>` tag in HTML will come up almost constantly in your quests to scrape the web, so make note. 

## `xpath` and Selectors

It's great that we foud the correct node, but this is often of little use _per se_. We need a way to tell an HTML parser (which is basically what all web scraping is -- parsing HTML) how to find the node we're interested in.

There are two sort-of languages used for specifying HTML/XML nodes. The first is `xpath`. If you've got time, it's really worth investing some time in learning the [`xpath` syntax](http://www.w3schools.com/xsl/xpath_syntax.asp), which will really open the door to becoming a web scraping wizard (I to date have only a facile mastery of the subject).

Happily, we don't really need to know anything about how `xpath` works, because Google Chrome is awesome. We can simply right click the XML element in the Inspection sidebar and hit `Copy` -> `Copy XPath`, and the `xpath` associated with that element is copied to our clipboard. For example, the `xpath` of our table is: `//*[@id="article"]/table`, which basically says that this `<table>` tag is the _child_ of another tag (of any sort -- `*`) which has the HTML attribute `id` equal to `"article"`. Indeed, scanning up the HTML tree, we see: `<div id="article" class="article">`. These two characteristics (being a `<table>` and being the child of a tag with `id="article"`) are enough to uniquely identify our object of interest in this page.

An alternative to `xpath` is CSS Selectors. They're generally very similar to `xpath` -- which you use is a matter of persona preference, and I prefer `xpath`, so we'll stick with that for today. See [this](http://www.w3schools.com/cssref/css_selectors.asp) page for more on CSS selectors.

## Scraping U.S. News

OK, enough front-end lead-up, where's the payoff?

We've now got all the ingredients necessary to scrape this page successfully. Before we do that, I want to introduce one more thing -- the infamous pipe of R: `%>%`.

`%>%` is originally found in the `magrittr` package, but has permeated throughout the hadleyverse, and is particularly in fashion in code associated with `rvest`. The pipe sends the output of the thing to the left of `%>%` to the function on the right of it. In particular, it's sent as the first argument to that function unless we specify otherwise. Some quick examples to show what I mean:

```{r piping}
#the pipe is one of the functions
#  exported by rvest, so we need to 
#  load it first
library(rvest)

#convoluted version of cbind(1:3, 4:6)
1:3 %>% cbind(4:6)

#we can also be explicit about where to send
#  the output but using its alias on the RHS, .
1:3 %>% cbind(., 4:6)
1:3 %>% cbind(4:6, .)

#something slightly more interesting;
#  note we have to open and close
#  the parentheses for it to work
rnorm(10) %>% sort()
```

OK, enough build-up. Let's scrape!

```{r scrape_us}
#I always store the page URL
#  as its own variable
URL <- paste0("http://colleges.usnews.rankingsandreviews.com/",
              "best-colleges/rankings/national-universities/data")

#now we break out the rvest weapons
#  read_html takes to the web and 
#  retrieves the web page at the
#  address you give it (i.e., it's
#  doing the first part of what
#  Chrome does when you enter a URL
#  in your browser search bar).
#  It returns the XML tree of the page.
pg <- read_html(URL)

#Now that we've got the XML tree, we
#  have to find the node we were after
#  from before. To do this, we use
#  the html_node function, which
#  has an xpath option, to which
#  we can supply the xpath from above --
#  making sure that we're careful
#  about quotation marks.
xp <- '//*[@id="article"]/table'
pg %>% html_node(xpath = xp)

#This has returned an `xml_node` corresponding
#  to our table. We just need one more function 
#  and we'll really be cooking -- html_table.
#  html_table tries to parse an table from
#  HTML form into a data.frame so that we can
#  start dealing with it like we would other
#  objects read from CSV, Excel, etc.
pg %>% html_node(xpath = xp) %>%
  html_table()

#Now that we know it works, we can assign it to
#  an object by repeating the code and also 
#  sending it to setDT so we have a data.table
usnews <- pg %>% html_node(xpath = xp) %>%
  html_table() %>% setDT()

#First, the last two columns are only available
#  to paid subscribers, let's just drop them:
usnews[ , (ncol(usnews) - 1):ncol(usnews) := NULL]

#The table is a bit of a mess; we'll start
#  by assigning more manageable names:
setnames(usnews, c("rank", "name", "tuition_fees",
                   "enrollment", "acceptance_2014", 
                   "retention_freshman", "grad_rate_6_yr"))[]

#There's still some mess to be cleaned up. Time to 
#  break out the regex skills from this morning:
usnews[ , score := 
          gsub(".*Overall Score: ([0-9]*) out of.*", "\\1", rank)]
usnews[ , rank := gsub("#([0-9]*).*", "\\1", rank)]
usnews[ , name := gsub("\n.*", "", name)]
usnews[ , enrollment := 
          as.integer(gsub(",", "", enrollment, fixed = TRUE))]
usnews[ , acceptance_2014 := 
          as.numeric(gsub("%", "", acceptance_2014, fixed = TRUE))]
usnews[ , retention_freshman := 
          as.numeric(gsub("%", "", retention_freshman, fixed = TRUE))]
usnews[ , grad_rate_6_yr := 
          as.numeric(gsub("%", "", grad_rate_6_yr, fixed = TRUE))]

#What remains is the issue of tuition_fees:
usnews[]

#We have to account for the fact that tuition is 
#  sometimes listed as in- and sometimes as 
#  out-of-state. We have three options that I see:
#  1) Pick one or the other and delete the other
#  2) Average the two
#  3) Create two columns -- in- and out-of-state tuition
#  We'll go with number 3.

#We'll have to introduce some new tricks to get this to work.
#  Let's do this step by step, then go all at once.

## Get _only_ the tuition_fees values which differentiate
##   between in- and out-of-state tuition:
in_out <- usnews[grepl("in-state", tuition_fees), tuition_fees]
in_out

## Now, split each element of this string into pieces.
##   We can do this with the strsplit function, which
##   takes a character vector and an argument split.
##   split is a regular expression saying where to
##   split each element of the string. In our current
##   case, we'll split at the colon following in-state
##   and the out-of-state, as well as the comma preceding
##   out-of-state. We can't just split on commas, though,
##   since the tuition values themselves still have these.
##   What distinguishes the comma we want (and, as luck
##   would have it, the colons as well) is that they're
##   followed by a space, which we'll leverage to split it:
in_out_split <- strsplit(in_out, split = "[,:] ")
in_out_split

## Next, we want to extract only the numbers from this split.
##   To do this, extract only the 2nd and 4th element, and then
##   (as above) remove the dollar sign and comma. We want to
##   return a list since the right hand side of := has to be a list.
in_out_num_l <- lapply(in_out_split, function(io)
  as.integer(gsub("[$,]", "", io[c(2, 4)])))
in_out_num_l

## Finally, one more trick to do. := expects to find each 
##   _element_ of the list it receives corresponding to one 
##   _column_. Right now, this is messed up (it would be more 
##   obvious if the list wasn't 2x2...), since the first element
##   has one in-state and one out-of-state value (and same
##   for the second element). What we need is for the first
##   element to have BOTH in-state values, and the second
##   element to have BOTH out-of-state values. data.table
##   has a tool designed for just such a situation:
##   transpose. Basically, transpose flips the order of
##   a list. Right now, the list has the list elements
##   corresponding to different ROWS and the elements
##   WITHIN a list element corresponding to different COLUMNS
##   If we transpose this, we'll get list elements
##   corresponding to different COLUMNS and the elements
##   WITHIN a list element corresponding to different ROWS,
##   which is just what we want.
transpose(in_out_num_l)

##Now, we can do all of that at once like so:
usnews[grepl("in-state", tuition_fees),
       paste0("tuition_", c("in", "out"), "_state") :=
       transpose(lapply(strsplit(tuition_fees, split = "[:,] "),
               function(io) as.integer(gsub("[$,]", "", io[c(2, 4)]))))]

#And to finish off, we assign the same value to in- and 
#  out-of-state tuition for those (private) universities which
#  don't make a distinction:
usnews[is.na(tuition_in_state), 
       paste0("tuition_", c("in", "out"), "_state") :=
         #since we haven't quite fixed the punctuation in
         #  the other rows yet, we do so here as an 
         #  intermediate step
       {x <- as.integer(gsub("[$,]", "", tuition_fees))
         .(x, x)}][]

#And now we can delete the old column
usnews[ , tuition_fees := NULL][]
```

That wasn't so bad, was it?

Well, we're not quite done yet. Observe the following:

```{r stupid_nbsp}
print(usnews, quote = TRUE)

#See that space after name? It's gonna be a bugbear.
#Observe:
gsub("\\s", "", usnews$name)

#The problem is, that space at the end isn't just
#  any space. It's the dreaded NON-BREAKING SPACE
#  (you'll sometimes see this on website or in
#   newspapers: &nbsp;). The \\s token in 
#  reges doesn't recognize it as blank space.
#  We have to do something really ugly to get
#  rid of the non-breaking space. First, the magic:

usnews[ , name := gsub(intToUtf8(160), "", name)][]
```

So the space is gone, but what happened? It turns out that we can create the `&nbsp;` character from _basically_ binary code. It turns out that 160 is the code corresponding in UTF-8 encoding to the `&nbsp;`. So `intToUtf8(160)` outputs the `&nbsp;` as a character understood by R and `gsub`, after which point we can delete it like we would any other character.

### Extending

All that work for very little payoff, right?

Well, sort of. We got 9*25 = 225 data points from this scraping operation... which is not many, admittedly.

However, we can now re-use this code to scrape _other_ pages of data from the U.S. News Site, since the format is basically the same (crucially, we'll be able to re-use the same `xpath` to get the table on all the other pages). We can then stitch together the table we get from each page into a much bigger data set. The additional footwork required to go from scraping the first page to scraping _all_ the pages is typically not much (unless there are major differences in format from one page to the next).

Let's demonstrate:

```{r scrape_all}
#Only the first 8 pages contain
#  universities that are
#  actually ranked. We could have
#  determined this programmatically
#  (useful in case the paradigm later
#   changes, for example),
#  but I didn't see the point for today
URLS <- paste0(URL, "/page+", 1:8)

usnews <- 
  #rbindlist compiles a list of data.tables
  #  into a single data.table. This is particularly
  #  useful for situations where we create a bunch of
  #  tables in a loop, as is happening here
  rbindlist(lapply(URLS, function(uu){
    #for each page uu, repeat the above...
    usn <- read_html(uu) %>% html_node(xpath = xp) %>%
      html_table() %>% setDT()
    usn[ , (ncol(usn) - 1):ncol(usn) := NULL]
    setnames(usn, c("rank", "name", "tuition_fees",
                    "enrollment", "acceptance_2014", 
                    "retention_freshman", "grad_rate_6_yr"))
    usn[ , score := 
           gsub(".*Overall Score: ([0-9]*) out of.*", "\\1", rank)]
    usn[ , rank := gsub("#([0-9]*).*", "\\1", rank)]
    usn[ , name := gsub("\n.*", "", name)]
    usn[ , enrollment := 
           as.integer(gsub(",", "", enrollment, fixed = TRUE))]
    usn[ , acceptance_2014 := 
           as.numeric(gsub("%", "", acceptance_2014, fixed = TRUE))]
    #Well, slight adjustment here. There are some colleges where
    #  these fields have a footnote because it's calculated
    #  idiosyncratically. Everything follows the percent sign,
    #  so it's only a minor adjustment.
    usn[ , retention_freshman := 
           as.numeric(gsub("%.*", "", retention_freshman))]
    usn[ , grad_rate_6_yr := 
           as.numeric(gsub("%.*", "", grad_rate_6_yr))]
    usn[grepl("in-state", tuition_fees),
        paste0("tuition_", c("in", "out"), "_state") :=
          transpose(lapply(strsplit(tuition_fees, split = "[:,] "),
                           function(io) 
                             as.integer(gsub("[$,]", "", io[c(2, 4)]))))]
    usn[is.na(tuition_in_state), 
        paste0("tuition_", c("in", "out"), "_state") :=
        {x <- as.integer(gsub("[$,]", "", tuition_fees))
        .(x, x)}]
    usn[ , tuition_fees := NULL]
    usn[ , name := gsub(intToUtf8(160), "", name)]}))
    
#just like that! All done. Just some clean-up:
usnews <- usnews[!grepl("Rank Not", score)]
usnews
```

# Geospatial Tools (2:30 PM - 3:30 PM)

`S4` methods/`@`slots, `CRS`

# Automatic Presentations/Papers: `knitr` (3:30 PM - 4:30 PM)

# Dynamic/Interactive Output: `shiny` (4:30 PM - 5:30 PM)

See [here](http://shiny.rstudio.com/tutorial/).

# Appendix

`vapply`

`CJ`

`setkey`

## Data Cleaning

## Parallelization

## Source Code

## Package development

## Advanced types