---
title: "IES R Tutorial"
author: "Michael Chirico"
date: "Compiled `r format(Sys.time(), '%B %d, %Y at %R')`"
output: 
  rmarkdown::html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

First and foremost is to install R (following instructions [here](http://lib.stat.cmu.edu/R/CRAN/)) so that your computer knows how to interpret R code. Then, I highly recommend you install RStudio (following instructions [here](https://www.rstudio.com/products/rstudio/download/)), which is a powerful program for organizing your interaction with the R language (a subtle distinction, and you wouldn't really be ill-served by considering RStudio and R to be the same thing); for the jargon-hungry, RStudio is an Integrated Development Environment (IDE) for the R language.

Some general guidelines for getting the most out of this workshop:

 1. **Active participation is key!** You should be running every snippet of code I go over on your own machine. As with human languages, exposure is one of the building blocks of fluency. You would be insane to think that you can osmote the ability to understand and use R code just by watching me do things on a screen. This facility takes practice, practice, hair pulling, and more practice. For those with little exposure to more advanced programming languages, today's workshop will be _very_ heavy with material. You shouldn't expect to absorb everything, but actively participating will help _a lot_.
 
 2. **Questions are openly encouraged!** As I mentioned, this will, despite my efforts to make things as straightforward as possible, inevitably be a somewhat dense workshop. So nobody should hesitate to stop me if they're lost. I am no fan of hearing myself speak, so if nobody in the audience is following me, that's a waste of everyone's time. 

***
***
***
# R Basics (8 AM - 9 AM)

Before we get to the fun stuff (statistical analysis), it's important that we gain some facility with doing basic things in R first -- adding numbers, creating objects, and tools for exploring/understanding new objects as we come across them, including understanding and overcoming common errors that can creep into our code.

First things first, let's follow the time-honored tradition of making R say "Hello World".

Make sure you're in the console (the cursor in front of the right angle bracket (`>`) should be blinking) and type (or copy-paste) the following:

```{r hello}
print("Hello World")
```


Easy-peasy. The console is a great place to do what I call sandboxing -- running small commands to test whether they run as expected and produce the right output in the right format, etc. But it would be very cumbersome to use the console to do everything. There's a distinct lack of permanency to anything we do in the console.

More common is to run code from an R script. You can create an R script in RStudio by clicking the plus-page icon (![New R script](http://imgur.com/eCEtq9e.png "plus-page icon")) and hitting "R Script" or by using the keyboard shortcut `Ctrl`+`Shift`+`N`. Try printing "Hello World" again by adding `print("Hello World")` to your new script and pressing `Ctrl`+`Enter` on this line. `Ctrl`+`Enter` is the shortcut for running the selected line, or for running a highlighted section of code.

Once we save our R script, we can easily share it with coauthors or the general public (or even ourselves, on a different computer), who can simply run the code again on their machine to reproduce your analysis. 

***
## Assignment

Consider the following:

```{r assign_scalar}
x = 3
```

When we execute the above line of code, we're creating the _variable_ `x` and associating with it the value `3`. This is like creating a `local` in Stata.

Variables (a.k.a. objects) are the most crucial building block for everything we want to do in R. When we create a variable, we create a shorthand for some value that we'll refer to lower in our code.

Assignment has to pass from right to left -- the _object_ on the right of `=` is assigned to the _name_ on the left of `=`.

The opposite is an error, since we haven't told R what `y` is yet:

```{r assign_error, error = TRUE}
3 = y
```

This error tells us that `3` is not a valid thing to which to assign; as a rule of thumb for beginners, all variable names must start with letters (though can contain other characters thereafter).

> > Side note: the following works just as well:
> >  `x <- 3`
> > this method of assignment is probably more common, but we'll stick with using `=` today since I think it's more intuitive. The differences between `=` and `<-` are all-but irrelevant for beginners, but for future reference, [this Q&A](http://stackoverflow.com/questions/1741820/) is worth a read: 

***
## Vectors

The easiest way to think of a vector is as a column (or a row) in Excel. A column in excel can contain many numbers, but instead of referring to each of them individually, we refer to the row. Typically, these numbers all have something in common (for example, a column Name in Excel should be filled with peoples' names).

The way to declare vectors in R is using `c`, which stands for _**c**_oncatenate:

```{r assign_vector}
x <- c(1, 2, 3, 4)
x
```

Now `x` contains four numbers. Note that since this type of sequence is so common, R has built in the colon (`:`) as an operator to create a variable like this more quickly/concisely:

```{r assign_sequence}
x <- 1:4
x
#Also works in reverse
x <- 4:1
x
```

***
## Types

Thus far, everything we've assigned to a variable has been a number. But this is _far_ from the only thing we can assign to a variable in R. Consider:

```{r assign_non_numeric}
x <- c("Philadelphia", "Pennsylvania")
y <- c(TRUE, FALSE)
z <- c(1L, 2L)
```

`x` contains a _string_ of letters _demarcated by `""` (or `''`) to distinguish them from variable names_. We refer to `x` as a `character` vector. 

`y` is a `logical` vector; it is often very convenient to keep binary variables (gender, treatment group, etc.) stored as `logical` variables, for reasons we'll see below.

`z` is an `integer` vector. This is distinguished from `c(1, 2)`, which gets stored as a `numeric` vector. The difference between the two probably won't affect you for quite some time, but it's often important for saving computer memory. `1`, as a `numeric`, takes up more space in your computer's memory than does `1L`. `L` signifies integer; don't worry about why (if you insist, it stands for _**L**_ong integer, which is too involved for this session).

### Lists

Consider this:

```{r assign_coerce}
x <- c(1, 2L, TRUE, "America")
```

All four components of `x` are of different type (namely, `numeric`, `integer`, `logical`, `character`). Recall above that we said vectors must have something in common, but as we declared it, this couldn't be further from the truth. In fact, R will force all of these components to have the same type -- namely, character:

```{r assign_coerce_print}
x
```

Note the quotation marks -- none of the components are any longer considered as `character` strings in `x`.

> > The specific heirarchy is `logical` < `integer` < `numeric` < `character`

However, mixing types is a fundamental feature of almost all data analysis, so it stands to reason that there is a straightforward way to do so. In R, this is done with the `list` type. We can replace the code above using `list`:

```{r assign_list}
x <- list(1, 2L, TRUE, "America")
x
```

Note how different the output looks, as compared to using `c`!! The quotation marks are gone except for the last component. You can ignore the mess of `[[` and `[` for now, but as an intimation, consider some more complicated `list`s:

```{r assign_lists}
x <- list(c(1, 2), c("a", "b"), c(TRUE, FALSE), c(5L, 6L))
x

y <- list(list(1, 2, 3), list(4:5), 6)
y
```

`x` is a `list` which has 4 components, each of which is a vector with 2 components. This gives the first hint at how R treats a dataset with many variables of different types -- at core, R stores a data set in a `list`!

`y` is a _nested_ `list` -- it's a `list` that has `list`s for some of its components. This is very useful for more advanced operations, but probably won't come up for quite some time, so don't worry if you haven't wrapped your head around this yet.

***
## Extraction/Indexing

Consider a simple `numeric` vector:

```{r extract_assign}
x <- 5:14
```

How do we get at the various numbers stored in certain positions in `x`? For example, how could we get the first number in `x`, `5`?

This process is called _extraction_, and, for vectors, is done with square brackets (`[]`), e.g.:

```{r extract_vector}
x[1] #first element of x

x[5] #fifth element of x
```

This also works on `list`s, but `list`s also have some other ways to get at their contents:

```{r extract_list}
y <- list(1:3, 4:6, 7:9)
y[1]
```

Note that the output still has `[[` in it. This means the result of `y[1]` is _still a list_.

More typically, we want to _remove_ the `list` structure and just get `1:3` instead of `list(1:3)`. To do this, we use `[[`:

```{r extract_list_item}
y[[1]]
```

### Named vectors and `list`s

It is also possible to name the elements of vectors and lists. This is convenient for making it easy to get certain elements without having to remember whether you stored it first, third, or whatever:

```{r extract_named}
x <- c("Iowa" = "Cruz", "Ohio" = "Kasich", "Pennsylvania" = "Trump")

y <- list(names = c("Kasich", "Cruz", "Trump"),
          ages = c(63, 45, 69),
          hates_clinton = c(TRUE, TRUE, TRUE))
```

For named vectors, we keep using `[]` to extract elements, but we can use the name instead of the index:

```{r extract_named_vector}
x["Iowa"]
```

For named `list`s, we can use `[]`, but, as with numbered indices, we'll get a `list` in return. If we want the actual object contained at that point in the list, we can still use `[[`, or we can also use `$`, which is another extraction operator:

```{r extract_named_list}
#now that y has names, we no longer see [[ -- instead, we see
#  the name of each element of the list
y["ages"]

y[["ages"]]

y$names
```

### Multiple extraction

As often as not, when we need to extract, we need _more than one_ element of the vector. 

To do this, we _pass a vector to `[]`_. It's must intuitive when we need elements in sequence, e.g.:

```{r extract_sequence}
# R conveniently keeps the letters of the
#   alphabet stored automatically in two
#   vectors, letters and LETTERS;
#   the former is lower-, the latter uppercase
x <- LETTERS
x

#Get the first 5 letters
x[1:5]
```

What if we need the first and 5th elements, but not the 3rd/4th/5th elements?

Remember that `1:5` is the same as `c(1, 2, 3, 4, 5)`. So we could have just used the latter:

```{r extract_sequence_long}
x[c(1, 2, 3, 4, 5)]
```

By extension, if we just want the first and fifth elements:

```{r extract_non_adjacent}
x[c(1, 5)]
```

_**NOTE**_: there's no* way to extract multiple elements using the other extraction operators, `[[` and `$`. It's an error to try:

```{r extract_errors, error = TRUE}
x <- list(a = 1:3, b = 4:6)
#multiple extraction by names works fine:
x[c("a", "b")]

#but not with [[ or $
x[[1:2]] #technically, R interprets this as a recursive index...

x[[c("a", "b")]] #again, a recursive index...

x$c("a", "b")
```

### Logical indexing

Another exceedingly common approach is to subset an object based on a condition satisfied by some of the elements.

Here, we must introduce the logical operators (also called _binary operators_, which is important for understanding associated error messages) in R.

These are `<`, `>`, `<=`, `>=`, `==`, and `!=`. These are all the same in Stata (note: `~=`, which works in Stata as an alternate to `!=`, _doesn't work in R_). First, let's explore them:

```{r logical_operators}
ages <- c(12, 14, 16, 18, 20, 22, 24, 30,
          38, 40, 55, 60, 63, 66, 68, 70)

#Who is voting age?
ages >= 18

#Who can't drink?
ages < 21

#Who is exactly 40?
ages == 40
```

Note that R automatically figures out whether _each element_ of `ages` satisfies the condition.

The way to extend this to extraction is simple:

```{r extract_logical}
#Only take people under age 18
ages[ages < 18]

#Only take AARPers
ages[ages >= 50]

#Exclude any 70 year old:
ages[ages != 70]
```

The other common logical operations are intersection, union and negation, aka **AND**, **OR** and **NOT**, which in R, as in Stata, are `&`, `|` and `!` (you'll also often see `&&` and `||` in other peoples' code; don't worry about the difference yet, but for those who know Matlab -- it's the same).

Quickly, let's get working-agers:

```{r extract_logical_pair}
ages[ages >= 18 & ages <= 65]
```

We can combine these to our heart's fancy and create any combination of logical requirements, e.g.:

```{r extract_logical_chain}
#get people who are over 18, but not exactly 34 or 70
ages[ages >= 18 & !(ages == 34 | ages == 70)]
```

### Negative indexing

Sometimes, we'd rather _exclude_ a small number of elements, rather than _include_ elements like we have been doing so far.

To do this, we precede the vector we want to extract with `-` (if it's `numeric`/`integer`/`character`) or `!` (if it's `logical`). 

Suppose we wanted to find out how many years have passed since our earliest observation, and to exclude the earliest observation. We might do something like:

```{r extract_negate}
years <- c(1970, 1973, 1978, 1980, 1990, 1995)
#the earliest year is 1970, which comes first; to exclude:
years[-1]
#to exclude and subtract 1970:
years[-1] - years[1]
```

***
## Base functions

R comes equipped with a vast (_vast_) library of built-in functions intended to make your life as a data analyst as easy as possible. As of this writing I count 1,203 functions included in base R, and 2,374 total functions that come ready to use as soon as you open up RStudio.

These are the real work horses of R, and many of them should be familiar to Excel and Stata users. Like in those programs, functions are distinguished by the use of parentheses `()` (in fact we've already used many functions to this point, most obviously the concatentation function `c`). We won't be able to get anyhwere near understanding all of the multitudinous functions R makes available to us today, but we can make a start.

### Basic Arithmetic/Stats

Finding the sum of a column in Excel is sort of a pain. You have to write `=SUM(` and then highlight all the proper cells. It's kind of a pain in Stata as well. You either have to `summarize var` the variable and run `di r(sum)` or create a new variable with `egen sum_var = sum(var)`. I find this and many other artihmetic operations much simpler to do in R.

```{r basic_arithmetic}
x <- 1:10

#find the sum
sum(x)

#find the mean
mean(x)

#find the variance and standard deviation
var(x)
sd(x)

#scalar arithmetic
3 * x
x - 2
x^2
x / 4

#exponentials
exp(1:3)
log(1:10)

#other arithmetic
abs(-5:5)

#rounding up/down
x <- c(.1, 1.1, 1.9, 2, 2.8)
floor(x)
ceiling(x)
```

### Function arguments

Most functions accept more than one argument. Consider rounding a number:

```{r round}
x <- c(1.12, 10.2, 1.56, 21.9, 2)
round(x)

#round to 1 digit past the decimal
round(x, 1)

#round to -1 digit past the decimal
#  (i.e., 1 digit befor the decimal)
round(x, -1)
```

There's no limit to the number of arguments a function can accept to accomplish a wide variety of tasks. With `round`, it's easy to handle this because there are only two arguments -- first, a vector of numbers, and second, a (single) number of digits to which to round the vector.

When the number of arguments balloons, however, it gets more and more difficult to keep track of which argument goes where.

Consider the `quantile` function, which takes 5 arguments -- `x`, a vector of numbers; `probs`, a vector of probabilities (quantiles); `na.rm`, which tells it whether to ignore missing data (more on that later); `names`, which tells R whether the result should be named (see example); and `type`, which tells R the algorithm to use for measuring the quantiles (there are 9 readily available).

It would be quite a pain to expect users to memorize the order in which `quantile` expects arguments, especially if we had to do so for every function we ever wanted to use (remember, there are _thousands_). To facilitate this, R allows us to _name_ our arguments.

```{r function_named_arguments}
x <- c(1, 1, 1, 2, 3, 4, 5, 6, 
       7, 8, 8, 8, 9, 10, 10, 10)

#by default, quantile calculates all 5 quartiles:
#  min, 25%-ile, median, 75%-ile, max
quantile(x)
median(x)

#if we just want the median
quantile(x, .5)
#or, we can name it to be explicit
quantile(x, probs = .5)

#if we want to exclude the names
quantile(x, probs = .5, names = FALSE)
```

In addition to making it easier for us as analysts to use the functions, naming arguments also makes it easier for _others_ to read our code. While it's fair to expect most users to know the basic functions and their arguments pretty well, the more advanced the function is that you're using, the more understandable your code gets when aided by named arguments.

### Infix operators and `match`

_Infix operators_ are things that are written from left to right like basic arithmetic, but which do things not covered by the five main arithmetic operators (`+`, `-`, `*`, `/`, `^`). These operators in R are always surrounded by two percent signs (`%`). The two math-y ones have to do with modular arithmetic:

```{r modular_arithmetic}
x <- 1:20
# integer division
x %/% 3
# modular division (remainder)
x %% 3
```

By far the most commonly used infix, though, has to be `%in%`. This is used to determine whether some objects (the left) can be found in another set (the right). Best with an example:

```{r %in%}
candidates <- 
  c("Bernie Sanders", "Hillary Clinton", "Lincoln Chafee", 
    "Jim Webb", "Laurence Lessig", "Donald Trump", "John Kasich",
    "Ted Cruz", "Marco Rubio", "Jeb Bush", "Ben Carson",
    "Carly Fiorina", "Lindsey Graham", "Chris Christie",
    "Rick Santorum", "Rand Paul", "Jim Gilmore", "Mike Huckabee",
    "George Pataki", "Bobby Jindal", "Scott Walker", "Rick Perry")

nominees <- c("Hillary Clinton", "Donald Trump")

candidates %in% nominees
```

A quick note that what `%in%` is actually doing is using the `match` function, which is also useful. `match` takes three arguments. The first and second are like the left- and right-hand side of `%in%`, respectively. `match` tries to find each element of the left in the right, and, if it's found, gives the position. The third argument, `nomatch` tells it what to do when the left element is _not_ found. Example:

```{r match}
color <- c("red", "green", "orange", "blue",
           "purple", "teal", "mahogany", "yellow",
           "orange", "white", "lavendar")

roygbiv <- c("red", "orange", "yellow", "green",
             "blue", "indigo", "violet")

#%in% tells you if each element of
#  color is in roygbiv at all
color %in% roygbiv

#match tells you WHERE in roygbiv 
#  each element of color was found;
#  we set no.match to say what
#  we expect when the color isn't found
#  (it must be an integer)
match(color, roygbiv, nomatch = -1)
```

### More useful functions

#### `length`

Length is used to find how many elements something has (in data, how many observations there are)

```{r length}
x <- 1:50
length(x)
```

#### `rep`

`rep` is used to _**rep**_eat an object a certain number of times. The `each` argument repeats each element a set number of times. `length.out` makes sure the output has a certain number of elements.

```{r rep}
rep(1:3, 4)

rep(1:3, each = 4)

rep(1:3, each = 4, length.out = 10)
```

#### `unique` / `duplicated`

`unique` eliminates all duplicates of a vector; `duplicated` returns a `logical` vector telling you which elements appeared earlier in the vector.

```{r unique/duplicated}
x <- c(1, 1, 1, 2, 2, 3)
unique(x)
duplicated(x)
### unique is the same as (but faster than) x[!duplicated(x)]
x[!duplicated(x)]

#unique is often used in conjunction with length
#  to find out the number of IDs in a vector, e.g.
length(unique(x))
```

#### `seq`

`seq` produces a _**seq**_uence. It has four main arguments: `to` (where to start), `from` (where to end), `by` (increment), and `length.out` (what should be the `length` of the result?). We must specify no more than 3 of these. Some examples:

```{r seq}
#same as 1:10
seq(10)

#same as 2:11
seq(2, 11)

#make a grid from 0 to 10 with 30 points;
#  R automatically figures out the increment
seq(0, 10, length.out = 30)

#get all the odd numbers from 1 to 50:
#  note that the upper endpoint won't
#  necessarily be included in the output
seq(1, 50, by = 2)
```

#### `sort`

`sort` will give an ordered version of its input. Default is increasing order; use the `decreasing` argument to reverse this.

```{r sort}
x <- c(1, 2, 4, -1, 8, 9, 20, 13, 0)
sort(x)
sort(x, decreasing = TRUE)
```

#### `paste` / `paste0`

`paste` and `paste0` are R's ways of concatenating strings, i.e., combining them together. `paste` has an argument `sep` which tells how to _**sep**_arate the components; `paste0` is a slightly faster version of `sep` that uses `sep = ""` (i.e., don't separate the output). The `collapse` argument, available to both, will reduce a vector to a single string, using `collapse` to separate. Examples:

```{r paste}
first <- c("Sam", "Charles", "Matthew", "Thom")
last <- c("Beam", "Mingus", "Embree", "Yorke")

#default sep is a space, sep = " "
paste(first, last)
paste(last, first, sep = ", ")
paste(last, first, collapse = ", ")
paste0(first, last, collapse = ", ")
```

***
## Random Numbers

As statistical analysts, randomness is our lifeblood. As a language designed for statistical analysis, then, it stands to reason that R comes well-equipped to handle many common statistical operations. This includes producing random numbers a number of common distributions, random permutations, random subsets, etc.

### Uniform and normal random numbers

The most common kind of random numbers that people want are uniform draws and normal draws. 

```{r unif/norm}
#generate 10 U[0, 1] draws
runif(10)
#generate 10 U[3, 5] draws
runif(10, min = 3, max = 5)

#generate 10 N(0, 1) draws
rnorm(10)
#generate 10 N(3, 5) draws
rnorm(10, mean = 3, sd = 5)
```

These examples highlight the common format of random number generators in R. To get random numbers from a distribution, there is probably a function named like `r`**`dist`**, where **`dist`** is an abbreviation for the distribution (here, **unif**orm and **norm**al). The first argument is the number of draws; the rest of the arguments are parameters.

Here is a complete table of all of the common distributions built in to R, and the function used to invoke their random number generator (RNG):

                   Distribution RNG          Other Parameters          Wikipedia
 ------------------------------ ------------ ------------------------- ----------
                           Beta `rbeta`      `shape1`, `shape2`, `ncp` https://en.wikipedia.org/wiki/Beta_distribution
                       Binomial `rbinom`     `size`, `prob`            https://en.wikipedia.org/wiki/Binomial_distribution
                         Cauchy `rcauchy`    `location`, `scale`       https://en.wikipedia.org/wiki/Cauchy_distribution
                    Chi-Squared `rchisq`     `df`, `ncp`               https://en.wikipedia.org/wiki/Chi-squared_distribution
                    Exponential `rexp`       `rate`                    https://en.wikipedia.org/wiki/Exponential_distribution
                              F `rf`         `df1`, `df2`, `ncp`       https://en.wikipedia.org/wiki/F-distribution
                          Gamma `rgamma`     `shape`, `rate`, `scale`  https://en.wikipedia.org/wiki/Gamma_distribution
                      Geometric `rgeom`      `prob`                    https://en.wikipedia.org/wiki/Geometric_distribution
                 Hypergeometric `rhyper`     `m`, `n`, `k`             https://en.wikipedia.org/wiki/Hypergeometric_distribution
                     Log-Normal `rlnorm`     `meanlog`, `sdlog`        https://en.wikipedia.org/wiki/Log-normal_distribution
                       Logistic `rlogis`     `location`, `scale`       https://en.wikipedia.org/wiki/Logistic_distribution
                    Multinomial `rmultinom`  `size`, `prob`            https://en.wikipedia.org/wiki/Multinomial_distribution
              Negative Binomial `rnbinom`    `size`, `prob`, `mu`      https://en.wikipedia.org/wiki/Negative_binomial_distribution
                        Poisson `rpois`      `lambda`                  https://en.wikipedia.org/wiki/Poisson_distribution
 Wilcoxon Sign Ranked Statistic `rsignrank`* `n`                       https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
                      Student t `rt`         `df`, `ncp`               https://en.wikipedia.org/wiki/Student%27s_t-distribution
                        Weibull `rweibull`   `shape`, `scale`          https://en.wikipedia.org/wiki/Weibull_distribution
 Wilcoxon Sign Ranked Statistic `rwilcox`*   `m`, `n`                  https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
                        Wishart `rWishart`   `df`, `Sigma`             https://en.wikipedia.org/wiki/Wishart_distribution

Table: List of native distributions from built-in `stats` package

In addition to RNGs, R typically has three other functions associated with a given distribution: a **d**ensity function with prefix `d`, e.g. `dnorm`, giving the PDF; a **p**robability function with prefix `p`, e.g. `pnorm`, giving the CDF; and a **q**uantile function with prefix `q`, e.g. `qnorm`, giving quantiles. 

```{r dist}
#this is mainly used for plotting, see below
dnorm(seq(-3, 3, length.out = 30))

#great for calculating p values
pnorm(1.96)

#great for getting critical values
qnorm(c(.005, .025, .05))
```

### `sample`

As often as we need random draws from uniform or normal distributions, we need to take random subsets of vectors, or to get random integers. The main function for tasks like this is `sample`, which takes 4 arguments: `x` (a vector from which to sample, or an integer representing the max of numbers from which to sample), `size` (the number of elements to sample, which defaults to `length(x)`), `replace` (whether draws are taken with replacement), and `prob` (a vector of probabilities, for weighted sampling -- default is uniform).

As always, it's easiest to see with some examples:

```{r sample}
#A permutation of 1:5
sample(5)
#A size-3 subset of 1:5
sample(5, 3)
#A size-5 subset of 1:5, with replacement
sample(5, 5, replace = TRUE)

#A weighted sample of 1:5, heavily weighting 1
sample(5, 5, replace = TRUE, prob = c(.99, .0025, .0025, .0025, .0025))
```

We can use the basic tools here to run basic randomization exercises, like rolling dice, flipping coins, or drawing cards from a deck:

```{r dice/coins/cards}
#Flip 10 coins, get the percentage heads
flips <- sample(c("H", "T"), 10, replace = TRUE)
mean(flips == "H")

#Roll a pair of dice and add them
sum(sample(6, 2, replace = TRUE))

#create a deck of labeled cards:
#  (I copy-pasted the text for the suits from
#   Wikipedia, but we could have used S/H/D/C:
#   https://en.wikipedia.org/wiki/Playing_cards_in_Unicode)
deck <- paste0(rep(c(2:10, "J", "Q", "K", "A"), 4),      #card values
               rep(c("♠", "♥", "♦", "♣"), each = 13)) #suits
deck

#Now draw a poker hand
sample(deck, 5)
```

### `replicate`

Full scale Monte Carlo/bootstrapping requires replicating a simulation exercise over and over. Again, R is ready and has the `replicate` function for us. Let's simulate a few poker hands to find their frequency and compare with the theoretical frequency:

```{r poker}
#It'll be easier to deal with numbers than
#  with the fancy strings we used above
deck_nos <- 1:52
#Simulating a flush
simulations <- 
  replicate(10000, #number of repetitions/simulations
            { #use curly braces to run a simulation that takes more than one line
              hand <- sample(deck_nos, 5) #draw a five-card hand
              suits <- hand %% 4 # force card number into one of 4 categories
              length(unique(suits)) == 1 #returns TRUE if all suits are equal, FALSE else
            }) #close braces and parentheses
#How often does a flush happen?
#  True frequency is 0.1965%
mean(simulations)

#What about a pair (just two cards match)?
#  true frequency: 42%
simulations <- 
  replicate(10000, {
    hand <- sample(deck_nos, 5)
    faces <- hand %% 13 # force into one of 13 categories
    sum(duplicated(faces)) == 1 #one pair if there's exactly _one_ match
  })
mean(simulations)
```

### `set.seed`

Just a quick note that any time we run a simulation, it's a good idea (for _replicability_) to _set and save the random seed_! Random numbers from computers may look random, but in fact they're completely deterministic -- if we know the value of the random seed! This is a disadvantage for information security (which is only sometimes important to us as data analysts), but an advantage for academic research, where replicability is paramount.

A simple example:
```{r seed}
set.seed(100)
runif(1)

#if we run again, we'll get a different number
runif(1)
#but if we re-set the seed, we'll get the same
set.seed(100)
runif(1)
```

***
## Vectorization / `*apply`

The last set of major workhorse functions in `R` are the `*apply` functions -- `sapply` and `lapply`. `apply` is also important, but I want to avoid touching on matrices today, if I can. The oft-ignored cousins are `rapply` (**r**ecursive), `tapply` (**t**agged), `mapply` (**m**ultivariate), `vapply` (**v**erified), and `eapply` (**e**nvironment). 

You should think of `lapply` as **l**ist apply and `sapply` as **s**implified apply, because the former returns a `list`, and the latter returns a more simplified object (vector/matrix/array).

Both of these functions are, by and large, what you should be using in R instead of a `for` loop. A `for` loop often consists of going through every element of a `list` and doing something to its contents. `for` loops exist in R, as they probably do in every langauge, but are typically much slower than what is often called a vectorized approach. We'll get into that more later, but for now I'll just leave an example of `lapply` and `sapply` in action:

```{r apply}
#pick three triplets of numbers at random
l <- list(sample(100, 3), sample(100, 3), sample(100, 3))
l

#now find the max within each group
lapply(l, max)

#note that the output is a list. It's typically
#  more convenient to have the output as a vector:
sapply(l, max)
```

***
## Packages

One of the things that makes R truly exceptional is its vast library of user-contributed packages.

R comes pre-loaded with a boat-load of the most common functions / methods of analysis. But in no way is this congenital library complete.

Complementing this core of the most common operations are external _packages_, which are basically sets of functions designed to accomplish specific tasks. The entire afternoon of this workshop is devoted to gaining facility in just a few of these.

Best of all, unlike some super-expensive programming languages, all of the thousands of packages available to R users (most importantly through CRAN, the **C**omprehensive **R** **A**rchive **N**etwork) are _completely free of charge_. 

The two most important things to know about packages for now (we'll start using my favorite, `data.table`, shortly) is where to find them, how to install them, and how to load them.

### Where to find packages

Long story short: Google. Got a particular statistical technique in mind? The best R package for this is almost always the top Google result if asked correctly.

How about heirarchical linear modeling (HLM)? A quick Google for "HLM R" or "heirarchical linear modeling R" both turn up some articles/tutorials/CRAN pages referencing `lmer`. 

### How to install packages

Just use `install.packages`!

```{r install.packages, eval = FALSE}
#we won't be able to run this, because it requires
#  administrative priviliges, but you can do
#  this easily on your own computer
install.packages("lmer")
```

### How to load packages

Just add it to your library!

```{r library, eval = FALSE}
#this won't run if it's not installed
library(lmer)
```

_Et voila_! You'll now be able to run HLM in R. You can also Google "tutorial lmer" (or in general "tutorial [package name]") and you're very likely to find a trove of sites trying to help you learn the package. Most popular packages also come with worked examples available through the example function, e.g., `example(package = "lmer")`.

***
## Toolkit: `ls`/`rm`/`?`/`class`/`dput`/`str`

Just a few more things that aren't related to statistical analysis _per se_, but which will often facilitate your coding experience.

First, `ls` and `rm`, short-hand for **l**i**s**t objects and **r**e**m**ove objects. These can be useful for understanding what names you've assigned to objects, and for occasionally cleaning up the clutter of variables you'll no longer use (for the meticulously clean, but also because leaving a bunch of unused objects lying around is a great way to create irritating errors / problems with your code).

Let's see all the junk we've created thus far in the workshop:

```{r lsrm}
#ls is a function, so we have to use () to call it
ls()

#one of the optional arguments to ls is to 
#  also list some "hidden" objects:
ls(all = TRUE)

#what a mess! let's get rid of some stuff
rm(ages, candidates, color)

#ugh, this is getting boring. What about
#  a scorched earth technique?
rm(list = ls(all = TRUE))

#after the apocalypse, nothing remains:
ls(all = TRUE)
```

### Troubleshooting

Finally, I'll leave you with some tools that are often the first place to look when trying to troubleshoot code that isn't working correctly.

#### `?`

`?` is the help operator in R. If you'd like to know _anything_ about any function in R, just type `?` before its name and hit enter. Let's start with the help page for `replicate` by entering `?replicate`. All help files are structured similarly, with the following common sections (I've highlighted in **bold** the most useful for troubleshooting):

 1. Description: an overview of the function(s) described on the page. `?replicate`, for example, redirects us to the help page for `lapply`, since in addition to `replicate` this page covers `lapply`, `sapply`, `vapply`, and `simplify2array` (don't worry about the last two, they're a bit more advanced and won't come up today).
 2. **Usage**: a generic snippet of code which shows the name and default value of every argument of every function on the page. This is very useful for understanding the ordering and naming of all the arguments to a function. `replicate`, for example, has 3 arguments, only 2 of which we used above: `n` is the number of repetitions, which has no default; `expr` is the expression to repeat (surrounded in curly braces `{}` above), which has no default; and `simplify`, which gives us control over how/whether the results are converted from a `list`, which has default `"array"`, leading to the output we saw above. 
 3. **Arguments**: gives a brief overview of what each function expects to receive for each argument (the type, whether it can be a vector or has to be a scalar, etc.). You shouldn't expect to fully understand everything that's being told to you here for a while, since it's pretty specific, but it can still help.
 4. Details: Some nitty-gritty about how the functions work, some mistakes to avoid, some of the more intricate details of how exactly the function goes about its task. Tends to be very dense and jargon-heavy.
 5. Value: Tells you details about the _type_ of object that you should expect to be returned as a result of running each function (e.g., we noted that `lapply` _always_ returns a `list`; this is noted in the Value section of `?replicate` in the first sentence).
 6. Note: Some more technical details about the function and its edge cases.
 7. References: Especially useful for packages -- this usually tells you where to go to find some more reading about the algorithms implemented.
 8. See Also: Gives some related functions which might also help accomplish similar tasks.
 9. **Examples**: A great place to look for some simple reproducible examples of how the functions work, with code that can be copy-pasted and run by you, the user.
 
The help files are an absolute _must_ go-to reference -- it's always the first place you should check when you're stuck.

#### `class`

`class` simply tells you the class/type of an object in memory. You'd be surprised how often the root of an issue is simply that _you_ think a certain variable is type X (say, `numeric`), but actually R is keeping track of it as if it's type Y (frighteningly often, `factor` -- more on that later).

#### `dput`

`dput` is like a microscope for R objects. It deconstructs any variable in R to its atoms -- its absolute fundamental components -- and prints out what's hidden under the hood. This is especially useful because all of the most complicated objects in R tend to have an associated `print` method which renders them in a digestible form. This is great for taking a glance at a complicated object, but not useful for understanding what's going wrong with that object.

Take, for example:

```{r dput}
#we'll get more into how R understands dates/times
#  later in the day; the key takeaway here is 
#  how R prints the object vs. how it's actually stored.
t <- as.POSIXlt("2015-05-12")
t
#when we just enter the object at the console, its
#  print method is invoked, i.e.:
print(t)

#this is a bit misleading. From what we see, it would appear
#  that t is just a character string. However:
t == "2015-05-15 EDT"

#so, what is t? use dput
dput(t)
#actually, it's a rather complicated list with components
#  describing every which thing about the date/time we entered
```

#### `str`

Often the output of `dput` is far too verbose to be of real use. `str` is a similar function which gives us a more detailed look at the components of a complicated object, but with the advantage that it doesn't give us too many details; it's more like a table of contents for R objects.

```{r str}
#don't worry about what lm is yet, we'll get there soon
y <- rnorm(1000)
x <- rnorm(1000)
r <- lm(y ~ x)
str(r)
```

Whew, that was a lot! Congratulations!

Now, on to the more fun and useful stuff!

***
***
***
# Data Basics (9 AM - 11 AM)

This section is intended to lay the groundwork of all of the most common data analysis tasks we face when tackling our empirical problems. 

## Reading .csv Files

The most common format of file you're like to find in the real world is in .csv (comma-separated value) format.

Quick clarification that not all comma-separated files have the extension. Often you'll find a file with the extension .txt that actually contains text that is comma-separated. You just have to inspect the file to figure out how it's stored.

In fact that's the case for or first example. We're going to look at teacher attendance data for the School District of Philadelphia.

The school district releases, through their Open Data Initiative, a whole bunch of data files related to the topics like school catchment zones, employee salaries, budget details, etc. All of the data can be found here: http://webgui.phila.k12.pa.us/offices/o/open-data-initiative.

We could use `R` to download the files (we may touch on this in the afternoon), but to ease us into things, I've gone ahead and downloaded some files and put them on my GitHub page. Let's look at the data from 2013-14. [Here's a link to the data](https://raw.githubusercontent.com/MichaelChirico/iesRTutorial/in_progress/data/School%20Profiles%20Teacher%20Attendance%202013-2014.TXT), but it should also be loaded on your machine.

We can preview the file using the terminal on a Mac. Learning to do some basic stuff in the terminal is a great investment of your time, but that's not the focus of today's lesson, so we'll just stick with doing things through R. We can send commands to the terminal in R using the `system` command. The terminal command `head` (which is also a function in R, but only applies to objects already loaded) will spit out the first couple lines of a file so we can see what it looks like without having to load the whole thing (which, for larger files, can be time-consuming).

```{r head}
#Just to flex some of R's muscles, we'll find
#  the data file from within R. As often as not,
#  we'll just navigate to the file ourselves and
#  just copy-paste the file path.
#  We're looking for the folder with the data
#  so we use include.dirs to include directories
list.files(include.dirs = TRUE) 

#I've kept things in the data folder here. We
#  can tell it's a folder as it has no extensions.
#  We can find the files in that folder using a 
#  relative path (./ means starting from the 
#  current folder, enter the folder after /).
#  We use the full.names of the file for the next step.
list.files("./data", full.names = TRUE)

#Now we see the data files. The one we're after
#  currently is "School Profiles Teacher Attendance 2013-2014.TXT"
#  To get it previewed, we send the command head to the system
#  and follow it with the quoted file name (since the
#  file name has spaces, which is a pain)
command = paste0('head ', '"./data/School Profiles Teacher Attendance 2013-2014.TXT"')
```
```{r system_uneval, eval = FALSE}
system(command)
```
```{r system_eval, echo = FALSE}
#See: http://stackoverflow.com/questions/27388964
#  seems I have to jump through hoops to get system to work with knitr
cat(system(command, intern = TRUE), sep = "\n")
```

We can see that even though the data has extension .txt, the file itself is very regular and clearly comma-separated.

### `fread`/`data.table`

To read in this data, I'm going to eschew the standard intro-to-R suggestion to use `read.csv` because we're presented now with the perfect opportunity to introduce to you the crown jewel of R -- `data.table` (full disclosure, I am an active contributor to it, but this has followed from my love for it).

`data.table` is a package created by Matthew Dowle and actively maintained by him, Arun Srinivasan, and Jan Gorecki. Basically, it's designed to make working with data in R _very straightforward/intuitive_ and _incredibly fast_ (since it's designed to handle data with tens of millions of observations). We'll be spending a lot of time working with `data.table` today, and picking most of it up as we go along. You can read more on the [`data.table` homepage](https://github.com/Rdatatable/data.table/wiki) when you get a chance.

The first and most famous tool of `data.table` is `fread`. The "f" means _**FAST**_. `fread` is a highly optimized tool for reading data into R at blitzing fast speeds. You'll barely notice the difference for typical files (less than 100,000 observations), but when you _do_ notice the difference, it's a sight to behold.

On your own machine, you'll need to install `data.table` (luckily for us, it's already been pre-installed on your machines for today) using:

```{r install_data.table, eval = FALSE}
install.packages("data.table")

#the more confident and ambitious can install
#  the development version of the package,
#  which tends to have more features, but may
#  occasionally be more error-prone.
#  Caveat coder.
install.packages("data.table", type = "source",
                 repos = "https://Rdatatable.github.io/data.table")
```

Once we're sure it's on our machine, we can read the data by running:

```{r fread}
#load the data.table package
library(data.table)

attendance <- 
  fread("./data/School Profiles Teacher Attendance 2013-2014.TXT")
```

Just like that!

Entering the data variable by itself will invoke the `print.data.table` command and display a preview of the data that was loaded:

```{r print.data.table}
attendance

#we can also explicitly invoke print,
#  which has a few more bells and whistles to
#  facilitate understanding what our data.table
#  looks like.
print(attendance, class = TRUE)
```

Using the `class` argument to `print`, the preview now includes an abbreviated type under each of the column headers letting you know what `class` R thinks each column is (_**note**_ `class` is a recent update to `data.table`; for the moment, it is not available in the version on CRAN, but should be available there soon).

What about `attendance` itself? It looks like something we've not seen yet. Let's explore:

```{r explore_data.table}
class(attendance)

str(attendance)

is.list(attendance)
```

OK, lots of important stuff here. We've discovered a new type of object! `attendance` is a `data.table`. You can think of this as being a single sheet of an Excel workbook. It is specific to the `data.table` package, and is an improvement on the `data.frame` type found in base R (we'll see this type a bit later).

As far as how R thinks of it, a `data.table` is a `list` on steroids. Each element of the `list` is a column, and each element has the same number of items. We can see from `str` that, like a `list`, we can use `$` to extract items (here: columns):

```{r extract_column}
attendance$SCH_TEACHER_ATTEND
```

### Exploring Attendance Data

What are the columns in this data? We usually hope we have a data dictionary, but I didn't find one on the SDP website. It appears that `ULCS_NO` is the same as `SCHOOL_ID` (padded with a `0`). It's not clear if the school district is anonymizing the schools or if we simply have to match the `SCHOOL_ID` to the school's name in another file.

The meat of the data are the two columns `SCH_TEACHER_ATTEND` and `SDP_TEACHER_ATTEND`. The former is the attendance rate of teachers at a given school; the latter is the average for the whole district (which is why it's the same in every row).

We can now start to explore the data. The only real column of interest is `SCH_TEACHER_ATTEND`. With this, we can explore how teacher attendance varies across the city.

Interacting with a `data.table` for exploration involves accessing it with `[]`. Within `[]`, there are _**three**_ _main_ arguments:

 1. `i`: which _rows_ do we want? (**subsetting**)
 2. `j`: which _columns_ do we want? (**selection** and **manipulation**)
 3. `by`: **grouping** our operations in `j` _by_ categories within a column.
 
 The `i` and `j` notation come from thinking of a `data.table` as a matrix. To express the `i`th row and `j`th column of a matrix `A`, math people often write `A[i,j]`.
 
 Let's see the first two in action in exploring our first data set

```{r attend_summary}
#Summarize the variable to get a quick understanding
#we're looking at ALL rows, so we leave the first argument BLANK
#we're trying to summarize SCH_TEACHER_ATTEND, so we
#  use the summary function in the second argument
attendance[ , summary(SCH_TEACHER_ATTEND)]
```

So most schools are within 2 percent of the district average. What about schools that are particularly high or particularly low?

```{r attend_high_low}
#We are focusing on certain rows, so we
#  say the condition defining those
#  rows in the first argument
attendance[SCH_TEACHER_ATTEND < 90]

#And schools with particularly high attendance
attendance[SCH_TEACHER_ATTEND > 97]
```

Without knowing more about the schools, all we can say is that there are a certain number of schools with very high or very low attendance. 

### Teacher Salaries

Another file posted to the SDP website contains information on teachers' salaries. I've again downloaded this data already for you, but if you're curious [here's](http://webgui.phila.k12.pa.us/offices/o/open-data-initiative/documents/employee---20160401.zip) a link to the data from the website.

This one actually comes with a data dictionary of sorts in the form of a README file. We can use R to display the README like so:

```{r readme, warning = FALSE, comment = ""}
writeLines(readLines("./data/README_SDP_Employee.txt"))
```

`readLines` is a function used to bring in text files to R objects. It returns every line of the file as one element of a `character` vector. `writeLines` is typically used for _creating_ text files, but if we don't tell it a file to use, it prints the output to the console.

The file describes all of three of the files that came with the download; of importance is `employee_imformation.csv`. We see here the columns are all described, which will help us understand what we see when we load in the data. Let's do it!

```{r read_salaries}
salaries <- fread("./data/employee_information.csv")
print(salaries, class = TRUE)

#it's a pain to have to lay on the SHIFT key all the
#  time, so let's rename the columns so that
#  are in lower case. We need three functions:
#  tolower, which takes all upper-case letters in a 
#  string and replaces them with their lower-case
#  counterpart; names, which returns the names of
#  any object (vector, list, data.table, etc.);
#  and setnames, which has three arguments:
#  1) a data.table, 2) the columns to rename, and
#  3) their replacements. If we only use two arguments,
#  R assumes we want to replace _all_ column names, and
#  that the second argument is the replacement values.
setnames(salaries, tolower(names(salaries)))

#checking the result:
salaries
```

This is a much bigger data set -- covering all employees of the SDP and giving many more details about them. Let's explore a bit:

```{r explore_salaries}
#table is a great function for describing discrete variables.
#  it gives the count of observations in each cell, and can also
#  be used to create what Stata calls two-way tables.
salaries[ , table(pay_rate_type)]

salaries[ , table(organization_level)]

#a cross-tabulation of these two; the first argument will
#  appear in the rows, the second in the columns
salaries[ , table(pay_rate_type, organization_level)]

salaries[ , table(organization_level, gender)]
```

Let's say we're only interested in salaried employees. How do we get rid of the hourly/daily employees?

Unlike in Stata, where we'd use `drop` and would have to jump through hoops to recover the lost observations (in case we decided to expand our focus later in our analysis), we can simply create a new `data.table` in R that contain only the observations we care about. This is, in my opinion, one of the most salient examples of a feature of R that blows Stata out of the water. We can keep many data sets at our disposal at once, without having to unload/reload the ones we're not currently using. This will prove _especially_ helpful when it comes to data cleaning/merging/reshaping.

```{r salaried}
salaried <- salaries[pay_rate_type == "SALARIED"]
```


Even more useful may be to exclude anyone who's not a teacher. First, we have to exclude anyone who's not a teacher. How do we figure out who's a teacher?

Approach:

 1. Count the number of observations in each category of `title_description` (we can see from the preview that there are at least some teachers under `TEACHER,FULL TIME`)
 2. Sort these categories to find the most common categories, since presumably teachers are by far the most common SDP employees.
 
Carrying this out will introduce a few new things:

 - Grouping. We're going to *group by `title_description`*, and count observations in each group. If I remember correctly, in Stata, this would be like `by title_description: count`. When `data.table` does this, you should think of your `data.table` being split into a whole bunch of smaller `data.table`s, each of which has all of the observations for exactly one level of `title_description`. 
 - `.N`. `data.table` uses the abbreviation `.N` to stand for "number of observations". It is defined _locally_, within each group, so all of the many `data.table`s mentioned above has its own `.N`. This is basically like `count` in Stata.
 - `order`. We can arrange the rows of a table by using `order` in the first argument (`i`); use a minus sign (`-`) to go in decreasing order, otherwise it's increasing order (like we saw with `sort`).
 - Chaining. We can tack on more and more `[]` to any given `data.table` operation to hone or continue or adjust our outcomes beyond what we could do in a single call to `[]`. The general syntax is `DT[...][...][...][...]`, where each `[...]` consists of doing some manipulation/subset/aggregation, etc.

```{r find_teachers}
#Let's take it step-by-step
# 1: Find the count of employees in each category
salaried[ , .N, by = title_description]

# 2: reorder these to find the most common
salaried[ , .N, by = title_description][order(N)]

# 3: put it in decreasing order
salaried[ , .N, by = title_description][order(-N)]

# 4: Only look at the top 20 most frequent positions
# (idea -- once it's resorted, these are simply
#  the first twenty rows of the sorted table)
salaried[ , .N, by = title_description][order(-N)][1:20]
```

We can start to see many interesting facets of employment at SDP here. Almost 300 school police officers but only 180 nurses. Ripe ground for exploring on your own!

I'll continue focusing on teachers; it appears all but the special ed. teachers are grouped under `TEACHER,FULL TIME`, so we can subset:

```{r get_teachers}
teachers <- salaried[title_description == "TEACHER,FULL TIME"]
teachers
```

### Group mean salaries -- various cuts

Now we can explore data by group. How does pay differ by gender?

```{r gender_gap}
#simple to code!
teachers[ , mean(pay_rate), by = gender]

#mean automatically got called V1. If we want
#  a more friendly output, we have to change a little:
teachers[ , .(avg_wage = mean(pay_rate)), by = gender]
```

So in teaching in Philly, actually women earn significantly more than men (this is indeed significant; I'll show you how to test that in a bit)!

(Of course men and women with the same experience and certification are paid basically the same -- it's written explicitly in the teachers' contract -- but we don't have any experience or certification measures to control for this)

How does pay differ by organization level?

```{r organization_gap}
teachers[ , .(avg_wage = mean(pay_rate)), by = organization_level]
```

Which school has the best-paid teachers?

```{r rich_school}
teachers[ , .(avg_wage = mean(pay_rate)),
         by = home_organization_description
          ][order(-avg_wage)]
```

This looks fishy! It's rare for any average worth knowing to come out exactly to an integer. I suspect there are very few teachers at these schools. How can we check?

```{r rich_school_2}
#remember our handy friend .N!!
teachers[ , .(.N, avg_wage = mean(pay_rate)),
          by = home_organization_description
          ][order(-avg_wage)]
```

In fact, almost all of the "best-" and "worst-" paid schools are outliers because there's only one employee listed. So that's not really a fair measure of dominance.

So what can we do to improve our measure?

What's typically done is to exclude all schools that don't have pass some cutoff minimum number of employees. For this exercise, let's say we want to exclude all schools that have fewer than 10 full-time teachers in the data.

The way to do so is pretty natural. Remember that when we use `by`, we're essentially splitting our main `data.table` up into `r teachers[ , uniqueN(home_organization)]` smaller `data.table`s, one for each school. We want to keep only those sub-`data.table`s that have at least 10 teachers -- i.e., they have at least 10 rows, i.e., `.N` is at least 10:

```{r rich_school_3}
#No longer keep .N as an output -- this is just for focus,
#  since we already know all the schools have at least
#  ten teachers
teachers[ , if (.N >= 10) .(avg_wage = mean(pay_rate)),
          by = home_organization_description
          ][order(-avg_wage)]
```

Here we see our first `if` statement in R. The basic form of an `if` statement in R is:

```{r if_statements, eval = FALSE}
#Note the parentheses around the logical!! Very easy to forget
if (some_logical){
  # what to do if true
} else {
  # what to do if false
}
```

It's important to note that `some_logical` has to be either `TRUE` or `FALSE` -- it can't be a vector!! This sounds easy now but it's guaranteed to trip you up at some point.

There's no `else` statement in the condition we used above. So, for all the schools where there are fewer than 10 teachers, nothing happens (technically, the statement returns `NULL` -- but don't worry about that for now), and `data.table` ignores any such sub-`data.table`.

I don't really know what `NON-PUBLIC PROGRAMS` means... but besides that, we see a lot of staple high school names. Of course, all teachers in Philly are on the same teachers' contract and hence the same pay scale -- so what this exercise has _really_ told us is that South Philly high has the most experienced and/or certified teachers, and that teachers at Dunbar are young and/or uncertified.

We could go on exploring / slicing&dicing this data all day, but I think we've learned enough essentials of manipulation/inspection for now. We'll pick up more tricks as we move along.

***
## Merging

Recall our teacher attendance data:

```{r attendance_redux}
attendance
```

We couldn't really tell anything about the schools because we didn't know what the school codes meant.

Well, it appears we've found the Rosetta Stone in the teacher salary data. The `home_organization` field looks very much like the `ULCS_NO` field in the attendance data.

Obviously, it'd be ideal to _know_ for sure that they represent the same ID for each school. In lieu of that, we can bolster our confidence in the alignment by comparing the fields a bit. Here are some techniques:

```{r compare_ids}
#We know from attendance there are 212 schools.
#  How many unique schools are represented in the teacher data?
teachers[ , uniqueN(home_organization)]

#How many of the ULCS_NO values are also found in home_organization
#  intersect find the middle of the venn diagram for the 
#  first and second arguments
length(intersect(attendance$ULCS_NO, teachers[ , unique(home_organization)]))
```

That's pretty convincing if you ask me.

So, how do we match the two data sets? In Stata, depending on which data set you currently have in memory, we would either perform a one-to-many or a many-to-one merge.

In R, merging either way is a cinch. But first, we need a few more tools.

### `:=` - Adding a column to a `data.table`

When we want to add a new column to an existing `data.table`, we use the `:=` operator `j` (the second argument). A quick aside, this is done _by reference_, which means it's **memory-efficient** and **fast**; see [this Q&A for more details](http://stackoverflow.com/questions/7029944/when-should-i-use-the-operator-in-data-table).

Let's try it out with two examples. Right now, in `attendance`, teacher attendance rates are listed as a percentage (out of 100); sometimes, it's more convenient to have percentages as a proportion (out of one).

Also, we might like to know the _relative_ attendance rates -- the ratio of attendance at a given school to average attendance.

In `teachers`, `gender` is stored as a `character` -- either `M` or `F`. But it's often quite convenient to have binary variables stored as `logical` -- for subsetting. 

```{r defining}
#add attendance as a proportion
attendance[ , attendance_proportion := SCH_TEACHER_ATTEND / 100]

#add relative attendance
attendance[ , attendance_relative := SCH_TEACHER_ATTEND / SDP_TEACHER_ATTEND]

#a quirk of data.table is that after using
#  :=, we sometimes have to force printing,
#  which we can do by appending [] to the end
attendance[]

#add logical version of gender
teachers[ , male := gender == "M"]

#now, subset to show only male teachers
teachers[(male)]
```

### Merge and update

Now we're equipped to understand the syntax for a merge. Let's add the school's name (`home_organization_description` in `teachers`) to the attendance data.

First, note that `teachers` has more data in it than we need. There are many teachers in each school, meaning the mapping between `home_organization` code and school name is repeated many times (once for each teacher in a given school). This presents a sort of an issue when we try and merge the `teachers` data into `attendance` -- for each `ULCS_NO` that gets matched to a `home_organization` in `teachers`, there will be many rows, even if they all have the same information. To deal with this we'll use `unique` to eliminate all the duplicates. We'll go step-by-step first, then all at once:

```{r merge_school_name}
#this will take the FIRST row associated
#  with each school. This is important in
#  general, but not here since we only
#  care about the school-level data for now,
#  which is the same in all rows.
schools <- unique(teachers, by = "home_organization")
schools

#now merge, using := to add the school name column
attendance[schools, school_name :=
             home_organization_description, 
           on = c(ULCS_NO = "home_organization")]
attendance
```

This general form of the syntax we just used is `X[Y, variable_in_x := variable_in_y, on = c(a = "b")]`. Let's break that down a bit:

 - `X` and `Y` are `data.table`s. We match their rows according to alignment in the variables specified by the `on` argument within the square brackets (`[]`) used to access `X`.
 - `on = c(a = "b")` tells R that rows in column `a` in `X` (both things on the _left_) should be matched & associated with rows of the same value in column `b` in `Y`.
 - we add the variable on the left of `:=` to `data.table` `X`. The variable on the right of `:=` is typically found in `Y` -- it could also be an expression consisting of columns in `X` _and_ `Y`.
 
 Let's explore a few other ways we could have gone about doing this merge. First, let's remove the column that we just created so that we're really starting from scratch. The way to remove columns from a `data.table` is to set them to `NULL`, which is _very_ fast.
 
 
```{r merge_alternatives}
attendance[ , school_name := NULL][]

#alternative 1: skip defining schools
#  extra tidbit: sometimes, both the 
#  main table (x) and the merging table
#  (in the first argument, i -- we called
#   it Y above) have columns with the same
#  name. To overcome the ambiguity this 
#  engenders, we can prepend i. to the
#  column names in teachers to tell R
#  that we're referring _explicitly_
#  to that column in teachers. Similarly,
#  we can prepend x. to tell R that we're
#  referring to the column in attendance
attendance[unique(teachers, by = "home_organization"),
           school_name := i.home_organization_description,
           on = c(ULCS_NO = "home_organization")][]

#reset again
attendance[ , school_name := NULL]

#alternative 2: use unique within the merge
#  by = .EACHI is a syntax unique to merging.
#  Using this tells R to do the thing in j
#  within each group that's matched.
#  So, here, each ULCS_NO gets matched to many
#  home_organization rows in teachers.
#  within this group, we use unique to
#  get the school's name, since unique turns the
#  vector of school names (one per teacher in
#  each school that's matched) into a single value.
attendance[teachers, 
           school_name := unique(home_organization_description),
           on = c(ULCS_NO = "home_organization"), by = .EACHI][]

#reset again
attendance[ , school_name := NULL]

#alternative 3: like 2, except use [] instead of unique.
#  This alternative works, and is faster, but is less robust
#  to finding mistakes in your data. Thankfully, this data is
#  pretty clean, but we often find messy data in the wild --
#  maybe there was a typo in the organization code or school name,
#  and instead of each home_organization being matched to
#  exactly one home_organization_description, we might find
#  it matched to several (e.g., "PS 131" and "PS #131").
#  If we use the unique approach from alternative 2,
#  we'll get an error, which tells us we need to examine our data
#  more closely. This approach will definitely not generate an error.
attendance[teachers,
           school_name := home_organization_description[1],
           on = c(ULCS_NO = "home_organization"), by = .EACHI][]
```

It's up to your taste which approach suits you -- each is perfect for a certain situation.

### Missing Data

## Regressions

### OLS (`lm`) / `predict`/`coeftest`/`summary`

### Probit/Logit (`glm`)

## Reshaping

## String Data / regex

## Non-.csv Data

### SAS: `read_sas`

### Excel: `readxl` / `xlsx`

### Fixed-width `read_fwf`

# Plotting / Tables (11 AM - 12 PM)

## Descriptive Tables

## Regression Tables: `texreg`/`xtable`

## `base` Plots

`axes`/`matplot`

## `ggplot`

# Web Scraping: `rvest` (1:30 PM - 2:30 PM)

`xpath`

# Geospatial Tools (2:30 PM - 3:30 PM)

`S4` methods/`@`slots, `CRS`

# Automatic Presentations/Papers: `knitr` (3:30 PM - 4:30 PM)

# Dynamic/Interactive Output: `shiny` (4:30 PM - 5:30 PM)

# Appendix

`vapply`

`CJ`

`setkey`

## Data Cleaning

## Source Code

## Package development

## Advanced types